{
    "id": "H-82",
    "original_text": "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites. These pages are often referred to as the Hidden Web or the Deep Web. Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results. However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users. In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web. Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site. Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically. Our policies proceed iteratively, issuing a different query in every iteration. We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising. For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries. Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval. General Terms: Algorithms, Performance, Design. 1. INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12]. In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms. These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user). According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7]. In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web. Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7]. For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art. In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them. Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links). We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present. Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users. Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result. By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28]. Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28]. According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort. Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required. Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface. The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented. Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages. Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward. We exhaustively issue all possible queries, one query at a time. When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries. In this case, what queries should we pick? Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form? In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically. We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites. In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions. Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy. Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites. Our experiments will show the relative advantages of various crawling policies and demonstrate their potential. The results from our experiments are very promising. In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2. FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem. In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites. Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2. Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics. Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database. A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]). Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1). In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K. Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2). In this paper, we will mainly focus on textual databases that support single-attribute keyword queries. We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1. Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1. Step 1. First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2. Step 2. Shortly after the user issues the query, she is presented with a result index page. That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3. Step 3. From the list in the result index page, the user identifies the pages that look interesting and follows the links. Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section. That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages. In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources. In Figure 4 we show the generic algorithm for a Hidden-Web crawler. For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)). Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)). This same process is repeated until all the available resources are used up (Step (1)). Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next. If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources. In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages. Therefore, how the crawler selects the next query can greatly affect its effectiveness. In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results. Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver. Figure 3: Pages from the PubMed Web site. ALGORITHM 2.1. Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site. S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5). We represent each Web page in S as a point (dots in Figure 5). Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site. Each subset is associated with a weight that represents the cost of issuing the query. Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost). This problem is equivalent to the set-covering problem in graph theory [16]. There are two main difficulties that we need to address in this formalization. First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance. Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage. Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found. In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost. Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned. Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site. We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics. Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site. For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3. We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)). Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)). We also use Cost(qi) to represent the cost of issuing the query qi. Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these. As we will see later, our proposed algorithms are independent of the exact cost function. In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)). We assume that submitting a query incurs a fixed cost of cq. The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed. Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries. In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries. Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi. Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper. When we need a concrete cost function, however, we will use Equation 2. Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1. Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3. KEYWORD SELECTION How should a crawler select the queries to issue? Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database. The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword. Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result. We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources. The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents. Based on this analysis, we issue the most promising query, and repeat the process. Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst. Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic. The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus. We will experimentally compare these three policies in Section 4. While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy. We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query. That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value. In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1. Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi). We may consider a number of different ways to estimate P(qi), including the following: 1. Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1. That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus. Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25]. That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc.), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection. Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset. For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27]. After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi). In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table. In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources. Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi. For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj. Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable. Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries). Cost(qi) represents the cost of issuing the query qi. Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site. However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1. Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi. Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value. By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents. In Figure 6, we show the query selection function that uses the concept of efficiency. In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step. We can estimate the efficiency of every query using the estimation method described in Section 3.1. That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3. We can also estimate Cost(qi) similarly. For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi. This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm. In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table. The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1. We record these counts in a table, as shown in Figure 7(a). The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term. For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents. Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents. This update can be done efficiently as we illustrate in the following example. EXAMPLE 1. After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi. From the new query qi = computer, we downloaded 20 more new pages. Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk. The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages. We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b). The result is shown on Figure 7(c). For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi. According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages. For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query. Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler. First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages. Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1). That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1. However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler. Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries. However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure. Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8). That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate. Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8). Now we need to update our query statistics table so that it has accurate information for the next step. That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3. Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1. The term P(qi+1 ∧ qi) however is unknown and we need to estimate it. Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4. EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper. Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases. Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms. The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2. To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant. That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries. Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model. In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages. Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26]. Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution). In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M). The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms. The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless. The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2]. According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles. We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm. Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed. The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology. In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books. The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form. The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply. Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection. Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books. Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000. As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site. The links are searchable through a keyword-search interface. We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying. On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list. As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into. The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed. Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites. That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above? More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases. In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively. On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number. A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms. In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies. Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific. For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,. For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites. The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set. Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection. Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies. For each keyword, we present the number of the iteration, along with the number of results that it returned. As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site. In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency. It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords. On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words. For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11). A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results. Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance. We are currently running another experiment to verify whether this is indeed the case. Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling. As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query. Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz. In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries. When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results . On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g. Amazon). Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations. The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet. Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary. For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million. These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low. We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles. As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query. Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query. In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers. Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance. We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site. Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query. Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites. In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query. In Figure 14 we plot the coverage for the two policies as a function of the number of queries. As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage. For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000. On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries. The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000. Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit. In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost. In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process. As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages. For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website. The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger. We believe that these values are reasonable for the PubMed Web site. Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process. The horizontal axis is the amount of resources used, and the vertical axis is the coverage. As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units. However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9. The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost. Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before. That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost. Still, we observe noticeable savings from the adaptive policy. At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5. RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler. The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically. The potential queries are either provided manually by users or collected from the query interfaces. In contrast, our main focus is to generate queries automatically without any human intervention. The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts. For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database. In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them. In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes. In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection. Our work differs from the previous studies in two ways. First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work. Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms. Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form. This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites. Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database. In [3] the authors study query-based techniques that can extract relational data from large text databases. Again, these works study orthogonal issues and are complementary to our work. In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]). These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents. Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces. There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18]. This body of work is often referred to as meta-searching or database selection problem over the Hidden Web. For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category. Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6. CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages. Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces. In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it. We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site. Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential. In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries. Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues. Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases. While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes. For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books. Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one. The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute. Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples. Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler. For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites. But how can the crawler discover the query interfaces? The method proposed in [15] may be a good starting point. In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results. In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically. Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day). In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site. Page similarity detection algorithms may be useful for this purpose [9, 13]. 7. REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano. Querying text databases for efficient information extraction. In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano. Modeling query-based access to text databases. In WebDB, 2003. [5] Article on New York Times. Old Search Engine, the Library, Tries to Fit Into a Google World. Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire. Siphoning hidden-web data through keyword-based interfaces. In SBBD, 2004. [7] M. K. Bergman. The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder. A technique for measuring the relative size and overlap of public web search engines. In WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic clustering of the web. In WWW, 1997. [10] J. Callan, M. Connell, and A. Du. Automatic discovery of language models for text databases. In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell. Query-based sampling of text databases. Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. He, C. Li, and Z. Zhang. Structured databases on the web: Observations and implications. Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina. Finding replicated web collections. In SIGMOD, 2000. [14] W. Cohen and Y. Singer. Learning to query the web. In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J. Cope, N. Craswell, and D. Hawking. Automated discovery of search interfaces on the web. In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms, 2nd Edition. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy, and A. O. Mendelzon. Database techniques for the world-wide web: A survey. SIGMOD Record, 27(3):59-74, 1998. [18] B. He and K. C.-C. Chang. Statistical schema matching across web query interfaces. In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano. Distributed search over the hidden web: Hierarchical database sampling and selection. In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami. Probe, count, and classify: Categorizing hidden web databases. In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel. The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles. Searching the World Wide Web. Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu. Dpro: A probabilistic approach for hidden web database selection using dynamic probing. In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson. DP9-An OAI Gateway Service for Web Crawlers. In JCDL, 2002. [25] B. B. Mandelbrot. Fractal Geometry of Nature. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston. Whats new on the web? the evolution of the web from a search engine perspective. In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho. Downloading hidden web content. Technical report, UCLA, 2004. [28] S. Olsen. Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina. Crawling the hidden web. In VLDB, 2001. [30] G. K. Zipf. Human Behavior and the Principle of Least-Effort. Addison-Wesley, Cambridge, MA, 1949. 109",
    "original_translation": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento. Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático. Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta. ¿Pero cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados. En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente. Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio. Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7. REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano. Consultando bases de datos de texto para una extracción eficiente de información. En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo en New York Times. El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google. Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire. Sifonando datos de la web oculta a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. La web profunda: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Agrupación sintáctica de la web. En WWW, 1997. [10] J. Callan, M. Connell y A. Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español? Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: Observaciones e implicaciones. Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina. Encontrando colecciones web replicadas. En SIGMOD, 2000. [14] W. Cohen y Y. Cantante. Aprendiendo a hacer consultas en la web. En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los Algoritmos, 2da Edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de bases de datos para la web mundial: Una encuesta. SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B. Él y K. C.-C. Chang. Emparejamiento de esquemas estadísticos en interfaces de consulta web. En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Explorar, contar y clasificar: Categorizando bases de datos de la web oculta. En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la World Wide Web. Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9: Un servicio de puerta de enlace OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda. En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargando contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina. Explorando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109",
    "original_sentences": [
        "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
        "These pages are often referred to as the Hidden Web or the Deep Web.",
        "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
        "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
        "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
        "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
        "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
        "Our policies proceed iteratively, issuing a different query in every iteration.",
        "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
        "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
        "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
        "General Terms: Algorithms, Performance, Design. 1.",
        "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
        "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
        "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
        "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
        "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
        "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
        "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
        "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
        "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
        "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
        "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
        "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
        "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
        "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
        "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
        "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
        "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
        "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
        "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
        "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
        "We exhaustively issue all possible queries, one query at a time.",
        "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
        "In this case, what queries should we pick?",
        "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
        "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
        "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
        "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
        "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
        "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
        "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
        "The results from our experiments are very promising.",
        "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
        "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
        "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
        "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
        "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
        "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
        "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
        "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
        "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
        "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
        "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
        "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
        "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
        "Step 1.",
        "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
        "Step 2.",
        "Shortly after the user issues the query, she is presented with a result index page.",
        "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
        "Step 3.",
        "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
        "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
        "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
        "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
        "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
        "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
        "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
        "This same process is repeated until all the available resources are used up (Step (1)).",
        "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
        "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
        "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
        "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
        "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
        "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
        "Figure 3: Pages from the PubMed Web site.",
        "ALGORITHM 2.1.",
        "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
        "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
        "We represent each Web page in S as a point (dots in Figure 5).",
        "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
        "Each subset is associated with a weight that represents the cost of issuing the query.",
        "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
        "This problem is equivalent to the set-covering problem in graph theory [16].",
        "There are two main difficulties that we need to address in this formalization.",
        "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
        "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
        "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
        "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
        "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
        "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
        "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
        "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
        "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
        "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
        "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
        "We also use Cost(qi) to represent the cost of issuing the query qi.",
        "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
        "As we will see later, our proposed algorithms are independent of the exact cost function.",
        "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
        "We assume that submitting a query incurs a fixed cost of cq.",
        "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
        "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
        "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
        "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
        "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
        "When we need a concrete cost function, however, we will use Equation 2.",
        "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
        "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
        "KEYWORD SELECTION How should a crawler select the queries to issue?",
        "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
        "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
        "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
        "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
        "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
        "Based on this analysis, we issue the most promising query, and repeat the process.",
        "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
        "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
        "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
        "We will experimentally compare these three policies in Section 4.",
        "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
        "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
        "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
        "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
        "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
        "We may consider a number of different ways to estimate P(qi), including the following: 1.",
        "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
        "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
        "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
        "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
        "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
        "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
        "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
        "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
        "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
        "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
        "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
        "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
        "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
        "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
        "Cost(qi) represents the cost of issuing the query qi.",
        "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
        "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
        "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
        "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
        "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
        "In Figure 6, we show the query selection function that uses the concept of efficiency.",
        "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
        "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
        "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
        "We can also estimate Cost(qi) similarly.",
        "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
        "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
        "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
        "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
        "We record these counts in a table, as shown in Figure 7(a).",
        "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
        "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
        "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
        "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
        "This update can be done efficiently as we illustrate in the following example.",
        "EXAMPLE 1.",
        "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
        "From the new query qi = computer, we downloaded 20 more new pages.",
        "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
        "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
        "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
        "The result is shown on Figure 7(c).",
        "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
        "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
        "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
        "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
        "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
        "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
        "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
        "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
        "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
        "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
        "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
        "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
        "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
        "Now we need to update our query statistics table so that it has accurate information for the next step.",
        "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
        "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
        "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
        "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
        "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
        "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
        "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
        "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
        "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
        "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
        "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
        "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
        "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
        "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
        "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
        "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
        "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
        "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
        "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
        "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
        "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
        "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
        "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
        "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
        "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
        "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
        "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
        "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
        "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
        "The links are searchable through a keyword-search interface.",
        "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
        "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
        "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
        "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
        "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
        "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
        "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
        "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
        "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
        "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
        "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
        "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
        "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
        "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
        "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
        "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
        "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
        "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
        "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
        "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
        "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
        "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
        "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
        "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
        "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
        "We are currently running another experiment to verify whether this is indeed the case.",
        "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
        "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
        "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
        "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
        "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
        "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
        "Amazon).",
        "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
        "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
        "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
        "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
        "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
        "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
        "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
        "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
        "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
        "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
        "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
        "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
        "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
        "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
        "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
        "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
        "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
        "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
        "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
        "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
        "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
        "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
        "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
        "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
        "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
        "We believe that these values are reasonable for the PubMed Web site.",
        "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
        "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
        "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
        "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
        "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
        "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
        "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
        "Still, we observe noticeable savings from the adaptive policy.",
        "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
        "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
        "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
        "The potential queries are either provided manually by users or collected from the query interfaces.",
        "In contrast, our main focus is to generate queries automatically without any human intervention.",
        "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
        "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
        "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
        "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
        "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
        "Our work differs from the previous studies in two ways.",
        "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
        "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
        "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
        "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
        "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
        "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
        "Again, these works study orthogonal issues and are complementary to our work.",
        "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
        "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
        "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
        "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
        "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
        "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
        "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
        "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
        "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
        "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
        "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
        "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
        "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
        "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
        "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
        "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
        "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
        "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
        "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
        "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
        "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
        "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
        "But how can the crawler discover the query interfaces?",
        "The method proposed in [15] may be a good starting point.",
        "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
        "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
        "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
        "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
        "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
        "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
        "Querying text databases for efficient information extraction.",
        "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
        "Modeling query-based access to text databases.",
        "In WebDB, 2003. [5] Article on New York Times.",
        "Old Search Engine, the Library, Tries to Fit Into a Google World.",
        "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
        "Siphoning hidden-web data through keyword-based interfaces.",
        "In SBBD, 2004. [7] M. K. Bergman.",
        "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
        "A technique for measuring the relative size and overlap of public web search engines.",
        "In WWW, 1998. [9] A.",
        "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
        "Syntactic clustering of the web.",
        "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
        "Du.",
        "Automatic discovery of language models for text databases.",
        "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
        "Query-based sampling of text databases.",
        "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
        "He, C. Li, and Z. Zhang.",
        "Structured databases on the web: Observations and implications.",
        "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
        "Finding replicated web collections.",
        "In SIGMOD, 2000. [14] W. Cohen and Y.",
        "Singer.",
        "Learning to query the web.",
        "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
        "Cope, N. Craswell, and D. Hawking.",
        "Automated discovery of search interfaces on the web.",
        "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
        "Introduction to Algorithms, 2nd Edition.",
        "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
        "Levy, and A. O. Mendelzon.",
        "Database techniques for the world-wide web: A survey.",
        "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
        "He and K. C.-C. Chang.",
        "Statistical schema matching across web query interfaces.",
        "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
        "Distributed search over the hidden web: Hierarchical database sampling and selection.",
        "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
        "Probe, count, and classify: Categorizing hidden web databases.",
        "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
        "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
        "Searching the World Wide Web.",
        "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
        "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
        "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
        "DP9-An OAI Gateway Service for Web Crawlers.",
        "In JCDL, 2002. [25] B.",
        "B. Mandelbrot.",
        "Fractal Geometry of Nature.",
        "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
        "Whats new on the web? the evolution of the web from a search engine perspective.",
        "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
        "Downloading hidden web content.",
        "Technical report, UCLA, 2004. [28] S. Olsen.",
        "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
        "Crawling the hidden web.",
        "In VLDB, 2001. [30] G. K. Zipf.",
        "Human Behavior and the Principle of Least-Effort.",
        "Addison-Wesley, Cambridge, MA, 1949. 109"
    ],
    "translated_text_sentences": [
        "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave.",
        "Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda.",
        "Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados.",
        "Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios.",
        "En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma.",
        "Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio.",
        "Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente.",
        "Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración.",
        "Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores.",
        "Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas.",
        "Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información.",
        "Términos generales: Algoritmos, Rendimiento, Diseño. 1.",
        "INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12].",
        "En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios.",
        "Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web).",
        "Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7].",
        "En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta.",
        "Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7].",
        "Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica.",
        "En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas.",
        "Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces).",
        "Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad.",
        "Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web.",
        "A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados.",
        "Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28].",
        "Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28].",
        "Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo.",
        "Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido.",
        "Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta.",
        "El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda.",
        "Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta.",
        "Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla.",
        "Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez.",
        "Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles.",
        "En este caso, ¿qué consultas deberíamos seleccionar?",
        "¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda?",
        "En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente.",
        "También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web.",
        "En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones.",
        "Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima.",
        "Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales.",
        "Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial.",
        "Los resultados de nuestros experimentos son muy prometedores.",
        "En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas.",
        "En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web.",
        "En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios.",
        "Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2.",
        "Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas.",
        "Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada.",
        "Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]).",
        "Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1).",
        "Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K.",
        "Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2).",
        "En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo.",
        "Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1.",
        "Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1.",
        "Paso 1.",
        "Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2.",
        "Paso 2.",
        "Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados.",
        "Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a).",
        "Paso 3.",
        "Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces.",
        "Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior.",
        "Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales.",
        "En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos.",
        "En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web.",
        "Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3).",
        "Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4).",
        "Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)).",
        "Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación.",
        "Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos.",
        "Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales.",
        "Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad.",
        "En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados.",
        "Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado.",
        "Figura 3: Páginas del sitio web de PubMed.",
        "ALGORITMO 2.1.",
        "Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto.",
        "Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5).",
        "Representamos cada página web en S como un punto (puntos en la Figura 5).",
        "Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio.",
        "Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta.",
        "Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo).",
        "Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16].",
        "Hay dos dificultades principales que necesitamos abordar en esta formalización.",
        "Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano.",
        "Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura.",
        "Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico.",
        "En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable.",
        "Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas.",
        "Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web.",
        "Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento.",
        "Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio.",
        "Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3.",
        "Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)).",
        "De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)).",
        "También usamos Cost(qi) para representar el costo de emitir la consulta qi.",
        "Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos.",
        "Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta.",
        "En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)).",
        "Suponemos que enviar una consulta conlleva un costo fijo de cq.",
        "El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo.",
        "Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores.",
        "En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores.",
        "Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi.",
        "Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo.",
        "Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2.",
        "Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1.",
        "Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador.",
        "SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir?",
        "Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos.",
        "La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave.",
        "Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado.",
        "Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga.",
        "La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos.",
        "Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso.",
        "Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento.",
        "Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado.",
        "La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico.",
        "Experimentalmente compararemos estas tres políticas en la Sección 4.",
        "Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa.",
        "Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta.",
        "Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor.",
        "Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1.",
        "Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi).",
        "Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1.",
        "Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1.",
        "Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
        "Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus.",
        "Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25].",
        "Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto.",
        "Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto.",
        "Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27].",
        "Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi).",
        "En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa.",
        "En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga.",
        "Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi.",
        "Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj.",
        "Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable.",
        "Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores).",
        "Cost(qi) representa el costo de emitir la consulta qi.",
        "De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio.",
        "Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1.",
        "SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi.",
        "Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto.",
        "Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos.",
        "En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia.",
        "En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso.",
        "Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1.",
        "Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3.",
        "También podemos estimar Cost(qi) de manera similar.",
        "Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi.",
        "Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo.",
        "En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta.",
        "La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1.",
        "Registramos estos conteos en una tabla, como se muestra en la Figura 7(a).",
        "La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo.",
        "Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos.",
        "Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
        "Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos.",
        "Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo.",
        "EJEMPLO 1.",
        "Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi.",
        "Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales.",
        "De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco.",
        "La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas.",
        "Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b).",
        "El resultado se muestra en la Figura 7(c).",
        "Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi.",
        "Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas.",
        "Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta.",
        "Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta.",
        "Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas.",
        "Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1).",
        "Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1.",
        "Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador.",
        "Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas.",
        "Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura.",
        "Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8).",
        "Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa.",
        "Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8).",
        "Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso.",
        "Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3.",
        "Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1.",
        "El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo.",
        "Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4.",
        "EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo.",
        "Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto.",
        "Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo.",
        "El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2.",
        "Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante.",
        "Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas.",
        "Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado.",
        "Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas.",
        "Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas.",
        "Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme).",
        "Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M).",
        "El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad.",
        "El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido.",
        "Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto.",
        "Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos.",
        "Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo.",
        "Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed.",
        "La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología.",
        "En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros.",
        "La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML.",
        "El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML.",
        "Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección.",
        "De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros.",
        "También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000.",
        "En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado.",
        "Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave.",
        "Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta.",
        "En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave.",
        "Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\". ",
        "El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed.",
        "Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios.",
        "Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente?",
        "Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta.",
        "En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente.",
        "En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta.",
        "Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar.",
        "En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas.",
        "Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema.",
        "Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura.",
        "Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados.",
        "El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados.",
        "Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica.",
        "La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas.",
        "Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió.",
        "Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web.",
        "En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica.",
        "Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas.",
        "Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta.",
        "Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11).",
        "Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos.",
        "Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental.",
        "Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto.",
        "Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo.",
        "En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta.",
        "Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico.",
        "En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas.",
        "Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados.",
        "Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo.",
        "Amazon.",
        "Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores.",
        "La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado.",
        "Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional.",
        "Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones.",
        "Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja.",
        "También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos.",
        "Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25.",
        "Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial.",
        "En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia.",
        "Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final.",
        "Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular.",
        "Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial.",
        "Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta.",
        "Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta.",
        "En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas.",
        "Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura.",
        "Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000.",
        "Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas.",
        "La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000.",
        "Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados.",
        "En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante.",
        "En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas.",
        "Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales.",
        "Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed.",
        "Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor.",
        "Creemos que estos valores son razonables para el sitio web de PubMed.",
        "La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga.",
        "El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura.",
        "Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso.",
        "Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9.",
        "La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo.",
        "Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes.",
        "Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total.",
        "Sin embargo, observamos ahorros significativos gracias a la política adaptativa.",
        "A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3.",
        "TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta.",
        "El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente.",
        "Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta.",
        "Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana.",
        "La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos.",
        "Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos.",
        "En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos.",
        "De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices.",
        "En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos.",
        "Nuestro trabajo difiere de los estudios anteriores en dos aspectos.",
        "Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo.",
        "Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos.",
        "Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda.",
        "Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web.",
        "La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos.",
        "En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes.",
        "Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo.",
        "Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]).",
        "Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos.",
        "Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda.",
        "Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18].",
        "Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta.",
        "Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática.",
        "Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6.",
        "CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas.",
        "Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta.",
        "En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él.",
        "Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta.",
        "La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial.",
        "En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas.",
        "Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras.",
        "Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo.",
        "Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta.",
        "Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes.",
        "Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor.",
        "El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo.",
        "Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento.",
        "Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático.",
        "Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta.",
        "¿Pero cómo puede el rastreador descubrir las interfaces de consulta?",
        "El método propuesto en [15] puede ser un buen punto de partida.",
        "Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados.",
        "En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente.",
        "Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día).",
        "En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio.",
        "Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7.",
        "REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano.",
        "Consultando bases de datos de texto para una extracción eficiente de información.",
        "En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano.",
        "Modelado de acceso basado en consultas a bases de datos de texto.",
        "En WebDB, 2003. [5] Artículo en New York Times.",
        "El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google.",
        "Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire.",
        "Sifonando datos de la web oculta a través de interfaces basadas en palabras clave.",
        "En SBBD, 2004. [7] M. K. Bergman.",
        "La web profunda: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder.",
        "Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos.",
        "En WWW, 1998. [9] A.",
        "Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig.",
        "Agrupación sintáctica de la web.",
        "En WWW, 1997. [10] J. Callan, M. Connell y A.",
        "Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español?",
        "Descubrimiento automático de modelos de lenguaje para bases de datos de texto.",
        "En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell.",
        "Muestreo basado en consultas de bases de datos de texto.",
        "Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
        "Él, C. Li y Z. Zhang.",
        "Bases de datos estructuradas en la web: Observaciones e implicaciones.",
        "Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina.",
        "Encontrando colecciones web replicadas.",
        "En SIGMOD, 2000. [14] W. Cohen y Y.",
        "Cantante.",
        "Aprendiendo a hacer consultas en la web.",
        "En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J.",
        "Cope, N. Craswell y D. Hawking.",
        "Descubrimiento automatizado de interfaces de búsqueda en la web.",
        "En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest.",
        "Introducción a los Algoritmos, 2da Edición.",
        "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
        "Levy y A. O. Mendelzon.",
        "Técnicas de bases de datos para la web mundial: Una encuesta.",
        "SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B.",
        "Él y K. C.-C. Chang.",
        "Emparejamiento de esquemas estadísticos en interfaces de consulta web.",
        "En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano.",
        "Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos.",
        "En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami.",
        "Explorar, contar y clasificar: Categorizando bases de datos de la web oculta.",
        "En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel.",
        "La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles.",
        "Buscando en la World Wide Web.",
        "Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu.",
        "Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas.",
        "En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson.",
        "DP9: Un servicio de puerta de enlace OAI para rastreadores web.",
        "En JCDL, 2002. [25] B.",
        "B. Mandelbrot.",
        "Geometría fractal de la naturaleza.",
        "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston.",
        "¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda.",
        "En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho.",
        "Descargando contenido web oculto.",
        "Informe técnico, UCLA, 2004. [28] S. Olsen.",
        "¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina.",
        "Explorando la web oculta.",
        "En VLDB, 2001. [30] G. K. Zipf.",
        "Comportamiento humano y el principio de menor esfuerzo.",
        "Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109"
    ],
    "error_count": 2,
    "keys": {
        "hidden web": {
            "translated_key": "Web Oculta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual <br>hidden web</br> Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the <br>hidden web</br> or the Deep Web.",
                "Since there are no static links to the <br>hidden web</br> pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many <br>hidden web</br> sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective <br>hidden web</br> crawler that can autonomously discover and download pages from the <br>hidden web</br>.",
                "Since the only entry point to a <br>hidden web</br> site is a query interface, the main challenge that a <br>hidden web</br> crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the <br>hidden web</br> and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real <br>hidden web</br> sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a <br>hidden web</br> site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the <br>hidden web</br> [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the <br>hidden web</br> increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the <br>hidden web</br>, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the <br>hidden web</br>. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their <br>hidden web</br> sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for <br>hidden web</br> sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to <br>hidden web</br> pages is through querying a search form, there are two core challenges to implementing an effective <br>hidden web</br> crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the <br>hidden web</br> pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the <br>hidden web</br>, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of <br>hidden web</br> sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic <br>hidden web</br> crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the <br>hidden web</br> pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a <br>hidden web</br> site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a <br>hidden web</br> site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the <br>hidden web</br> site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the <br>hidden web</br> crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the <br>hidden web</br> site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our <br>hidden web</br> crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the <br>hidden web</br> site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for <br>hidden web</br> crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular <br>hidden web</br> sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three <br>hidden web</br> sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The <br>hidden web</br> crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third <br>hidden web</br> site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two <br>hidden web</br> crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the <br>hidden web</br> site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two <br>hidden web</br> crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a <br>hidden web</br> site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific <br>hidden web</br> sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in <br>hidden web</br> sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive <br>hidden web</br> crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other <br>hidden web</br> sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a <br>hidden web</br> site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a <br>hidden web</br> crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of <br>hidden web</br> crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the <br>hidden web</br>.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the <br>hidden web</br> pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a <br>hidden web</br> crawler that can automatically query a <br>hidden web</br> site and download pages from it.",
                "We proposed three different query generation policies for the <br>hidden web</br>: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the <br>hidden web</br> site.",
                "Experimental evaluation on 4 real <br>hidden web</br> sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a <br>hidden web</br> site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some <br>hidden web</br> sites may contain an infinite number of <br>hidden web</br> pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the <br>hidden web</br>: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing <br>hidden web</br> databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for <br>hidden web</br> database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading <br>hidden web</br> content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the <br>hidden web</br>.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "Downloading Textual <br>hidden web</br> Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the <br>hidden web</br> or the Deep Web.",
                "Since there are no static links to the <br>hidden web</br> pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many <br>hidden web</br> sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective <br>hidden web</br> crawler that can autonomously discover and download pages from the <br>hidden web</br>."
            ],
            "translated_annotated_samples": [
                "Descargando contenido textual de la <br>Web Oculta</br> a través de consultas de palabras clave.",
                "Estas páginas son a menudo referidas como la <br>Web Oculta</br> o la Web Profunda.",
                "Dado que no hay enlaces estáticos a las páginas de la <br>Web Oculta</br>, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados.",
                "Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la <br>Web Oculta</br> suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios.",
                "En este artículo, estudiamos cómo podemos construir un rastreador de la <br>Hidden Web</br> efectivo que pueda descubrir y descargar páginas de la <br>Hidden Web</br> de forma autónoma."
            ],
            "translated_text": "Descargando contenido textual de la <br>Web Oculta</br> a través de consultas de palabras clave. Estas páginas son a menudo referidas como la <br>Web Oculta</br> o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la <br>Web Oculta</br>, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la <br>Web Oculta</br> suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la <br>Hidden Web</br> efectivo que pueda descubrir y descargar páginas de la <br>Hidden Web</br> de forma autónoma. ",
            "candidates": [],
            "error": [
                [
                    "Web Oculta",
                    "Web Oculta",
                    "Web Oculta",
                    "Web Oculta",
                    "Hidden Web",
                    "Hidden Web"
                ]
            ]
        },
        "deep web": {
            "translated_key": "Web Profunda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the <br>deep web</br>.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the <br>deep web</br> [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The <br>deep web</br>: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "These pages are often referred to as the Hidden Web or the <br>deep web</br>.",
                "These pages are often referred to as the Hidden Web [17] or the <br>deep web</br> [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "The <br>deep web</br>: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder."
            ],
            "translated_annotated_samples": [
                "Estas páginas son a menudo referidas como la Web Oculta o la <br>Web Profunda</br>.",
                "Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web).",
                "La <br>web profunda</br>: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la <br>Web Profunda</br>. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento. Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático. Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta. ¿Pero cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados. En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente. Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio. Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7. REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano. Consultando bases de datos de texto para una extracción eficiente de información. En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo en New York Times. El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google. Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire. Sifonando datos de la web oculta a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. La <br>web profunda</br>: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Agrupación sintáctica de la web. En WWW, 1997. [10] J. Callan, M. Connell y A. Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español? Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: Observaciones e implicaciones. Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina. Encontrando colecciones web replicadas. En SIGMOD, 2000. [14] W. Cohen y Y. Cantante. Aprendiendo a hacer consultas en la web. En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los Algoritmos, 2da Edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de bases de datos para la web mundial: Una encuesta. SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B. Él y K. C.-C. Chang. Emparejamiento de esquemas estadísticos en interfaces de consulta web. En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Explorar, contar y clasificar: Categorizando bases de datos de la web oculta. En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la World Wide Web. Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9: Un servicio de puerta de enlace OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda. En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargando contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina. Explorando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "keyword queries": {
            "translated_key": "consultas de múltiples palabras clave",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute <br>keyword queries</br>.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-<br>keyword queries</br>, single-term queries return the maximum number of results.",
                "Extending our work to multi-<br>keyword queries</br> is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-<br>keyword queries</br> that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "In this paper, we will mainly focus on textual databases that support single-attribute <br>keyword queries</br>.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-<br>keyword queries</br>, single-term queries return the maximum number of results.",
                "Extending our work to multi-<br>keyword queries</br> is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-<br>keyword queries</br> that can return a large fraction of a document collection."
            ],
            "translated_annotated_samples": [
                "En este documento, nos centraremos principalmente en bases de datos textuales que admiten <br>consultas de palabras clave</br> de un solo atributo.",
                "En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para <br>consultas de múltiples palabras clave</br>, las consultas de un solo término devuelven el máximo número de resultados.",
                "Extender nuestro trabajo a <br>consultas de múltiples palabras clave</br> es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado.",
                "En [6], Barbosa y Freire evalúan experimentalmente métodos para construir <br>consultas de múltiples palabras clave</br> que pueden devolver una gran fracción de una colección de documentos."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten <br>consultas de palabras clave</br> de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para <br>consultas de múltiples palabras clave</br>, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a <br>consultas de múltiples palabras clave</br> es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir <br>consultas de múltiples palabras clave</br> que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento. Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático. Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta. ¿Pero cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados. En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente. Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio. Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7. REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano. Consultando bases de datos de texto para una extracción eficiente de información. En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo en New York Times. El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google. Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire. Sifonando datos de la web oculta a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. La web profunda: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Agrupación sintáctica de la web. En WWW, 1997. [10] J. Callan, M. Connell y A. Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español? Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: Observaciones e implicaciones. Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina. Encontrando colecciones web replicadas. En SIGMOD, 2000. [14] W. Cohen y Y. Cantante. Aprendiendo a hacer consultas en la web. En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los Algoritmos, 2da Edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de bases de datos para la web mundial: Una encuesta. SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B. Él y K. C.-C. Chang. Emparejamiento de esquemas estadísticos en interfaces de consulta web. En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Explorar, contar y clasificar: Categorizando bases de datos de la web oculta. En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la World Wide Web. Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9: Un servicio de puerta de enlace OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda. En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargando contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina. Explorando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109 ",
            "candidates": [],
            "error": [
                [
                    "consultas de palabras clave",
                    "consultas de múltiples palabras clave",
                    "consultas de múltiples palabras clave",
                    "consultas de múltiples palabras clave"
                ]
            ]
        },
        "hidden web crawler": {
            "translated_key": "rastreador de la Hidden Web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective <br>hidden web crawler</br> that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a <br>hidden web crawler</br> has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective <br>hidden web crawler</br>: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the <br>hidden web crawler</br> can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our <br>hidden web crawler</br>.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a <br>hidden web crawler</br>.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a <br>hidden web crawler</br> that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "In this paper, we study how we can build an effective <br>hidden web crawler</br> that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a <br>hidden web crawler</br> has to face is how to automatically generate meaningful queries to issue to the site.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective <br>hidden web crawler</br>: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "Thus, the <br>hidden web crawler</br> can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "Obviously, this kind of limitation has an immediate effect on our <br>hidden web crawler</br>."
            ],
            "translated_annotated_samples": [
                "En este artículo, estudiamos cómo podemos construir un <br>rastreador de la Hidden Web</br> efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma.",
                "Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un <br>rastreador de la Hidden Web</br> es cómo generar automáticamente consultas significativas para enviar al sitio.",
                "Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta.",
                "Por lo tanto, el <br>rastreador de la Web Oculta</br> puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto.",
                "Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro <br>rastreador de la Web Oculta</br>."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un <br>rastreador de la Hidden Web</br> efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un <br>rastreador de la Hidden Web</br> es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el <br>rastreador de la Web Oculta</br> puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro <br>rastreador de la Web Oculta</br>. ",
            "candidates": [],
            "error": [
                [
                    "rastreador de la Hidden Web",
                    "rastreador de la Hidden Web",
                    "rastreador de la Web Oculta",
                    "rastreador de la Web Oculta"
                ]
            ]
        },
        "query-selection policy": {
            "translated_key": "política de selección de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its <br>query-selection policy</br> automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "Our adaptive policy examines the pages returned from previous queries and adapts its <br>query-selection policy</br> automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites."
            ],
            "translated_annotated_samples": [
                "Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su <br>política de selección de consultas</br> basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su <br>política de selección de consultas</br> basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento. Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático. Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta. ¿Pero cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados. En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente. Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio. Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7. REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano. Consultando bases de datos de texto para una extracción eficiente de información. En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo en New York Times. El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google. Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire. Sifonando datos de la web oculta a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. La web profunda: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Agrupación sintáctica de la web. En WWW, 1997. [10] J. Callan, M. Connell y A. Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español? Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: Observaciones e implicaciones. Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina. Encontrando colecciones web replicadas. En SIGMOD, 2000. [14] W. Cohen y Y. Cantante. Aprendiendo a hacer consultas en la web. En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los Algoritmos, 2da Edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de bases de datos para la web mundial: Una encuesta. SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B. Él y K. C.-C. Chang. Emparejamiento de esquemas estadísticos en interfaces de consulta web. En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Explorar, contar y clasificar: Categorizando bases de datos de la web oculta. En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la World Wide Web. Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9: Un servicio de puerta de enlace OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda. En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargando contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina. Explorando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "crawling policies": {
            "translated_key": "políticas de rastreo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of <br>crawling policies</br> for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various <br>crawling policies</br> through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various <br>crawling policies</br> and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of <br>crawling policies</br> for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various <br>crawling policies</br> through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various <br>crawling policies</br> and demonstrate their potential."
            ],
            "translated_annotated_samples": [
                "En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de <br>políticas de rastreo</br> para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones.",
                "Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas <br>políticas de rastreo</br> a través de experimentos en sitios web reales.",
                "Nuestros experimentos mostrarán las ventajas relativas de diversas <br>políticas de rastreo</br> y demostrarán su potencial."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de <br>políticas de rastreo</br> para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas <br>políticas de rastreo</br> a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas <br>políticas de rastreo</br> y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento. Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático. Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta. ¿Pero cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados. En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente. Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio. Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7. REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano. Consultando bases de datos de texto para una extracción eficiente de información. En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo en New York Times. El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google. Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire. Sifonando datos de la web oculta a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. La web profunda: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Agrupación sintáctica de la web. En WWW, 1997. [10] J. Callan, M. Connell y A. Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español? Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: Observaciones e implicaciones. Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina. Encontrando colecciones web replicadas. En SIGMOD, 2000. [14] W. Cohen y Y. Cantante. Aprendiendo a hacer consultas en la web. En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los Algoritmos, 2da Edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de bases de datos para la web mundial: Una encuesta. SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B. Él y K. C.-C. Chang. Emparejamiento de esquemas estadísticos en interfaces de consulta web. En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Explorar, contar y clasificar: Categorizando bases de datos de la web oculta. En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la World Wide Web. Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9: Un servicio de puerta de enlace OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda. En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargando contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina. Explorando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "textual database": {
            "translated_key": "base de datos textual",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a <br>textual database</br> or a structured database.",
                "A <br>textual database</br> is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a <br>textual database</br>, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a <br>textual database</br> depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the <br>textual database</br>: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "Depending on the type of information, we may categorize a Hidden-Web site either as a <br>textual database</br> or a structured database.",
                "A <br>textual database</br> is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Given that the goal is to download the maximum number of unique documents from a <br>textual database</br>, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "Since the number of documents that are discovered and downloaded from a <br>textual database</br> depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the <br>textual database</br>: estimate the frequency of each attribute value and pick the most promising one."
            ],
            "translated_annotated_samples": [
                "Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una <br>base de datos textual</br> o una base de datos estructurada.",
                "Una <br>base de datos textual</br> es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]).",
                "Dado que el objetivo es descargar el máximo número de documentos únicos de una <br>base de datos textual</br>, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos.",
                "Dado que el número de documentos que se descubren y descargan de una <br>base de datos textual</br> depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo.",
                "Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la <br>base de datos textual</br>: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una <br>base de datos textual</br> o una base de datos estructurada. Una <br>base de datos textual</br> es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una <br>base de datos textual</br>, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una <br>base de datos textual</br> depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la <br>base de datos textual</br>: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "generic-frequency policy": {
            "translated_key": "política de frecuencia genérica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the <br>generic-frequency policy</br> if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the <br>generic-frequency policy</br>, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the <br>generic-frequency policy</br> (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the <br>generic-frequency policy</br> proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the <br>generic-frequency policy</br> regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the <br>generic-frequency policy</br> for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "The adaptive policy, however, may perform significantly better than the <br>generic-frequency policy</br> if the database has a very specialized collection that is different from the generic corpus.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the <br>generic-frequency policy</br>, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the <br>generic-frequency policy</br> (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "On the other hand, the <br>generic-frequency policy</br> proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Finally, our result shows that our adaptive policy consistently outperforms the <br>generic-frequency policy</br> regardless of the result limit."
            ],
            "translated_annotated_samples": [
                "La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la <br>política de frecuencia genérica</br> si la base de datos tiene una colección muy especializada que es diferente al corpus genérico.",
                "Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la <br>política de frecuencia genérica</br>, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas.",
                "Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la <br>política de frecuencia genérica</br> (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M).",
                "Por otro lado, la <br>política de frecuencia genérica</br> también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo.",
                "Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la <br>política genérica de frecuencia</br> independientemente del límite de resultados."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la <br>política de frecuencia genérica</br> si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la <br>política de frecuencia genérica</br>, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la <br>política de frecuencia genérica</br> (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la <br>política de frecuencia genérica</br> también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la <br>política genérica de frecuencia</br> independientemente del límite de resultados. ",
            "candidates": [],
            "error": [
                [
                    "política de frecuencia genérica",
                    "política de frecuencia genérica",
                    "política de frecuencia genérica",
                    "política de frecuencia genérica",
                    "política genérica de frecuencia"
                ]
            ]
        },
        "independence estimator": {
            "translated_key": "estimador de independencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "<br>independence estimator</br>: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the <br>independence estimator</br> (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the <br>independence estimator</br> is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "<br>independence estimator</br>: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "In addition, we use the <br>independence estimator</br> (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the <br>independence estimator</br> is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26]."
            ],
            "translated_annotated_samples": [
                "Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1.",
                "Además, utilizamos el <br>estimador de independencia</br> (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas.",
                "Aunque el <br>estimador de independencia</br> es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el <br>estimador de independencia</br> (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el <br>estimador de independencia</br> es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que devolvió. Como se puede ver en la tabla, estas palabras clave son altamente relevantes para los temas de medicina y biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en azar tienen un rendimiento mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política basada en la aleatoriedad con el pequeño y cuidadosamente seleccionado conjunto de 16,000 palabras de calidad logra descargar una fracción considerable del 42.5% 106 Iteración Palabra clave Número de resultados 23 departamento 2,719,031 34 pacientes 1,934,428 53 clínico 1,198,322 67 tratamiento 4,034,565 69 médico 1,368,200 70 hospital 503,307 146 enfermedad 1,520,908 172 proteína 2,620,938 Tabla 1: Muestra de palabras clave consultadas en PubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura para la sección de Artes de dmoz alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en la aleatoriedad que hace uso de la vasta colección de 1 millón de palabras, entre las cuales un gran número son palabras clave falsas, no logra descargar ni siquiera un mísero 1% de la colección total, después de enviar el mismo número de palabras de consulta. Para las colecciones genéricas de Amazon y los sitios dmoz, mostrados en las Figuras 10 y 11 respectivamente, obtenemos resultados mixtos: La política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo claramente supera a la frecuencia genérica para el sitio general de dmoz (Figura 11). Un examen más detallado de los archivos de registro de los dos rastreadores de la Hidden Web revela la razón principal: Amazon estaba funcionando de manera muy inestable cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente deficiente de la política adaptativa se debe a esta variabilidad experimental. Actualmente estamos llevando a cabo otro experimento para verificar si esto es realmente cierto. Además de esta variabilidad experimental, el resultado del Amazonas indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricas, entonces el enfoque de frecuencia genérica puede ser un algoritmo candidato adecuado para un rastreo efectivo. En el caso de los sitios web ocultos específicos de un tema, las políticas basadas en aleatoriedad también muestran un rendimiento deficiente en comparación con los otros dos algoritmos al rastrear sitios genéricos: para el sitio web de Amazon, el aleatorio-16K logra descargar casi un 36.7% después de emitir 775 consultas, lamentablemente para la colección genérica de dmoz, la fracción de la colección de enlaces descargados es del 13.5% después de la 770ª consulta. Finalmente, como se esperaba, random-1M es aún peor que random16K, descargando solo el 14.5% de Amazon y el 0.3% del dmoz genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios de la Web Oculta emitiendo el menor número de consultas. Cuando la colección se refiere a un tema específico, es capaz de identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que es más probable que devuelvan un gran número de resultados. Por otro lado, la política de frecuencia genérica también resulta ser bastante efectiva, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico de un tema, su efectividad puede alcanzar la de la adaptativa (por ejemplo. Amazon. Finalmente, la política aleatoria tiene un rendimiento deficiente en general y no debería ser preferida. 4.2 Impacto de la consulta inicial. Un tema interesante que merece un examen más detenido es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección de la fracción de documentos 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 número de consulta Convergencia de adaptación bajo diferentes consultas iniciales - Sitio web de PubMed datos de información de PubMed devuelve Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el sitio web de PubMed el algoritmo adaptativo en sí mismo y debe ser configurado manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que para automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores adaptativos de la Hidden Web dirigidos al sitio web de PubMed con diferentes palabras clave: la palabra \"data\", que devuelve 1,344,999 resultados, la palabra \"information\" que reporta 308,474 documentos, y la palabra \"return\" que recupera 29,707 páginas, de un total de 14 millones. Estas palabras clave representan diferentes grados de popularidad de términos en PubMed, siendo la primera de alta popularidad, la segunda de mediana y la tercera de baja. También mostramos resultados para la palabra clave pubmed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de inicio: Sus coberturas son aproximadamente equivalentes a partir de la consulta número 25. Finalmente, los cuatro rastreadores utilizan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, a partir de la 36ª consulta en adelante, los cuatro rastreadores utilizan los mismos términos para sus consultas en cada iteración, o se utilizan los mismos términos con una o dos consultas de diferencia. Nuestro resultado confirma la observación de [11] de que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto de manera intuitiva de la siguiente manera: Nuestro algoritmo aproxima el conjunto óptimo de consultas a utilizar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las ejecuciones del algoritmo, los rastreadores utilizarán aproximadamente las mismas consultas. 4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y dmoz tienen un límite respectivo de 32,000 y 10,000 en el tamaño de sus resultados, estos límites pueden ser mayores que los impuestos por otros sitios de la Web Oculta. Para investigar cómo un límite más estricto en el tamaño del resultado afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales en el sitio genérico-dmoz: ejecutamos las políticas genérica-frecuencia y adaptativa, pero solo recuperamos hasta los primeros 1,000 resultados para cada consulta. En la Figura 14 trazamos la cobertura de las dos políticas en función del número de consultas. Como era de esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11 donde el límite de resultado era de 10,000, concluimos que el límite más estricto requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados era de 10,000, el pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 Fracción de páginas únicas Número de consulta Fracción acumulada de páginas únicas descargadas por consulta - Sitio web de Dmoz (límite máximo 1000) adaptativo genérico-frecuencia Figura 14: Cobertura de Dmoz general después de limitar el número de resultados a 1,000 icy pudo descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2,600 consultas para descargar el 70% del sitio cuando el límite era de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados ajustado, todavía es posible descargar la mayor parte de un sitio de la Web Oculta después de emitir un número razonable de consultas. La política adaptativa pudo descargar más del 85% del sitio después de emitir 3,500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera consistentemente a la política genérica de frecuencia independientemente del límite de resultados. En tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política genérica de frecuencia para el mismo número de consultas. 4.4 Incorporación del costo de descarga del documento. Para mayor concisión en la presentación, los resultados de la evaluación de rendimiento proporcionados hasta ahora asumieron un modelo de costos simplificado donde cada consulta implicaba un costo constante. En esta sección presentamos resultados sobre el rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para guiar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costos de consulta incluye el costo de enviar la consulta al sitio, recuperar la página de índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos cq = 100, cr = 100 y cd = 10000, como valores para los parámetros de la Ecuación 2, y para el experimento particular que realizamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página de índice de resultados es aproximadamente el mismo, mientras que el costo de descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recurso utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como se evidencia en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que es capaz de descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recurso. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La diferencia más pequeña se debe al hecho de que, bajo la métrica de costos actual, el costo de descarga de documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron la misma cantidad de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Esa 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 Fracción de Páginas Únicas Costo Total (cq=100, cr=100, cd=10000) Fracción acumulativa de páginas únicas descargadas por unidad de costo - Frecuencia adaptativa del sitio web de PubMed Figura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, el ahorro en el costo de la consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Sin embargo, observamos ahorros significativos gracias a la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la póliza adaptativa es aproximadamente 0.5, mientras que la cobertura de la póliza de frecuencia es solo 0.3. TRABAJO RELACIONADO En un estudio reciente, Raghavan y Garcia-Molina [29] presentan un modelo arquitectónico para un rastreador de la Web Oculta. El enfoque principal de este trabajo es aprender interfaces de consulta de la Web Oculta, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por los usuarios o recopiladas de las interfaces de consulta. Por el contrario, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados ha sido utilizada previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso recolectando una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la Web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de múltiples palabras clave que pueden devolver una gran fracción de una colección de documentos. Nuestro trabajo difiere de los estudios anteriores en dos aspectos. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo cual puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo previo. Segundo, aplicamos nuestro marco de trabajo al problema del rastreo de la Web Oculta y demostramos la eficiencia de nuestros algoritmos. Cope et al. [15] proponen un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario al nuestro; una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos utilizar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. La referencia [4] informa sobre métodos para estimar qué fracción de una base de datos de texto se puede adquirir eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de bases de datos de texto grandes. Nuevamente, estos trabajos estudian temas ortogonales y son complementarios a nuestro trabajo. Para hacer que los documentos en múltiples bases de datos textuales sean buscables en un lugar central, se han propuesto varios enfoques de recolección (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen fundamentalmente bases de datos de documentos cooperativas que comparten voluntariamente parte de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos solo son accesibles a través de interfaces de búsqueda. Existe un extenso cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta del usuario [20, 19, 14, 23, 18]. Este conjunto de trabajos se conoce comúnmente como meta-búsqueda o problema de selección de bases de datos en la Web Oculta. Por ejemplo, [19] sugiere el uso de preguntas específicas para clasificar bases de datos en una categoría temática, de modo que, dada una consulta, se pueda seleccionar una base de datos relevante en función de su categoría temática. Nuestra visión es diferente de este cuerpo de trabajo en el sentido de que pretendemos descargar e indexar las páginas ocultas en un lugar central con antelación, para que los usuarios puedan acceder a toda la información a su conveniencia desde un único lugar. 6. CONCLUSIÓN Y TRABAJO FUTURO Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden acceder a las páginas de la Web Oculta que solo son accesibles a través de interfaces de consulta. En este artículo, estudiamos cómo podemos construir un rastreador de la Web Oculta que pueda consultar automáticamente un sitio de la Web Oculta y descargar páginas de él. Propusimos tres políticas diferentes de generación de consultas para la Web Oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico, y una política que elige de forma adaptativa una buena consulta basada en el contenido de las páginas descargadas del sitio de la Web Oculta. La evaluación experimental en 4 sitios reales de la Web Oculta muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dado estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda en la web y la experiencia del usuario en la búsqueda web. 6.1 Trabajo Futuro Discutimos brevemente algunas posibles líneas de investigación futuras. Bases de datos multiatributo Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas multiatributo. Si bien generar consultas para bases de datos de múltiples atributos es claramente un problema más difícil, podemos aprovechar la siguiente observación para abordar este problema: Cuando un sitio admite consultas de múltiples atributos, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de la consulta. Por ejemplo, cuando una librería en línea admite consultas por título, autor e ISBN, las páginas devueltas por una consulta típicamente contienen el título, autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores de cada campo (por ejemplo, título = Harry Potter, autor = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia de cada valor de atributo y seleccionar el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para poder identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitados al presentar múltiples atributos, por ejemplo, la mayoría de los títulos de libros van precedidos por la etiqueta Título:, creemos que podemos aprender reglas de segmentación de páginas automáticamente a partir de un pequeño conjunto de ejemplos de entrenamiento. Otros problemas prácticos Además del problema de generación automática de consultas, hay muchos problemas prácticos que deben abordarse para construir un rastreador de la Hidden-Web completamente automático. Por ejemplo, en este artículo asumimos que el rastreador ya conoce todas las interfaces de consulta para los sitios de la Web Oculta. ¿Pero cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de la Web oculta devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario tiene que hacer clic en un botón de siguiente para ver más resultados. En este caso, un rastreador de la Hidden-Web completamente automático debería saber que la página de índice del primer resultado contiene solo un resultado parcial y presionar automáticamente el botón de siguiente. Finalmente, algunos sitios de la Web oculta pueden contener un número infinito de páginas de la Web oculta que no aportan contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de la Web Oculta debería ser capaz de detectar que el sitio no tiene mucho más contenido nuevo y detener la descarga de páginas del sitio. Los algoritmos de detección de similitud de páginas pueden ser útiles para este propósito [9, 13]. 7. REFERENCIAS [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein y L. Gravano. Consultando bases de datos de texto para una extracción eficiente de información. En ICDE, 2003. [4] E. Agichtein, P. Ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo en New York Times. El antiguo motor de búsqueda, la biblioteca, intenta encajar en un mundo dominado por Google. Disponible en: http://www.nytimes.com/2004/06/21/technology/21LIBR.html, junio de 2004. [6] L. Barbosa y J. Freire. Sifonando datos de la web oculta a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. La web profunda: Descubriendo valor oculto, http://www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En WWW, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Agrupación sintáctica de la web. En WWW, 1997. [10] J. Callan, M. Connell y A. Lo siento, pero necesito una oración completa para poder traducirla. ¿Puedes proporcionar más contexto o una oración completa en inglés para que pueda traducirla al español? Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En SIGMOD, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Sistemas de Información, 19(2):97-130, 2001. [12] K. C.-C. Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: Observaciones e implicaciones. Informe técnico, UIUC. [13] J. Cho, N. Shivakumar y H. Garcia-Molina. Encontrando colecciones web replicadas. En SIGMOD, 2000. [14] W. Cohen y Y. Cantante. Aprendiendo a hacer consultas en la web. En el taller de AAAI sobre Sistemas de Información Basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª conferencia australiana sobre tecnologías de bases de datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los Algoritmos, 2da Edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.\n\nMIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de bases de datos para la web mundial: Una encuesta. SIGMOD Record, 27(3):59-74, 1998. [18] B.\nRegistro de SIGMOD, 27(3):59-74, 1998. [18] B. Él y K. C.-C. Chang. Emparejamiento de esquemas estadísticos en interfaces de consulta web. En la Conferencia SIGMOD, 2003. [19] P. Ipeirotis y L. Gravano. Búsqueda distribuida en la web oculta: Muestreo y selección jerárquica de bases de datos. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Explorar, contar y clasificar: Categorizando bases de datos de la web oculta. En SIGMOD, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Abiertos: Construyendo un marco de interoperabilidad de bajo nivel de barreras en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la World Wide Web. Ciencia, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. Dpro: Un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondas dinámicas. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9: Un servicio de puerta de enlace OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web? La evolución de la web desde la perspectiva de un motor de búsqueda. En WWW, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargando contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de la web? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan y H. Garcia-Molina. Explorando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109\n\nAddison-Wesley, Cambridge, MA, 1949. 109 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "adaptive policy": {
            "translated_key": "política adaptativa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new <br>adaptive policy</br> that approximates the optimal policy.",
                "Our <br>adaptive policy</br> examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our <br>adaptive policy</br> downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The <br>adaptive policy</br>, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the <br>adaptive policy</br>.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the <br>adaptive policy</br>, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the <br>adaptive policy</br> we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the <br>adaptive policy</br> is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the <br>adaptive policy</br> from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the <br>adaptive policy</br> for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the <br>adaptive policy</br> is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The <br>adaptive policy</br> could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our <br>adaptive policy</br> consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our <br>adaptive policy</br> shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the <br>adaptive policy</br> makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the <br>adaptive policy</br> is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the <br>adaptive policy</br>.",
                "At the total cost of 8000, for example, the coverage of the <br>adaptive policy</br> is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the <br>adaptive policy</br> can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new <br>adaptive policy</br> that approximates the optimal policy.",
                "Our <br>adaptive policy</br> examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "In one experiment, for example, our <br>adaptive policy</br> downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "The <br>adaptive policy</br>, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the <br>adaptive policy</br>."
            ],
            "translated_annotated_samples": [
                "Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva <br>política adaptativa</br> que aproxima la política óptima.",
                "Nuestra <br>política adaptativa</br> examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales.",
                "En un experimento, por ejemplo, nuestra <br>política adaptativa</br> descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas.",
                "La <br>política adaptativa</br>, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico.",
                "Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la <br>política adaptativa</br>."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva <br>política adaptativa</br> que aproxima la política óptima. Nuestra <br>política adaptativa</br> examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra <br>política adaptativa</br> descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La <br>política adaptativa</br>, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la <br>política adaptativa</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "hide web crawl": {
            "translated_key": "ocultar rastreo web",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "deep web crawler": {
            "translated_key": "rastreador de la web profunda",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "adaptive algorithm": {
            "translated_key": "algoritmo adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this query selection problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal query selection problem. 2.3 Problem formalization Theoretically, the problem of query selection can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our query selection algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our query selection algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the query selection problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 Query selection algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the query selection function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the query selection method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The <br>adaptive algorithm</br> learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the <br>adaptive algorithm</br> to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the <br>adaptive algorithm</br> issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The <br>adaptive algorithm</br>, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the <br>adaptive algorithm</br>, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the <br>adaptive algorithm</br>, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the <br>adaptive algorithm</br> performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the <br>adaptive algorithm</br> affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the <br>adaptive algorithm</br> using different initial queries for crawling the PubMed Web site <br>adaptive algorithm</br> itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our query selection process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "The <br>adaptive algorithm</br> learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the <br>adaptive algorithm</br> to drive the selection of new keywords for querying.",
                "For example, for the PubMed site (Figure 9), the <br>adaptive algorithm</br> issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "The <br>adaptive algorithm</br>, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the <br>adaptive algorithm</br>, but not by the other policies."
            ],
            "translated_annotated_samples": [
                "El <br>algoritmo adaptativo</br> aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2.",
                "Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al <br>algoritmo adaptativo</br> para impulsar la selección de nuevas palabras clave para la consulta.",
                "Por ejemplo, para el sitio de PubMed (Figura 9), el <br>algoritmo adaptativo</br> emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura.",
                "El <br>algoritmo adaptativo</br>, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados.",
                "La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el <br>algoritmo adaptativo</br>, pero no por las otras políticas."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de selección de consultas puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de selección de consultas El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. Dado este objetivo, el rastreador de la Web oculta debe tener en cuenta dos factores. (1) el número de nuevos documentos que se pueden obtener de la consulta qi y (2) el costo de emitir la consulta qi. Por ejemplo, si dos consultas, qi y qj, incurren en el mismo costo, pero qi devuelve más páginas nuevas que qj, qi es más deseable que qj. Del mismo modo, si qi y qj devuelven la misma cantidad de documentos nuevos, pero qi incurre en menos costos que qj, qi es más deseable. Basándose en esta observación, el rastreador de la Web oculta puede utilizar la siguiente métrica de eficiencia para cuantificar la deseabilidad de la consulta qi: Eficiencia(qi) = Pnuevos(qi) Costo(qi) Aquí, Pnuevos(qi) representa la cantidad de documentos nuevos devueltos para qi (las páginas que no han sido devueltas en consultas anteriores). Cost(qi) representa el costo de emitir la consulta qi. De manera intuitiva, la eficiencia de qi mide cuántos documentos nuevos se recuperan por unidad de costo, y puede utilizarse como indicador de 4. Para una estimación exacta, necesitamos conocer el número total de páginas en el sitio. Sin embargo, para comparar solo los valores relativos entre las consultas, esta información en realidad no es necesaria. 103 ALGORITMO 3.1. SelectTerm() codicioso Parámetros: T: La lista de palabras clave potenciales del query Procedimiento (1) Para cada tk en T hacer (2) Estimar Eficiencia(tk) = Pnew(tk) Costo(tk) (3) hecho (4) devolver tk con la máxima Eficiencia(tk) Figura 6: Algoritmo para seleccionar el siguiente término del query. qué tan bien se gastan nuestros recursos al emitir qi. Por lo tanto, el rastreador de la Web Oculta puede estimar la eficiencia de cada candidato qi y seleccionar el que tenga el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consulta que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta qi, Pnew(qi), es Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) de la Ecuación 3, donde P(qi) puede estimarse utilizando uno de los métodos descritos en la sección 3. También podemos estimar Cost(qi) de manera similar. Por ejemplo, si Cost(qi) es Cost(qi) = cq + crP(qi) + cdPnew(qi) (Ecuación 2), podemos estimar Cost(qi) estimando P(qi) y Pnew(qi). 3.3 Cálculo eficiente de estadísticas de consulta. Al estimar la eficiencia de las consultas, encontramos que necesitamos medir P(qi|q1∨· · ·∨qi−1) para cada consulta potencial qi. Este cálculo puede ser muy consumidor de tiempo si lo repetimos desde cero para cada consulta qi en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P(qi|q1 ∨ · · · ∨ qi−1) de manera eficiente manteniendo una pequeña tabla que llamamos tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P(qi|q1 ∨· · ·∨ qi−1) se puede medir contando cuántas veces aparece la palabra clave qi dentro de los documentos descargados de q1, . . . , qi−1. Registramos estos conteos en una tabla, como se muestra en la Figura 7(a). La columna izquierda de la tabla contiene todos los posibles términos de consulta y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7(a) muestra que hasta ahora hemos descargado 50 documentos, y el término \"modelo\" aparece en 10 de estos documentos. Dado este número, podemos calcular que P(modelo|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consultas debe actualizarse cada vez que emitimos una nueva consulta qi y descargamos más documentos. Esta actualización se puede realizar de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consultas de la Figura 7(a), hemos decidido usar el término computadora como nuestra próxima consulta qi. Desde la nueva consulta qi = computadora, descargamos 20 páginas nuevas adicionales. De estos, 12 contienen la palabra clave modelo Término tk N(tk) modelo 10 computadora 38 digital 50 Término tk N(tk) modelo 12 computadora 20 disco 18 Páginas totales: 50 Nuevas páginas: 20 (a) Después de q1, . . . , qi−1 (b) Nuevas desde qi = computadora Término tk N(tk) modelo 10+12 = 22 computadora 38+20 = 58 disco 0+18 = 18 digital 50+0 = 50 Páginas totales: 50 + 20 = 70 (c) Después de q1, . . . , qi Figura 7: Actualización de la tabla de estadísticas de consulta. q i1 i−1 q\\/ ... \\/q q i / S Figura 8: Un sitio web que no devuelve todos los resultados. y 18 la palabra clave disco. La tabla en la Figura 7(b) muestra la frecuencia de cada término en las páginas recién descargadas. Podemos actualizar la antigua tabla (Figura 7(a)) para incluir esta nueva información simplemente añadiendo las entradas correspondientes en las Figuras 7(a) y (b). El resultado se muestra en la Figura 7(c). Por ejemplo, el modelo de palabra clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de q1, . . . , qi. Según esta nueva tabla, P(modelo|q1∨· · ·∨qi) es ahora 22 70 = 0.3. 3.4 Rastreo de sitios que limitan el número de resultados. En ciertos casos, cuando una consulta coincide con un gran número de páginas, el sitio de la Web Oculta devuelve solo una parte de esas páginas. Por ejemplo, el Proyecto Directorio Abierto [2] permite a los usuarios ver solo hasta 10,000 resultados después de realizar una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador de la Web Oculta. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. Segundo, el método de selección de consultas que presentamos en la Sección 3.2 asume que para cada consulta potencial qi, podemos encontrar P(qi|q1 ∨ · · · ∨ qi−1). Es decir, para cada consulta qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene qi con al menos uno de q1, . . . , qi−1. Sin embargo, si la base de datos de texto devuelve solo una parte de los resultados para cualquiera de los q1, . . . , qi−1, entonces el valor P(qi|q1 ∨ · · · ∨ qi−1) no es preciso y puede afectar nuestra decisión para la siguiente consulta qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo permitido por el sitio web, nuestro rastreador no tiene otra opción que enviar más consultas. Sin embargo, hay una forma de estimar el valor correcto de P(qi|q1 ∨ · · · ∨ qi−1) en el caso en que el sitio web devuelva solo una parte de los resultados. 104 De nuevo, suponga que el sitio web oculto que estamos rastreando actualmente está representado como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Supongamos que ya hemos emitido las consultas q1, . . . , qi−1 que devolvieron un número de resultados menor que el número máximo permitido por el sitio, y por lo tanto hemos descargado todas las páginas de estas consultas (círculo grande en la Figura 8). Eso es, en este punto, nuestra estimación para P(qi|q1 ∨· · ·∨qi−1) es precisa. Ahora supongamos que enviamos la consulta qi al sitio web, pero debido a una limitación en el número de resultados que recibimos, recuperamos el conjunto qi (círculo pequeño en la Figura 8) en lugar del conjunto qi (círculo punteado en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consultas para que tenga información precisa para el próximo paso. Es decir, aunque recuperamos el conjunto qi, para cada posible consulta qi+1 necesitamos encontrar P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) En la ecuación anterior, podemos encontrar P(q1 ∨· · ·∨qi) estimando P(qi) con el método mostrado en la Sección 3. Además, podemos calcular P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) y P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) examinando directamente los documentos que hemos descargado de las consultas q1, . . . , qi−1. El término P(qi+1 ∧ qi) sin embargo es desconocido y necesitamos estimarlo. Suponiendo que qi es una muestra aleatoria de qi, entonces: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) A partir de la Ecuación 6 podemos calcular P(qi+1 ∧ qi) y después de reemplazar este valor en la Ecuación 5 podemos encontrar P(qi+1|q1 ∨ · · · ∨ qi). 4. EVALUACIÓN EXPERIMENTAL En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo de la Web Oculta presentados en este artículo. Nuestro objetivo es validar nuestro análisis teórico a través de experimentos en el mundo real, rastreando sitios populares de la Hidden Web que contienen bases de datos de texto. Dado que el número de documentos que se descubren y descargan de una base de datos textual depende de la selección de las palabras que se emitirán como consultas a la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la sección 3, a saber, los algoritmos aleatorio, de frecuencia genérica y adaptativo. El <br>algoritmo adaptativo</br> aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está guiado por un modelo de costos como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simples en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más adelante, en la Sección 4.4 presentaremos una comparación de nuestras políticas basada en un modelo de costos más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P(qi) a partir de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica. Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un corpus de 5.5 millones de páginas web. A lo largo de nuestros experimentos, una vez que un algoritmo ha enviado una consulta a una base de datos, excluimos la consulta de envíos posteriores a la misma base de datos por parte del mismo algoritmo. Dejamos la presentación de resultados basados en la estimación de Zipf para un trabajo futuro, descargado de 154 sitios web de diversos temas. Las palabras clave se seleccionan en función de su frecuencia decreciente en la que aparecen en este conjunto de documentos, seleccionando primero la más frecuente, seguida de la segunda palabra clave más frecuente, etc. En cuanto a la política aleatoria, utilizamos el mismo conjunto de palabras recopiladas del corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista de términos de consulta potenciales afecta al algoritmo basado en azar, construimos dos conjuntos: uno con las 16,000 palabras más frecuentes de la colección de términos utilizada en la política de frecuencia genérica (en adelante, la política aleatoria con el conjunto de 16,000 palabras se referirá como aleatorio-16K), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección mencionada anteriormente (en adelante, referido como aleatorio-1M). El conjunto anterior tiene palabras frecuentes que aparecen en un gran número de documentos (al menos 10,000 en nuestra colección), y por lo tanto pueden considerarse términos de alta calidad. El último conjunto, sin embargo, contiene una colección mucho más grande de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se llevaron a cabo empleando cada uno de los algoritmos mencionados anteriormente (adaptativo, de frecuencia genérica, aleatorio de 16K y aleatorio de 1M) para rastrear y descargar contenidos de tres sitios de la Web Oculta: la Biblioteca Médica PubMed, Amazon y el Proyecto Directorio Abierto. Según la información en el sitio web de PubMed, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada para el algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante consultas repetidas a la interfaz de consulta web proporcionada por PubMed. La exploración de la Hidden Web en el sitio web de PubMed puede considerarse como específica del tema, debido a que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre libros. La consulta a Amazon se realiza a través del Kit de Desarrolladores de Software que Amazon proporciona para interactuar con su sitio web, y que devuelve resultados en forma de XML. El campo de palabra clave genérica se utiliza para la búsqueda, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las reseñas de los clientes cuando están presentes en la respuesta XML. Dado que Amazon no proporciona información sobre cuántos libros tiene en su catálogo, utilizamos muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10,000 números de ISBN aleatorios consultados, se encontraron 46 en el catálogo de Amazon, por lo tanto, se estima que el tamaño de su colección de libros es de 4.6 millones de libros. También vale la pena señalar aquí que Amazon impone un límite superior en el número de resultados (libros en nuestro caso) devueltos por cada consulta, que está establecido en 32,000. En cuanto al tercer sitio web oculto, el Proyecto Directorio Abierto (también conocido como dmoz en adelante), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio listado. Los enlaces son buscables a través de una interfaz de búsqueda por palabras clave. Consideramos cada enlace indexado junto con su breve resumen como el documento del sitio dmoz, y proporcionamos los resúmenes cortos al <br>algoritmo adaptativo</br> para impulsar la selección de nuevas palabras clave para la consulta. En el sitio web de dmoz, realizamos dos rastreos de la Web Oculta: el primero es en su colección genérica de 3.8 millones de índices 7. No excluimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto PubMed, devuelven documentos coincidentes para las palabras de parada, como \"the\".  El otro rastreo se realiza específicamente en la sección de Artes de dmoz (http:// dmoz.org/Arts), que comprende aproximadamente 429,000 sitios indexados relevantes para las Artes, lo que hace que este rastreo sea específico en temas, al igual que en PubMed. Al igual que Amazon, dmoz también impone un límite superior en el número de resultados devueltos, que son 10,000 enlaces con sus resúmenes. 4.1 Comparación de políticas. La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que enviamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio Hidden Web podemos descargar mientras consultamos continuamente nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P(q1 ∨ · · · ∨ qi−1 ∨ qi), después de enviar las consultas q1, . . . , qi, y a medida que i aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, dmoz general y dmoz específico de arte, respectivamente. En el eje y se representa la fracción de los documentos totales descargados del sitio web, mientras que el eje x representa el número de consulta. Una primera observación de estos gráficos es que, en general, las políticas de frecuencia genérica y adaptativa funcionan mucho mejor que los algoritmos basados en azar. En todas las figuras, las gráficas para el random-1M y el random-16K están significativamente por debajo de las de otras políticas. Entre las políticas de frecuencia genérica y adaptativa, podemos ver que esta última supera a la primera cuando el sitio es específico en cuanto al tema. Por ejemplo, para el sitio de PubMed (Figura 9), el <br>algoritmo adaptativo</br> emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérico requiere 106 consultas para la misma cobertura. Para el rastreo de dmoz/Arts (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% de los sitios totales indexados en el Directorio emitiendo 471 consultas, mientras que el algoritmo basado en frecuencia es mucho menos efectivo utilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El <br>algoritmo adaptativo</br>, al examinar el contenido de las páginas que descarga en cada iteración, es capaz de identificar el tema del sitio según las palabras que aparecen con mayor frecuencia en el conjunto de resultados. Por consiguiente, es capaz de seleccionar palabras para consultas posteriores que son más relevantes para el sitio que las preferidas por la política de frecuencia genérica, las cuales se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de las 211 seleccionadas y enviadas al sitio web de PubMed por el <br>algoritmo adaptativo</br>, pero no por las otras políticas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query selection": {
            "translated_key": "selección de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Downloading Textual Hidden Web Content Through Keyword Queries Alexandros Ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu Junghoo Cho UCLA Computer Science cho@cs.ucla.edu ABSTRACT An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites.",
                "These pages are often referred to as the Hidden Web or the Deep Web.",
                "Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results.",
                "However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.",
                "In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web.",
                "Since the only entry point to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site.",
                "Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically.",
                "Our policies proceed iteratively, issuing a different query in every iteration.",
                "We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising.",
                "For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.",
                "Categories and Subject Descriptors: H.3.7 [Information Systems]: Digital Libraries; H.3.1 [Information Systems]: Content Analysis and Indexing; H.3.3 [Information Systems]: Information Search and Retrieval.",
                "General Terms: Algorithms, Performance, Design. 1.",
                "INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].",
                "In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.",
                "These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).",
                "According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].",
                "In [12], Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on the Web.",
                "Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].",
                "For example, PubMed hosts many high-quality papers on medical research that were selected from careful peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping potential inventors examine prior art.",
                "In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.",
                "Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).",
                "We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.",
                "Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.",
                "Unless users go directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites. • Improving user experience: Even if a user is aware of a number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.",
                "By making the Hidden-Web pages searchable at a central location, we can significantly reduce the users wasted time and effort in searching the Hidden Web. • Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].",
                "Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].",
                "According to a recent article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.",
                "Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines. 100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for Hidden Web sites with textual content, thus minimizing the associated costs and effort required.",
                "Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.",
                "The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search interfaces was presented.",
                "Here, we present a solution to the second challenge, i.e. how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.",
                "Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.",
                "We exhaustively issue all possible queries, one query at a time.",
                "When the query forms have a free text input, however, an infinite number of queries are possible, so we cannot exhaustively issue all possible queries.",
                "In this case, what queries should we pick?",
                "Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?",
                "In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.",
                "We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.",
                "In summary, this paper makes the following contributions: • We present a formal framework to study the problem of HiddenWeb crawling. (Section 2). • We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.",
                "Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2). • We propose a new adaptive policy that approximates the optimal policy.",
                "Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy automatically based on them (Section 3). • We evaluate various crawling policies through experiments on real Web sites.",
                "Our experiments will show the relative advantages of various crawling policies and demonstrate their potential.",
                "The results from our experiments are very promising.",
                "In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 2.",
                "FRAMEWORK In this section, we present a formal framework for the study of the Hidden-Web crawling problem.",
                "In Section 2.1, we describe our assumptions on Hidden-Web sites and explain how users interact with the sites.",
                "Based on this interaction model, we present a highlevel algorithm for a Hidden-Web crawler in Section 2.2.",
                "Finally in Section 2.3, we formalize the Hidden-Web crawling problem. 2.1 Hidden-Web database model There exists a variety of Hidden Web sources that provide information on a multitude of topics.",
                "Depending on the type of information, we may categorize a Hidden-Web site either as a textual database or a structured database.",
                "A textual database is a site that Figure 2: A multi-attribute search interface mainly contains plain-text documents, such as PubMed and LexisNexis (an online database of legal documents [1]).",
                "Since plaintext documents do not usually have well-defined structure, most textual databases provide a simple search interface where users type a list of keywords in a single search box (Figure 1).",
                "In contrast, a structured database often contains multi-attribute relational data (e.g., a book on the Amazon Web site may have the fields title=Harry Potter, author=J.K.",
                "Rowling and isbn=0590353403) and supports multi-attribute search interfaces (Figure 2).",
                "In this paper, we will mainly focus on textual databases that support single-attribute keyword queries.",
                "We discuss how we can extend our ideas for the textual databases to multi-attribute structured databases in Section 6.1.",
                "Typically, the users need to take the following steps in order to access pages in a Hidden-Web database: 1.",
                "Step 1.",
                "First, the user issues a query, say liver, through the search interface provided by the Web site (such as the one shown in Figure 1). 2.",
                "Step 2.",
                "Shortly after the user issues the query, she is presented with a result index page.",
                "That is, the Web site returns a list of links to potentially relevant Web pages, as shown in Figure 3(a). 3.",
                "Step 3.",
                "From the list in the result index page, the user identifies the pages that look interesting and follows the links.",
                "Clicking on a link leads the user to the actual Web page, such as the one shown in Figure 3(b), that the user wants to look at. 2.2 A generic Hidden Web crawling algorithm Given that the only entry to the pages in a Hidden-Web site is its search from, a Hidden-Web crawler should follow the three steps described in the previous section.",
                "That is, the crawler has to generate a query, issue it to the Web site, download the result index page, and follow the links to download the actual pages.",
                "In most cases, a crawler has limited time and network resources, so the crawler repeats these steps until it uses up its resources.",
                "In Figure 4 we show the generic algorithm for a Hidden-Web crawler.",
                "For simplicity, we assume that the Hidden-Web crawler issues single-term queries only.3 The crawler first decides which query term it is going to use (Step (2)), issues the query, and retrieves the result index page (Step (3)).",
                "Finally, based on the links found on the result index page, it downloads the Hidden Web pages from the site (Step (4)).",
                "This same process is repeated until all the available resources are used up (Step (1)).",
                "Given this algorithm, we can see that the most critical decision that a crawler has to make is what query to issue next.",
                "If the crawler can issue successful queries that will return many matching pages, the crawler can finish its crawling early on using minimum resources.",
                "In contrast, if the crawler issues completely irrelevant queries that do not return any matching pages, it may waste all of its resources simply issuing queries without ever retrieving actual pages.",
                "Therefore, how the crawler selects the next query can greatly affect its effectiveness.",
                "In the next section, we formalize this <br>query selection</br> problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "Extending our work to multi-keyword queries is straightforward. 101 (a) List of matching pages for query liver. (b) The first matching page for liver.",
                "Figure 3: Pages from the PubMed Web site.",
                "ALGORITHM 2.1.",
                "Crawling a Hidden Web site Procedure (1) while ( there are available resources ) do // select a term to send to the site (2) qi = SelectTerm() // send query and acquire result index page (3) R(qi) = QueryWebSite( qi ) // download the pages of interest (4) Download( R(qi) ) (5) done Figure 4: Algorithm for crawling a Hidden Web site.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal <br>query selection</br> problem. 2.3 Problem formalization Theoretically, the problem of <br>query selection</br> can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "We represent each Web page in S as a point (dots in Figure 5).",
                "Every potential query qi that we may issue can be viewed as a subset of S, containing all the points (pages) that are returned when we issue qi to the site.",
                "Each subset is associated with a weight that represents the cost of issuing the query.",
                "Under this formalization, our goal is to find which subsets (queries) cover the maximum number of points (Web pages) with the minimum total weight (cost).",
                "This problem is equivalent to the set-covering problem in graph theory [16].",
                "There are two main difficulties that we need to address in this formalization.",
                "First, in a practical situation, the crawler does not know which Web pages will be returned by which queries, so the subsets of S are not known in advance.",
                "Without knowing these subsets the crawler cannot decide which queries to pick to maximize the coverage.",
                "Second, the set-covering problem is known to be NP-Hard [16], so an efficient algorithm to solve this problem optimally in polynomial time has yet to be found.",
                "In this paper, we will present an approximation algorithm that can find a near-optimal solution at a reasonable computational cost.",
                "Our algorithm leverages the observation that although we do not know which pages will be returned by each query qi that we issue, we can predict how many pages will be returned.",
                "Based on this information our <br>query selection</br> algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our <br>query selection</br> algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the <br>query selection</br> problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "Given a query qi, we use P(qi) to denote the fraction of pages that we will get back if we issue query qi to the site.",
                "For example, if a Web site has 10,000 pages in total, and if 3,000 pages are returned for the query qi = medicine, then P(qi) = 0.3.",
                "We use P(q1 ∧ q2) to represent the fraction of pages that are returned from both q1 and q2 (i.e., the intersection of P(q1) and P(q2)).",
                "Similarly, we use P(q1 ∨ q2) to represent the fraction of pages that are returned from either q1 or q2 (i.e., the union of P(q1) and P(q2)).",
                "We also use Cost(qi) to represent the cost of issuing the query qi.",
                "Depending on the scenario, the cost can be measured either in time, network bandwidth, the number of interactions with the site, or it can be a function of all of these.",
                "As we will see later, our proposed algorithms are independent of the exact cost function.",
                "In the most common case, the query cost consists of a number of factors, including the cost for submitting the query to the site, retrieving the result index page (Figure 3(a)) and downloading the actual pages (Figure 3(b)).",
                "We assume that submitting a query incurs a fixed cost of cq.",
                "The cost for downloading the result index page is proportional to the number of matching documents to the query, while the cost cd for downloading a matching document is also fixed.",
                "Then the overall cost of query qi is Cost(qi) = cq + crP(qi) + cdP(qi). (1) In certain cases, some of the documents from qi may have already been downloaded from previous queries.",
                "In this case, the crawler may skip downloading these documents and the cost of qi can be Cost(qi) = cq + crP(qi) + cdPnew(qi). (2) Here, we use Pnew(qi) to represent the fraction of the new documents from qi that have not been retrieved from previous queries.",
                "Later in Section 3.1 we will study how we can estimate P(qi) and Pnew(qi) to estimate the cost of qi.",
                "Since our algorithms are independent of the exact cost function, we will assume a generic cost function Cost(qi) in this paper.",
                "When we need a concrete cost function, however, we will use Equation 2.",
                "Given the notation, we can formalize the goal of a Hidden-Web crawler as follows: 102 PROBLEM 1.",
                "Find the set of queries q1, . . . , qn that maximizes P(q1 ∨ · · · ∨ qn) under the constraint n i=1 Cost(qi) ≤ t. Here, t is the maximum download resource that the crawler has. 3.",
                "KEYWORD SELECTION How should a crawler select the queries to issue?",
                "Given that the goal is to download the maximum number of unique documents from a textual database, we may consider one of the following options: • Random: We select random keywords from, say, an English dictionary and issue them to the database.",
                "The hope is that a random query will return a reasonable number of matching documents. • Generic-frequency: We analyze a generic document corpus collected elsewhere (say, from the Web) and obtain the generic frequency distribution of each keyword.",
                "Based on this generic distribution, we start with the most frequent keyword, issue it to the Hidden-Web database and retrieve the result.",
                "We then continue to the second-most frequent keyword and repeat this process until we exhaust all download resources.",
                "The hope is that the frequent keywords in a generic corpus will also be frequent in the Hidden-Web database, returning many matching documents. • Adaptive: We analyze the documents returned from the previous queries issued to the Hidden-Web database and estimate which keyword is most likely to return the most documents.",
                "Based on this analysis, we issue the most promising query, and repeat the process.",
                "Among these three general policies, we may consider the random policy as the base comparison point since it is expected to perform the worst.",
                "Between the generic-frequency and the adaptive policies, both policies may show similar performance if the crawled database has a generic document collection without a specialized topic.",
                "The adaptive policy, however, may perform significantly better than the generic-frequency policy if the database has a very specialized collection that is different from the generic corpus.",
                "We will experimentally compare these three policies in Section 4.",
                "While the first two policies (random and generic-frequency policies) are easy to implement, we need to understand how we can analyze the downloaded pages to identify the most promising query in order to implement the adaptive policy.",
                "We address this issue in the rest of this section. 3.1 Estimating the number of matching pages In order to identify the most promising query, we need to estimate how many new documents we will download if we issue the query qi as the next query.",
                "That is, assuming that we have issued queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for every potential next query qi and compare this value.",
                "In estimating this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi) as: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) In the above formula, note that we can precisely measure P(q1 ∨ · · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of all pages downloaded from q1, . . . , qi−1, since we have already issued q1, . . . , qi−1 and downloaded the matching pages.4 We can also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi appears in the pages from q1, . . . , qi−1, by counting how many times qi appears in the pages from q1, . . . , qi−1.",
                "Therefore, we only need to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi).",
                "We may consider a number of different ways to estimate P(qi), including the following: 1.",
                "Independence estimator: We assume that the appearance of the term qi is independent of the terms q1, . . . , qi−1.",
                "That is, we assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2.",
                "Zipf estimator: In [19], Ipeirotis et al. proposed a method to estimate how many times a particular term occurs in the entire corpus based on a subset of documents from the corpus.",
                "Their method exploits the fact that the frequency of terms inside text collections follows a power law distribution [30, 25].",
                "That is, if we rank all terms based on their occurrence frequency (with the most frequent term having a rank of 1, second most frequent a rank of 2 etc. ), then the frequency f of a term inside the text collection is given by: f = α(r + β)−γ (4) where r is the rank of the term and α, β, and γ are constants that depend on the text collection.",
                "Their main idea is (1) to estimate the three parameters, α, β and γ, based on the subset of documents that we have downloaded from previous queries, and (2) use the estimated parameters to predict f given the ranking r of a term within the subset.",
                "For a more detailed description on how we can use this method to estimate P(qi), we refer the reader to the extended version of this paper [27].",
                "After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we can calculate P(q1 ∨ · · · ∨ qi).",
                "In Section 3.3, we explain how we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a succinct summary table.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 <br>query selection</br> algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources.",
                "Given this goal, the Hidden-Web crawler has to take two factors into account. (1) the number of new documents that can be obtained from the query qi and (2) the cost of issuing the query qi.",
                "For example, if two queries, qi and qj, incur the same cost, but qi returns more new pages than qj, qi is more desirable than qj.",
                "Similarly, if qi and qj return the same number of new documents, but qi incurs less cost then qj, qi is more desirable.",
                "Based on this observation, the Hidden-Web crawler may use the following efficiency metric to quantify the desirability of the query qi: Efficiency(qi) = Pnew(qi) Cost(qi) Here, Pnew(qi) represents the amount of new documents returned for qi (the pages that have not been returned for previous queries).",
                "Cost(qi) represents the cost of issuing the query qi.",
                "Intuitively, the efficiency of qi measures how many new documents are retrieved per unit cost, and can be used as an indicator of 4 For exact estimation, we need to know the total number of pages in the site.",
                "However, in order to compare only relative values among queries, this information is not actually needed. 103 ALGORITHM 3.1.",
                "Greedy SelectTerm() Parameters: T: The list of potential query keywords Procedure (1) Foreach tk in T do (2) Estimate Efficiency(tk) = Pnew(tk) Cost(tk) (3) done (4) return tk with maximum Efficiency(tk) Figure 6: Algorithm for selecting the next query term. how well our resources are spent when issuing qi.",
                "Thus, the Hidden Web crawler can estimate the efficiency of every candidate qi, and select the one with the highest value.",
                "By using its resources more efficiently, the crawler may eventually download the maximum number of unique documents.",
                "In Figure 6, we show the <br>query selection</br> function that uses the concept of efficiency.",
                "In principle, this algorithm takes a greedy approach and tries to maximize the potential gain in every step.",
                "We can estimate the efficiency of every query using the estimation method described in Section 3.1.",
                "That is, the size of the new documents from the query qi, Pnew(qi), is Pnew(qi) = P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1) = P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) from Equation 3, where P(qi) can be estimated using one of the methods described in section 3.",
                "We can also estimate Cost(qi) similarly.",
                "For example, if Cost(qi) is Cost(qi) = cq + crP(qi) + cdPnew(qi) (Equation 2), we can estimate Cost(qi) by estimating P(qi) and Pnew(qi). 3.3 Efficient calculation of query statistics In estimating the efficiency of queries, we found that we need to measure P(qi|q1∨· · ·∨qi−1) for every potential query qi.",
                "This calculation can be very time-consuming if we repeat it from scratch for every query qi in every iteration of our algorithm.",
                "In this section, we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently by maintaining a small table that we call a query statistics table.",
                "The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨ qi−1) can be measured by counting how many times the keyword qi appears within the documents downloaded from q1, . . . , qi−1.",
                "We record these counts in a table, as shown in Figure 7(a).",
                "The left column of the table contains all potential query terms and the right column contains the number of previously-downloaded documents containing the respective term.",
                "For example, the table in Figure 7(a) shows that we have downloaded 50 documents so far, and the term model appears in 10 of these documents.",
                "Given this number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10 50 = 0.2.",
                "We note that the query statistics table needs to be updated whenever we issue a new query qi and download more documents.",
                "This update can be done efficiently as we illustrate in the following example.",
                "EXAMPLE 1.",
                "After examining the query statistics table of Figure 7(a), we have decided to use the term computer as our next query qi.",
                "From the new query qi = computer, we downloaded 20 more new pages.",
                "Out of these, 12 contain the keyword model Term tk N(tk) model 10 computer 38 digital 50 Term tk N(tk) model 12 computer 20 disk 18 Total pages: 50 New pages: 20 (a) After q1, . . . , qi−1 (b) New from qi = computer Term tk N(tk) model 10+12 = 22 computer 38+20 = 58 disk 0+18 = 18 digital 50+0 = 50 Total pages: 50 + 20 = 70 (c) After q1, . . . , qi Figure 7: Updating the query statistics table. q i1 i−1 q\\/ ... \\/q q i / S Figure 8: A Web site that does not return all the results. and 18 the keyword disk.",
                "The table in Figure 7(b) shows the frequency of each term in the newly-downloaded pages.",
                "We can update the old table (Figure 7(a)) to include this new information by simply adding corresponding entries in Figures 7(a) and (b).",
                "The result is shown on Figure 7(c).",
                "For example, keyword model exists in 10 + 12 = 22 pages within the pages retrieved from q1, . . . , qi.",
                "According to this new table, P(model|q1∨· · ·∨qi) is now 22 70 = 0.3. 3.4 Crawling sites that limit the number of results In certain cases, when a query matches a large number of pages, the Hidden Web site returns only a portion of those pages.",
                "For example, the Open Directory Project [2] allows the users to see only up to 10, 000 results after they issue a query.",
                "Obviously, this kind of limitation has an immediate effect on our Hidden Web crawler.",
                "First, since we can only retrieve up to a specific number of pages per query, our crawler will need to issue more queries (and potentially will use up more resources) in order to download all the pages.",
                "Second, the <br>query selection</br> method that we presented in Section 3.2 assumes that for every potential query qi, we can find P(qi|q1 ∨ · · · ∨ qi−1).",
                "That is, for every query qi we can find the fraction of documents in the whole text database that contains qi with at least one of q1, . . . , qi−1.",
                "However, if the text database returned only a portion of the results for any of the q1, . . . , qi−1 then the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our decision for the next query qi, and potentially the performance of our crawler.",
                "Since we cannot retrieve more results per query than the maximum number the Web site allows, our crawler has no other choice besides submitting more queries.",
                "However, there is a way to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case where the Web site returns only a portion of the results. 104 Again, assume that the Hidden Web site we are currently crawling is represented as the rectangle on Figure 8 and its pages as points in the figure.",
                "Assume that we have already issued queries q1, . . . , qi−1 which returned a number of results less than the maximum number than the site allows, and therefore we have downloaded all the pages for these queries (big circle in Figure 8).",
                "That is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.",
                "Now assume that we submit query qi to the Web site, but due to a limitation in the number of results that we get back, we retrieve the set qi (small circle in Figure 8) instead of the set qi (dashed circle in Figure 8).",
                "Now we need to update our query statistics table so that it has accurate information for the next step.",
                "That is, although we got the set qi back, for every potential query qi+1 we need to find P(qi+1|q1 ∨ · · · ∨ qi): P(qi+1|q1 ∨ · · · ∨ qi) = 1 P(q1 ∨ · · · ∨ qi) · [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+ P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5) In the previous equation, we can find P(q1 ∨· · ·∨qi) by estimating P(qi) with the method shown in Section 3.",
                "Additionally, we can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1)) by directly examining the documents that we have downloaded from queries q1, . . . , qi−1.",
                "The term P(qi+1 ∧ qi) however is unknown and we need to estimate it.",
                "Assuming that qi is a random sample of qi, then: P(qi+1 ∧ qi) P(qi+1 ∧ qi) = P(qi) P(qi) (6) From Equation 6 we can calculate P(qi+1 ∧ qi) and after we replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi). 4.",
                "EXPERIMENTAL EVALUATION In this section we experimentally evaluate the performance of the various algorithms for Hidden Web crawling presented in this paper.",
                "Our goal is to validate our theoretical analysis through realworld experiments, by crawling popular Hidden Web sites of textual databases.",
                "Since the number of documents that are discovered and downloaded from a textual database depends on the selection of the words that will be issued as queries5 to the search interface of each site, we compare the various selection policies that were described in section 3, namely the random, generic-frequency, and adaptive algorithms.",
                "The adaptive algorithm learns new keywords and terms from the documents that it downloads, and its selection process is driven by a cost model as described in Section 3.2.",
                "To keep our experiment and its analysis simple at this point, we will assume that the cost for every query is constant.",
                "That is, our goal is to maximize the number of downloaded pages by issuing the least number of queries.",
                "Later, in Section 4.4 we will present a comparison of our policies based on a more elaborate cost model.",
                "In addition, we use the independence estimator (Section 3.1) to estimate P(qi) from downloaded pages.",
                "Although the independence estimator is a simple estimator, our experiments will show that it can work very well in practice.6 For the generic-frequency policy, we compute the frequency distribution of words that appear in a 5.5-million-Web-page corpus 5 Throughout our experiments, once an algorithm has submitted a query to a database, we exclude the query from subsequent submissions to the same database from the same algorithm. 6 We defer the reporting of results based on the Zipf estimation to a future work. downloaded from 154 Web sites of various topics [26].",
                "Keywords are selected based on their decreasing frequency with which they appear in this document set, with the most frequent one being selected first, followed by the second-most frequent keyword, etc.7 Regarding the random policy, we use the same set of words collected from the Web corpus, but in this case, instead of selecting keywords based on their relative frequency, we choose them randomly (uniform distribution).",
                "In order to further investigate how the quality of the potential query-term list affects the random-based algorithm, we construct two sets: one with the 16, 000 most frequent words of the term collection used in the generic-frequency policy (hereafter, the random policy with the set of 16,000 words will be referred to as random-16K), and another set with the 1 million most frequent words of the same collection as above (hereafter, referred to as random-1M).",
                "The former set has frequent words that appear in a large number of documents (at least 10, 000 in our collection), and therefore can be considered of high-quality terms.",
                "The latter set though contains a much larger collection of words, among which some might be bogus, and meaningless.",
                "The experiments were conducted by employing each one of the aforementioned algorithms (adaptive, generic-frequency, random16K, and random-1M) to crawl and download contents from three Hidden Web sites: The PubMed Medical Library,8 Amazon,9 and the Open Directory Project[2].",
                "According to the information on PubMeds Web site, its collection contains approximately 14 million abstracts of biomedical articles.",
                "We consider these abstracts as the documents in the site, and in each iteration of the adaptive policy, we use these abstracts as input to the algorithm.",
                "Thus our goal is to discover as many unique abstracts as possible by repeatedly querying the Web query interface provided by PubMed.",
                "The Hidden Web crawling on the PubMed Web site can be considered as topic-specific, due to the fact that all abstracts within PubMed are related to the fields of medicine and biology.",
                "In the case of the Amazon Web site, we are interested in downloading all the hidden pages that contain information on books.",
                "The querying to Amazon is performed through the Software Developers Kit that Amazon provides for interfacing to its Web site, and which returns results in XML form.",
                "The generic keyword field is used for searching, and as input to the adaptive policy we extract the product description and the text of customer reviews when present in the XML reply.",
                "Since Amazon does not provide any information on how many books it has in its catalogue, we use random sampling on the 10-digit ISBN number of the books to estimate the size of the collection.",
                "Out of the 10, 000 random ISBN numbers queried, 46 are found in the Amazon catalogue, therefore the size of its book collection is estimated to be 46 10000 · 1010 = 4.6 million books.",
                "Its also worth noting here that Amazon poses an upper limit on the number of results (books in our case) returned by each query, which is set to 32, 000.",
                "As for the third Hidden Web site, the Open Directory Project (hereafter also referred to as dmoz), the site maintains the links to 3.8 million sites together with a brief summary of each listed site.",
                "The links are searchable through a keyword-search interface.",
                "We consider each indexed link together with its brief summary as the document of the dmoz site, and we provide the short summaries to the adaptive algorithm to drive the selection of new keywords for querying.",
                "On the dmoz Web site, we perform two Hidden Web crawls: the first is on its generic collection of 3.8-million indexed 7 We did not manually exclude stop words (e.g., the, is, of, etc.) from the keyword list.",
                "As it turns out, all Web sites except PubMed return matching documents for the stop words, such as the. 8 PubMed Medical Library: http://www.pubmed.org 9 Amazon Inc.: http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 fractionofdocuments query number Cumulative fraction of unique documents - PubMed website adaptive generic-frequency random-16K random-1M Figure 9: Coverage of policies for Pubmed 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - Amazon website adaptive generic-frequency random-16K random-1M Figure 10: Coverage of policies for Amazon sites, regardless of the category that they fall into.",
                "The other crawl is performed specifically on the Arts section of dmoz (http:// dmoz.org/Arts), which comprises of approximately 429, 000 indexed sites that are relevant to Arts, making this crawl topicspecific, as in PubMed.",
                "Like Amazon, dmoz also enforces an upper limit on the number of returned results, which is 10, 000 links with their summaries. 4.1 Comparison of policies The first question that we seek to answer is the evolution of the coverage metric as we submit queries to the sites.",
                "That is, what fraction of the collection of documents stored in the Hidden Web site can we download as we continuously query for new words selected using the policies described above?",
                "More formally, we are interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit q1, . . . , qi queries, and as i increases.",
                "In Figures 9, 10, 11, and 12 we present the coverage metric for each policy, as a function of the query number, for the Web sites of PubMed, Amazon, general dmoz and the art-specific dmoz, respectively.",
                "On the y-axis the fraction of the total documents downloaded from the website is plotted, while the x-axis represents the query number.",
                "A first observation from these graphs is that in general, the generic-frequency and the adaptive policies perform much better than the random-based algorithms.",
                "In all of the figures, the graphs for the random-1M and the random-16K are significantly below those of other policies.",
                "Between the generic-frequency and the adaptive policies, we can see that the latter outperforms the former when the site is topic spe0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 fractionofdocuments query number Cumulative fraction of unique documents - dmoz website adaptive generic-frequency random-16K random-1M Figure 11: Coverage of policies for general dmoz 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 fractionofdocuments query number Cumulative fraction of unique documents - dmoz/Arts website adaptive generic-frequency random-16K random-1M Figure 12: Coverage of policies for the Arts section of dmoz cific.",
                "For example, for the PubMed site (Figure 9), the adaptive algorithm issues only 83 queries to download almost 80% of the documents stored in PubMed, but the generic-frequency algorithm requires 106 queries for the same coverage,.",
                "For the dmoz/Arts crawl (Figure 12), the difference is even more substantial: the adaptive policy is able to download 99.98% of the total sites indexed in the Directory by issuing 471 queries, while the frequency-based algorithm is much less effective using the same number of queries, and discovers only 72% of the total number of indexed sites.",
                "The adaptive algorithm, by examining the contents of the pages that it downloads at each iteration, is able to identify the topic of the site as expressed by the words that appear most frequently in the result-set.",
                "Consequently, it is able to select words for subsequent queries that are more relevant to the site, than those preferred by the genericfrequency policy, which are drawn from a large, generic collection.",
                "Table 1 shows a sample of 10 keywords out of 211 chosen and submitted to the PubMed Web site by the adaptive algorithm, but not by the other policies.",
                "For each keyword, we present the number of the iteration, along with the number of results that it returned.",
                "As one can see from the table, these keywords are highly relevant to the topics of medicine and biology of the Public Medical Library, and match against numerous articles stored in its Web site.",
                "In both cases examined in Figures 9, and 12, the random-based policies perform much worse than the adaptive algorithm, and the generic-frequency.",
                "It is worthy noting however, that the randombased policy with the small, carefully selected set of 16, 000 quality words manages to download a considerable fraction of 42.5% 106 Iteration Keyword Number of Results 23 department 2, 719, 031 34 patients 1, 934, 428 53 clinical 1, 198, 322 67 treatment 4, 034, 565 69 medical 1, 368, 200 70 hospital 503, 307 146 disease 1, 520, 908 172 protein 2, 620, 938 Table 1: Sample of keywords queried to PubMed exclusively by the adaptive policy from the PubMed Web site after 200 queries, while the coverage for the Arts section of dmoz reaches 22.7%, after 471 queried keywords.",
                "On the other hand, the random-based approach that makes use of the vast collection of 1 million words, among which a large number is bogus keywords, fails to download even a mere 1% of the total collection, after submitting the same number of query words.",
                "For the generic collections of Amazon and the dmoz sites, shown in Figures 10 and 11 respectively, we get mixed results: The genericfrequency policy shows slightly better performance than the adaptive policy for the Amazon site (Figure 10), and the adaptive method clearly outperforms the generic-frequency for the general dmoz site (Figure 11).",
                "A closer look at the log files of the two Hidden Web crawlers reveals the main reason: Amazon was functioning in a very flaky way when the adaptive crawler visited it, resulting in a large number of lost results.",
                "Thus, we suspect that the slightly poor performance of the adaptive policy is due to this experimental variance.",
                "We are currently running another experiment to verify whether this is indeed the case.",
                "Aside from this experimental variance, the Amazon result indicates that if the collection and the words that a Hidden Web site contains are generic enough, then the generic-frequency approach may be a good candidate algorithm for effective crawling.",
                "As in the case of topic-specific Hidden Web sites, the randombased policies also exhibit poor performance compared to the other two algorithms when crawling generic sites: for the Amazon Web site, random-16K succeeds in downloading almost 36.7% after issuing 775 queries, alas for the generic collection of dmoz, the fraction of the collection of links downloaded is 13.5% after the 770th query.",
                "Finally, as expected, random-1M is even worse than random16K, downloading only 14.5% of Amazon and 0.3% of the generic dmoz.",
                "In summary, the adaptive algorithm performs remarkably well in all cases: it is able to discover and download most of the documents stored in Hidden Web sites by issuing the least number of queries.",
                "When the collection refers to a specific topic, it is able to identify the keywords most relevant to the topic of the site and consequently ask for terms that is most likely that will return a large number of results .",
                "On the other hand, the generic-frequency policy proves to be quite effective too, though less than the adaptive: it is able to retrieve relatively fast a large portion of the collection, and when the site is not topic-specific, its effectiveness can reach that of adaptive (e.g.",
                "Amazon).",
                "Finally, the random policy performs poorly in general, and should not be preferred. 4.2 Impact of the initial query An interesting issue that deserves further examination is whether the initial choice of the keyword used as the first query issued by the adaptive algorithm affects its effectiveness in subsequent iterations.",
                "The choice of this keyword is not done by the selection of the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 fractionofdocuments query number Convergence of adaptive under different initial queries - PubMed website pubmed data information return Figure 13: Convergence of the adaptive algorithm using different initial queries for crawling the PubMed Web site adaptive algorithm itself and has to be manually set, since its query statistics tables have not been populated yet.",
                "Thus, the selection is generally arbitrary, so for purposes of fully automating the whole process, some additional investigation seems necessary.",
                "For this reason, we initiated three adaptive Hidden Web crawlers targeting the PubMed Web site with different seed-words: the word data, which returns 1,344,999 results, the word information that reports 308, 474 documents, and the word return that retrieves 29, 707 pages, out of 14 million.",
                "These keywords represent varying degrees of term popularity in PubMed, with the first one being of high popularity, the second of medium, and the third of low.",
                "We also show results for the keyword pubmed, used in the experiments for coverage of Section 4.1, and which returns 695 articles.",
                "As we can see from Figure 13, after a small number of queries, all four crawlers roughly download the same fraction of the collection, regardless of their starting point: Their coverages are roughly equivalent from the 25th query.",
                "Eventually, all four crawlers use the same set of terms for their queries, regardless of the initial query.",
                "In the specific experiment, from the 36th query onward, all four crawlers use the same terms for their queries in each iteration, or the same terms are used off by one or two query numbers.",
                "Our result confirms the observation of [11] that the choice of the initial query has minimal effect on the final performance.",
                "We can explain this intuitively as follows: Our algorithm approximates the optimal set of queries to use for a particular Web site.",
                "Once the algorithm has issued a significant number of queries, it has an accurate estimation of the content of the Web site, regardless of the initial query.",
                "Since this estimation is similar for all runs of the algorithm, the crawlers will use roughly the same queries. 4.3 Impact of the limit in the number of results While the Amazon and dmoz sites have the respective limit of 32,000 and 10,000 in their result sizes, these limits may be larger than those imposed by other Hidden Web sites.",
                "In order to investigate how a tighter limit in the result size affects the performance of our algorithms, we performed two additional crawls to the generic-dmoz site: we ran the generic-frequency and adaptive policies but we retrieved only up to the top 1,000 results for every query.",
                "In Figure 14 we plot the coverage for the two policies as a function of the number of queries.",
                "As one might expect, by comparing the new result in Figure 14 to that of Figure 11 where the result limit was 10,000, we conclude that the tighter limit requires a higher number of queries to achieve the same coverage.",
                "For example, when the result limit was 10,000, the adaptive pol107 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FractionofUniquePages Query Number Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000) adaptive generic-frequency Figure 14: Coverage of general dmoz after limiting the number of results to 1,000 icy could download 70% of the site after issuing 630 queries, while it had to issue 2,600 queries to download 70% of the site when the limit was 1,000.",
                "On the other hand, our new result shows that even with a tight result limit, it is still possible to download most of a Hidden Web site after issuing a reasonable number of queries.",
                "The adaptive policy could download more than 85% of the site after issuing 3,500 queries when the limit was 1,000.",
                "Finally, our result shows that our adaptive policy consistently outperforms the generic-frequency policy regardless of the result limit.",
                "In both Figure 14 and Figure 11, our adaptive policy shows significantly larger coverage than the generic-frequency policy for the same number of queries. 4.4 Incorporating the document download cost For brevity of presentation, the performance evaluation results provided so far assumed a simplified cost-model where every query involved a constant cost.",
                "In this section we present results regarding the performance of the adaptive and generic-frequency algorithms using Equation 2 to drive our <br>query selection</br> process.",
                "As we discussed in Section 2.3.1, this query cost model includes the cost for submitting the query to the site, retrieving the result index page, and also downloading the actual pages.",
                "For these costs, we examined the size of every result in the index page and the sizes of the documents, and we chose cq = 100, cr = 100, and cd = 10000, as values for the parameters of Equation 2, and for the particular experiment that we ran on the PubMed website.",
                "The values that we selected imply that the cost for issuing one query and retrieving one result from the result index page are roughly the same, while the cost for downloading an actual page is 100 times larger.",
                "We believe that these values are reasonable for the PubMed Web site.",
                "Figure 15 shows the coverage of the adaptive and genericfrequency algorithms as a function of the resource units used during the download process.",
                "The horizontal axis is the amount of resources used, and the vertical axis is the coverage.",
                "As it is evident from the graph, the adaptive policy makes more efficient use of the available resources, as it is able to download more articles than the generic-frequency, using the same amount of resource units.",
                "However, the difference in coverage is less dramatic in this case, compared to the graph of Figure 9.",
                "The smaller difference is due to the fact that under the current cost metric, the download cost of documents constitutes a significant portion of the cost.",
                "Therefore, when both policies downloaded the same number of documents, the saving of the adaptive policy is not as dramatic as before.",
                "That 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FractionofUniquePages Total Cost (cq=100, cr=100, cd=10000) Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site adaptive frequency Figure 15: Coverage of PubMed after incorporating the document download cost is, the savings in the query cost and the result index download cost is only a relatively small portion of the overall cost.",
                "Still, we observe noticeable savings from the adaptive policy.",
                "At the total cost of 8000, for example, the coverage of the adaptive policy is roughly 0.5 while the coverage of the frequency policy is only 0.3. 5.",
                "RELATED WORK In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.",
                "The main focus of this work is to learn Hidden-Web query interfaces, not to generate queries automatically.",
                "The potential queries are either provided manually by users or collected from the query interfaces.",
                "In contrast, our main focus is to generate queries automatically without any human intervention.",
                "The idea of automatically issuing queries to a database and examining the results has been previously used in different contexts.",
                "For example, in [10, 11], Callan and Connel try to acquire an accurate language model by collecting a uniform random sample from the database.",
                "In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.",
                "In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.",
                "In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.",
                "Our work differs from the previous studies in two ways.",
                "First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the effectiveness of the methods presented in the previous work.",
                "Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.",
                "Cope et al. [15] propose a method to automatically detect whether a particular Web page contains a search form.",
                "This work is complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.",
                "Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.",
                "In [3] the authors study query-based techniques that can extract relational data from large text databases.",
                "Again, these works study orthogonal issues and are complementary to our work.",
                "In order to make documents in multiple textual databases searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).",
                "These approaches essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.",
                "Our approach assumes uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.",
                "There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].",
                "This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.",
                "For example, [19] suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.",
                "Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 6.",
                "CONCLUSION AND FUTURE WORK Traditional crawlers normally follow links on the Web to discover and download pages.",
                "Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.",
                "In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.",
                "We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.",
                "Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.",
                "In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.",
                "Given these results, we believe that our work provides a potential mechanism to improve the search-engine coverage of the Web and the user experience of Web search. 6.1 Future Work We briefly discuss some future-research avenues.",
                "Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.",
                "While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.",
                "For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.",
                "Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = Harry Potter, author = J.K. Rowling, etc), we can apply the same idea that we used for the textual database: estimate the frequency of each attribute value and pick the most promising one.",
                "The main challenge is to automatically segment the returned pages so that we can identify the sections of the pages that present the values corresponding to each attribute.",
                "Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.",
                "Other Practical Issues In addition to the automatic query generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.",
                "For example, in this paper we assumed that the crawler already knows all query interfaces for Hidden-Web sites.",
                "But how can the crawler discover the query interfaces?",
                "The method proposed in [15] may be a good starting point.",
                "In addition, some Hidden-Web sites return their results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.",
                "In this case, a fully automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button automatically.",
                "Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much significant content (e.g. a calendar with links for every day).",
                "In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.",
                "Page similarity detection algorithms may be useful for this purpose [9, 13]. 7.",
                "REFERENCES [1] Lexisnexis http://www.lexisnexis.com. [2] The Open Directory Project, http://www.dmoz.org. [3] E. Agichtein and L. Gravano.",
                "Querying text databases for efficient information extraction.",
                "In ICDE, 2003. [4] E. Agichtein, P. Ipeirotis, and L. Gravano.",
                "Modeling query-based access to text databases.",
                "In WebDB, 2003. [5] Article on New York Times.",
                "Old Search Engine, the Library, Tries to Fit Into a Google World.",
                "Available at: http: //www.nytimes.com/2004/06/21/technology/21LIBR.html, June 2004. [6] L. Barbosa and J. Freire.",
                "Siphoning hidden-web data through keyword-based interfaces.",
                "In SBBD, 2004. [7] M. K. Bergman.",
                "The deep web: Surfacing hidden value,http: //www.press.umich.edu/jep/07-01/bergman.html. [8] K. Bharat and A. Broder.",
                "A technique for measuring the relative size and overlap of public web search engines.",
                "In WWW, 1998. [9] A.",
                "Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig.",
                "Syntactic clustering of the web.",
                "In WWW, 1997. [10] J. Callan, M. Connell, and A.",
                "Du.",
                "Automatic discovery of language models for text databases.",
                "In SIGMOD, 1999. [11] J. P. Callan and M. E. Connell.",
                "Query-based sampling of text databases.",
                "Information Systems, 19(2):97-130, 2001. [12] K. C.-C. Chang, B.",
                "He, C. Li, and Z. Zhang.",
                "Structured databases on the web: Observations and implications.",
                "Technical report, UIUC. [13] J. Cho, N. Shivakumar, and H. Garcia-Molina.",
                "Finding replicated web collections.",
                "In SIGMOD, 2000. [14] W. Cohen and Y.",
                "Singer.",
                "Learning to query the web.",
                "In AAAI Workshop on Internet-Based Information Systems, 1996. [15] J.",
                "Cope, N. Craswell, and D. Hawking.",
                "Automated discovery of search interfaces on the web.",
                "In 14th Australasian conference on Database technologies, 2003. [16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest.",
                "Introduction to Algorithms, 2nd Edition.",
                "MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y.",
                "Levy, and A. O. Mendelzon.",
                "Database techniques for the world-wide web: A survey.",
                "SIGMOD Record, 27(3):59-74, 1998. [18] B.",
                "He and K. C.-C. Chang.",
                "Statistical schema matching across web query interfaces.",
                "In SIGMOD Conference, 2003. [19] P. Ipeirotis and L. Gravano.",
                "Distributed search over the hidden web: Hierarchical database sampling and selection.",
                "In VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano, and M. Sahami.",
                "Probe, count, and classify: Categorizing hidden web databases.",
                "In SIGMOD, 2001. [21] C. Lagoze and H. V. Sompel.",
                "The Open Archives Initiative: Building a low-barrier interoperability framework In JCDL, 2001. [22] S. Lawrence and C. L. Giles.",
                "Searching the World Wide Web.",
                "Science, 280(5360):98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu.",
                "Dpro: A probabilistic approach for hidden web database selection using dynamic probing.",
                "In ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair and M. L. Nelson.",
                "DP9-An OAI Gateway Service for Web Crawlers.",
                "In JCDL, 2002. [25] B.",
                "B. Mandelbrot.",
                "Fractal Geometry of Nature.",
                "W. H. Freeman & Co. [26] A. Ntoulas, J. Cho, and C. Olston.",
                "Whats new on the web? the evolution of the web from a search engine perspective.",
                "In WWW, 2004. [27] A. Ntoulas, P. Zerfos, and J. Cho.",
                "Downloading hidden web content.",
                "Technical report, UCLA, 2004. [28] S. Olsen.",
                "Does search engines power threaten webs independence? http://news.com.com/2009-1023-963618.html. [29] S. Raghavan and H. Garcia-Molina.",
                "Crawling the hidden web.",
                "In VLDB, 2001. [30] G. K. Zipf.",
                "Human Behavior and the Principle of Least-Effort.",
                "Addison-Wesley, Cambridge, MA, 1949. 109"
            ],
            "original_annotated_samples": [
                "In the next section, we formalize this <br>query selection</br> problem. 3 For most Web sites that assume AND for multi-keyword queries, single-term queries return the maximum number of results.",
                "S q1 q qq 2 34 Figure 5: A set-formalization of the optimal <br>query selection</br> problem. 2.3 Problem formalization Theoretically, the problem of <br>query selection</br> can be formalized as follows: We assume that the crawler downloads pages from a Web site that has a set of pages S (the rectangle in Figure 5).",
                "Based on this information our <br>query selection</br> algorithm can then select the best queries that cover the content of the Web site.",
                "We present our prediction method and our <br>query selection</br> algorithm in Section 3. 2.3.1 Performance Metric Before we present our ideas for the <br>query selection</br> problem, we briefly discuss some of our notation and the cost/performance metrics.",
                "In the next section, we first examine how we can use this value to decide which query we should issue next to the Hidden Web site. 3.2 <br>query selection</br> algorithm The goal of the Hidden-Web crawler is to download the maximum number of unique documents from a database using its limited download resources."
            ],
            "translated_annotated_samples": [
                "En la siguiente sección, formalizamos este problema de <br>selección de consultas</br>. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados.",
                "Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de <br>selección de consultas</br> puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5).",
                "Basándose en esta información, nuestro algoritmo de <br>selección de consultas</br> puede seleccionar las mejores consultas que cubran el contenido del sitio web.",
                "Presentamos nuestro método de predicción y nuestro algoritmo de <br>selección de consultas</br> en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de <br>selección de consultas</br>, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento.",
                "En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de <br>selección de consultas</br> El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga."
            ],
            "translated_text": "Descargando contenido textual de la Web Oculta a través de consultas de palabras clave. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda. Dado que no hay enlaces estáticos a las páginas de la Web Oculta, los motores de búsqueda no pueden descubrir e indexar dichas páginas y, por lo tanto, no las devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios de la Web Oculta suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web efectivo que pueda descubrir y descargar páginas de la Hidden Web de forma autónoma. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío al que se enfrenta un rastreador de la Hidden Web es cómo generar automáticamente consultas significativas para enviar al sitio. Aquí proporcionamos un marco teórico para investigar el problema de generación de consultas para la Web Oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas avanzan de forma iterativa, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios reales de la Web Oculta y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y Descriptores de Asignaturas: H.3.7 [Sistemas de Información]: Bibliotecas Digitales; H.3.1 [Sistemas de Información]: Análisis de Contenido e Indexación; H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información. Términos generales: Algoritmos, Rendimiento, Diseño. 1. INTRODUCCIÓN Estudios recientes muestran que una fracción significativa del contenido web no se puede alcanzar siguiendo enlaces [7, 12]. En particular, una gran parte de la Web está oculta detrás de formularios de búsqueda y solo es accesible cuando los usuarios ingresan un conjunto de palabras clave, o consultas, en los formularios. Estas páginas son a menudo referidas como la Web Oculta o la Web Profunda, porque los motores de búsqueda típicamente no pueden indexar las páginas y no las devuelven en sus resultados (por lo tanto, las páginas están esencialmente ocultas para un usuario típico de la Web). Según muchos estudios, el tamaño de la Web Oculta aumenta rápidamente a medida que más organizaciones ponen su contenido valioso en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al. estiman que actualmente existen en la Web más de 100,000 sitios de la Web Oculta. Además, el contenido proporcionado por muchos sitios de la Hidden Web suele ser de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos artículos de alta calidad sobre investigación médica que fueron seleccionados a través de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos pone a disposición documentos de patentes existentes, ayudando a posibles inventores a examinar el estado de la técnica. En este artículo, estudiamos cómo podemos construir un rastreador de la Hidden Web que pueda descargar automáticamente páginas de la Hidden Web, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la Web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de la Web Oculta (debido a la falta de enlaces). Creemos que un rastreador de la Hidden Web efectivo puede tener un impacto tremendo en cómo los usuarios buscan información en la Web: • Acceder a información inexplorada: El rastreador de la Hidden Web permitirá a un usuario promedio de la Web explorar fácilmente la gran cantidad de información que en su mayoría está oculta en la actualidad. Dado que la mayoría de los usuarios de la Web dependen de los motores de búsqueda para descubrir páginas, cuando las páginas no están indexadas por los motores de búsqueda, es poco probable que sean vistas por muchos usuarios de la Web. A menos que los usuarios vayan directamente a los sitios de la Hidden Web y emitan consultas allí, no pueden acceder a las páginas en los sitios. • Mejorar la experiencia del usuario: incluso si un usuario es consciente de varios sitios de la Hidden Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando los resultados. Al hacer que las páginas de la Web Oculta sean buscables en un lugar central, podemos reducir significativamente el tiempo y esfuerzo desperdiciados por los usuarios al buscar en la Web Oculta. • Reducción del sesgo potencial: Debido a la gran dependencia de muchos usuarios de la Web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la Web [28]. Los usuarios no perciben necesariamente lo que realmente existe en la web, sino lo que está indexado por los motores de búsqueda [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de llevar la información de sus sitios de la Web Oculta a la superficie, y han comprometido recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de EE. UU.: http://www.uspto.gov. Los rastreadores son los programas que recorren la Web automáticamente y descargan páginas para los motores de búsqueda. Figura 1: Una interfaz de búsqueda de un solo atributo. El rastreador de la Web oculta intenta automatizar este proceso para los sitios de la Web oculta con contenido textual, minimizando así los costos asociados y el esfuerzo requerido. Dado que la única forma de acceder a las páginas de la Web Oculta es a través de la consulta de un formulario de búsqueda, existen dos desafíos fundamentales para implementar un rastreador efectivo de la Web Oculta: (a) El rastreador debe ser capaz de entender y modelar una interfaz de consulta, y (b) El rastreador debe generar consultas significativas para enviar a la interfaz de consulta. El primer desafío fue abordado por Raghavan y Garcia-Molina en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para descubrir y descargar las páginas de la Web Oculta. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Expedimos exhaustivamente todas las consultas posibles, una consulta a la vez. Cuando los formularios de consulta tienen un campo de texto libre, sin embargo, es posible realizar un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas deberíamos seleccionar? ¿Puede el rastreador generar automáticamente consultas significativas sin entender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema del rastreo de la Hidden-Web y proponemos formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de la Hidden Web. En resumen, este artículo realiza las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo de la Hidden Web (Sección 2). • Investigamos una serie de políticas de rastreo para la Hidden Web, incluida la política óptima que potencialmente puede descargar el máximo número de páginas a través del mínimo número de interacciones. Desafortunadamente, demostramos que la política óptima es NP-difícil y no se puede implementar en la práctica (Sección 2.2). • Proponemos una nueva política adaptativa que aproxima la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta automáticamente su política de selección de consultas basándose en ellas (Sección 3). • Evaluamos diversas políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de diversas políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. En esta sección, presentamos un marco formal para el estudio del problema de rastreo de la Hidden-Web. En la Sección 2.1, describimos nuestras suposiciones sobre los sitios de la Web Oculta y explicamos cómo los usuarios interactúan con los sitios. Basándonos en este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de la Web Oculta en la Sección 2.2. Finalmente, en la Sección 2.3, formalizamos el problema del rastreo de la Hidden Web. 2.1 Modelo de base de datos de la Hidden Web Existe una variedad de fuentes de la Hidden Web que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos categorizar un sitio de la Web Oculta como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que contiene principalmente documentos de texto plano, como PubMed y LexisNexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales ofrecen una interfaz de búsqueda sencilla donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales de múltiples atributos (por ejemplo, un libro en el sitio web de Amazon puede tener los campos título=Harry Potter, autor=J.K. Rowling y isbn=0590353403) y admite interfaces de búsqueda multiatributo (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de un solo atributo. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de múltiples atributos en la Sección 6.1. Por lo general, los usuarios necesitan seguir los siguientes pasos para acceder a las páginas en una base de datos de la Web Oculta: 1. Paso 1. Primero, el usuario emite una consulta, digamos hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como la que se muestra en la Figura 1). 2. Paso 2. Poco después de que el usuario emita la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3(a). Paso 3. Desde la lista en la página de índice de resultados, el usuario identifica las páginas que parecen interesantes y sigue los enlaces. Hacer clic en un enlace lleva al usuario a la página web real, como la que se muestra en la Figura 3(b), que el usuario desea ver. 2.2 Un algoritmo genérico de rastreo de la Hidden Web Dado que la única entrada a las páginas en un sitio de Hidden Web es su formulario de búsqueda, un rastreador de Hidden Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, enviarla al sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene un tiempo y recursos de red limitados, por lo que repite estos pasos hasta que agota sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de la Hidden-Web. Para simplificar, asumimos que el rastreador de la Web oculta emite solo consultas de un solo término. El rastreador primero decide qué término de consulta va a utilizar (Paso 2), emite la consulta y recupera la página de índice de resultados (Paso 3). Finalmente, basándose en los enlaces encontrados en la página de índice de resultados, descarga las páginas de la Web Oculta del sitio (Paso 4). Este mismo proceso se repite hasta que se agoten todos los recursos disponibles (Paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador debe tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede finalizar su rastreo temprano utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin nunca recuperar páginas reales. Por lo tanto, la forma en que el rastreador selecciona la siguiente consulta puede afectar enormemente su efectividad. En la siguiente sección, formalizamos este problema de <br>selección de consultas</br>. Para la mayoría de los sitios web que asumen el operador Y (AND) para consultas de múltiples palabras clave, las consultas de un solo término devuelven el máximo número de resultados. Extender nuestro trabajo a consultas de múltiples palabras clave es sencillo. 101 (a) Lista de páginas coincidentes para la consulta hígado. (b) La primera página coincidente para hígado. Figura 3: Páginas del sitio web de PubMed. ALGORITMO 2.1. Procedimiento para rastrear un sitio web oculto (1) mientras (hay recursos disponibles) hacer // seleccionar un término para enviar al sitio (2) qi = SeleccionarTérmino() // enviar la consulta y adquirir la página de índice de resultados (3) R(qi) = ConsultarSitioWeb( qi ) // descargar las páginas de interés (4) Descargar( R(qi) ) (5) hecho Figura 4: Algoritmo para rastrear un sitio web oculto. Figura 5: Una formalización en conjunto del problema de selección óptima de consultas. 2.3 Formalización del problema Teóricamente, el problema de <br>selección de consultas</br> puede ser formalizado de la siguiente manera: Suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas S (el rectángulo en la Figura 5). Representamos cada página web en S como un punto (puntos en la Figura 5). Cada posible consulta qi que podamos emitir puede ser vista como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos qi en el sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el máximo número de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjuntos en teoría de grafos [16]. Hay dos dificultades principales que necesitamos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. Segundo, se sabe que el problema de la cobertura de conjuntos es NP-Difícil [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en tiempo polinómico. En este documento, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que, aunque no sabemos qué páginas serán devueltas por cada consulta qi que emitimos, podemos predecir cuántas páginas serán devueltas. Basándose en esta información, nuestro algoritmo de <br>selección de consultas</br> puede seleccionar las mejores consultas que cubran el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de <br>selección de consultas</br> en la Sección 3. 2.3.1 Métrica de rendimiento Antes de presentar nuestras ideas para el problema de <br>selección de consultas</br>, discutimos brevemente algunas de nuestras notaciones y las métricas de costo/rendimiento. Dada una consulta qi, usamos P(qi) para denotar la fracción de páginas que obtendremos si emitimos la consulta qi en el sitio. Por ejemplo, si un sitio web tiene un total de 10,000 páginas, y si 3,000 páginas son devueltas para la consulta qi = medicina, entonces P(qi) = 0.3. Usamos P(q1 ∧ q2) para representar la fracción de páginas que son devueltas tanto por q1 como por q2 (es decir, la intersección de P(q1) y P(q2)). De manera similar, usamos P(q1 ∨ q2) para representar la fracción de páginas que se devuelven de q1 o q2 (es decir, la unión de P(q1) y P(q2)). También usamos Cost(qi) para representar el costo de emitir la consulta qi. Dependiendo del escenario, el costo puede medirse ya sea en tiempo, ancho de banda de red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacta. En el caso más común, el costo de la consulta consiste en una serie de factores, incluyendo el costo de enviar la consulta al sitio, recuperar la página de índice de resultados (Figura 3(a)) y descargar las páginas reales (Figura 3(b)). Suponemos que enviar una consulta conlleva un costo fijo de cq. El costo de descargar la página de índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el costo cd de descargar un documento coincidente también es fijo. Entonces, el costo total de la consulta qi es Costo(qi) = cq + crP(qi) + cdP(qi). (1) En ciertos casos, algunos de los documentos de qi pueden haber sido descargados previamente en consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de qi puede ser Costo(qi) = cq + crP(qi) + cdPnew(qi). Aquí, usamos Pnew(qi) para representar la fracción de los nuevos documentos de qi que no han sido recuperados de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P(qi) y Pnew(qi) para estimar el costo de qi. Dado que nuestros algoritmos son independientes de la función de costo exacta, asumiremos una función de costo genérica Cost(qi) en este artículo. Cuando necesitemos una función de costo concreta, sin embargo, utilizaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de la Web oculta de la siguiente manera: PROBLEMA 1. Encuentra el conjunto de consultas q1, . . . , qn que maximice P(q1 ∨ · · · ∨ qn) bajo la restricción n i=1 Cost(qi) ≤ t. Aquí, t es el recurso máximo de descarga que tiene el rastreador. SELECCIÓN DE PALABRAS CLAVE ¿Cómo debería un rastreador seleccionar las consultas a emitir? Dado que el objetivo es descargar el máximo número de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • Aleatorio: Seleccionamos palabras al azar, por ejemplo, de un diccionario en inglés, y las enviamos a la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes. • Frecuencia genérica: Analizamos un corpus de documentos genéricos recopilados en otro lugar (por ejemplo, de la Web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Basándonos en esta distribución genérica, comenzamos con la palabra clave más frecuente, la introducimos en la base de datos de la Web Oculta y recuperamos el resultado. Luego continuamos con la segunda palabra clave más frecuente y repetimos este proceso hasta agotar todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de la Web Oculta, devolviendo muchos documentos coincidentes. • Adaptativo: Analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos de la Web Oculta y estimamos cuál palabra clave es la más probable de devolver la mayor cantidad de documentos. Basándonos en este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base ya que se espera que tenga el peor rendimiento. Entre las políticas de frecuencia genérica y adaptativa, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una colección de documentos genéricos sin un tema especializado. La política adaptativa, sin embargo, puede tener un rendimiento significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente al corpus genérico. Experimentalmente compararemos estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos entender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora con el fin de implementar la política adaptativa. Abordamos este problema en el resto de esta sección. 3.1 Estimación del número de páginas coincidentes. Para identificar la consulta más prometedora, necesitamos estimar cuántos documentos nuevos descargaremos si emitimos la consulta qi como la próxima consulta. Es decir, suponiendo que hemos emitido las consultas q1, . . . , qi−1, necesitamos estimar P(q1∨· · ·∨qi−1∨qi), para cada posible próxima consulta qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P(q1 ∨ · · · ∨ qi−1 ∨ qi) como: P((q1 ∨ · · · ∨ qi−1) ∨ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi) = P(q1 ∨ · · · ∨ qi−1) + P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3) En la fórmula anterior, observamos que podemos medir con precisión P(q1 ∨ · · · ∨ qi−1) y P(qi | q1 ∨ · · · ∨ qi−1) analizando páginas previamente descargadas: Sabemos P(q1 ∨ · · · ∨ qi−1), la unión de todas las páginas descargadas de q1, . . . , qi−1, ya que ya hemos emitido q1, . . . , qi−1 y descargado las páginas coincidentes. También podemos medir P(qi | q1 ∨ · · · ∨ qi−1), la probabilidad de que qi aparezca en las páginas de q1, . . . , qi−1, contando cuántas veces qi aparece en las páginas de q1, . . . , qi−1. Por lo tanto, solo necesitamos estimar P(qi) para evaluar P(q1 ∨ · · · ∨ qi). Podemos considerar varias formas diferentes de estimar P(qi), incluyendo las siguientes: 1. Estimador de independencia: Suponemos que la aparición del término qi es independiente de los términos q1, . . . , qi−1. Es decir, asumimos que P(qi) = P(qi|q1 ∨ · · · ∨ qi−1). 2. Estimador de Zipf: En [19], Ipeirotis et al. propusieron un método para estimar cuántas veces ocurre un término particular en todo el corpus basándose en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de términos dentro de colecciones de texto sigue una distribución de ley de potencias [30, 25]. Es decir, si clasificamos todos los términos según su frecuencia de ocurrencia (siendo el término más frecuente el de rango 1, el segundo más frecuente el de rango 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto está dada por: f = α(r + β)−γ (4) donde r es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, basados en el subconjunto de documentos que hemos descargado de consultas anteriores, y (2) utilizar los parámetros estimados para predecir f dado el ranking r de un término dentro del subconjunto. Para obtener una descripción más detallada sobre cómo podemos utilizar este método para estimar P(qi), remitimos al lector a la versión extendida de este artículo [27]. Después de estimar los valores de P(qi) y P(qi|q1 ∨ · · · ∨ qi−1), podemos calcular P(q1 ∨ · · · ∨ qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P(qi|q1 ∨ · · · ∨ qi−1) manteniendo una tabla de resumen concisa. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir a continuación en el sitio web Hidden Web. 3.2 Algoritmo de <br>selección de consultas</br> El objetivo del rastreador de Hidden-Web es descargar el máximo número de documentos únicos de una base de datos utilizando sus recursos limitados de descarga. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}