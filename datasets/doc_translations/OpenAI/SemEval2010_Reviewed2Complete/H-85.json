{
    "id": "H-85",
    "original_text": "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. We present a real-world study of modeling the behavior of web search users to predict web search result preferences. Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior. We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods. We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback. General Terms Algorithms, Measurement, Performance, Experimentation. 1. INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general. Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs. However, explicit human ratings are expensive and difficult to obtain. At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results. If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems. Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search. However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks. Therefore, it is not clear whether these techniques will work for general real-world web search. A significant distinction is that web search is not controlled. Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered. But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting. By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting. Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage. Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions. Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings. We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results. Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6). We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2. BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval. The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20]. A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features. Current search engines are commonly tuned on human relevance judgments. Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated. Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research. Several research groups have evaluated the relationship between implicit measures and user interest. In these studies, both reading time and explicit ratings of interest are collected. Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels. Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system. Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems. More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon. Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web. The authors hypothesized correlations between a high degree of page activity and a users interest. While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest. Claypool et al. [6] studied how several implicit measures related to the interests of the user. They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited. Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest. Fox et al. [7] explored the relationship between implicit and explicit measures in Web search. They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions. They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page. Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions. More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence. By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting. A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12]. Unfortunately, the extent to which existing research applies to real-world web search is unclear. In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3. LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences. Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces. Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately. Our general idea is to model the deviations from the expected user behavior. Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information. We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions. A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately. In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments. The data set is described in more detail in Section 5.2. For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs. For these queries we aggregate click data over more than 120,000 searches performed over a three week period. We also have explicit relevance judgments for the top 10 results for each query. Figure 3.1 shows the relative clickthrough frequency as a function of result position. The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p). These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1. The resulting distribution agrees with previous observations that users click more often on top-ranked results. This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches. First we consider the distribution of clicks for the relevant documents for these queries. Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR). While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result. For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries. Nevertheless, many users still click on the non-relevant results in position 1 for such queries. This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant). If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results. Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users. We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior. Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution. We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted. Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features. More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 . In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair. This aggregation gives additional robustness of not relying on individual noisy user interactions. In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values. We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result. Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize. This information was obtained via opt-in client-side instrumentation from users of a major web search engine. This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7]. An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior. Furthermore, we include derived, distributional features computed as described above. The features we use to represent user search interactions are summarized in Table 3.1. For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing. Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary. To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text. These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc. Browsing features: Simple aspects of the user web page interactions can be captured and quantified. These features are used to characterize interactions with pages beyond the results page. For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query. These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries). We include both the direct features and the derived features described above. Clickthrough features: Clicks are a special case of user interaction with the search engine. We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4. For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove). The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1. Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences. We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights. We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence. Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences. The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries. For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair. We can then analyze the user behavior for all the instances where these queries were submitted to the search engine. To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items. More specifically, for each judged query we check if a result link has been judged. If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result. These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set. RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4. PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences. These models range from using no implicit user feedback to using all available implicit user feedback. Ranking search results to predict user preferences is a fundamental problem in information retrieval. Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1). At the same time, user interactions with a search engine provide a wealth of information. A commonly considered type of interaction is user clicks on search results. Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results. We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2. As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3). We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker. For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine. Hence, we will call this system Current for the subsequent discussion. While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality. The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10]. By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies. We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next. Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one. Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking. Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above. For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above. We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper. These strategies are motivated and empirically tested for individual users in a laboratory setting. As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior. The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency. We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model. For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC . Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events. The choice of d selects the tradeoff between recall and precision. While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position. However, for informational queries, multiple results may be clicked, with varying frequency. Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results. We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m. As in CD, the choice of m selects the tradeoff between recall and precision. The pairs may be preferred in the original order or in reverse of it. Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other. Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui. Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior. CDiff and CD are complimentary. CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD. Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions. Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies. As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results. We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3). The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1. Relative user preferences are then estimated using the learned user behavior model described in Section 3.4. Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences. This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond. As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5. EXPERIMENTAL SETUP We now describe our experimental setup. We first describe the methodology used, including our evaluation metrics (Section 5.1). Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results. This allows us to compare to previous work [9,10]. Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9]. The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments. We discuss other applications of our models beyond web search ranking in Section 7. To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels. To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label). In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20]. While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way. Specifically, we report the average query recall and precision. For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted. The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively. A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture. We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine. For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort. In addition for these queries we also had user interaction data for more than 120,000 instances of these queries. The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine. This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31. These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market. In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs). These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings. Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison. We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1). Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page. In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions. This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1). Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation. Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method. To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%. The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets). It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results. Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above. To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6. RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results. Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2). The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively. Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence. In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset. Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08. In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels. Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies. However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence. But first, we consider the best performing strategy, UserBehavior. Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared. Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies. Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking. To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation. Figure 6.2 reports Precision vs. Recall for each feature group. Interestingly, Query-text alone has low accuracy (only marginally better than Random). Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model. Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs. Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively). Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier. For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy. Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data. We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query. Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query. Not surprisingly, CD+CDiff improves with more clicks. This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories. Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff. For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days). Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall. In contrast, the current search engine always makes a prediction for every result for a given query. As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available. Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time. We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625). As expected, Recall of both strategies improves quickly with more days of interaction data examined. We now briefly summarize our experimental results. We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences. Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results). Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N. Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone. Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7. CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting. We showed that our robust models result in higher prediction accuracy than previously published techniques. We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries. Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios. Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions. We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features. By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information. Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies. Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles. For example, the user behavior model on intranet search may be different from the web search behavior. Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings. A natural application of our preference prediction models is to improve web search ranking [1]. In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking. For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels. Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine. While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries. For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries. Hence, clustering queries and learning different predictive models for each query type is a promising research direction. Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models. Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences. As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting. Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods. The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8. REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan. HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents. In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,. In Proceedings of WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White. Evaluating implicit measures to improve the search experience. In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick. Learning users interests by unobtrusively observing their normal behavior. In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical. Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography. In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl. GroupLens: Applying collaborative filtering to usenet news. In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim. Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim. Modeling information content using observable behavior. In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web. In Working with Technology in Mind: Brunswikian. Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001",
    "original_translation": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001",
    "original_sentences": [
        "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
        "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
        "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
        "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
        "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
        "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
        "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
        "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
        "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
        "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
        "However, explicit human ratings are expensive and difficult to obtain.",
        "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
        "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
        "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
        "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
        "Therefore, it is not clear whether these techniques will work for general real-world web search.",
        "A significant distinction is that web search is not controlled.",
        "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
        "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
        "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
        "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
        "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
        "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
        "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
        "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
        "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
        "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
        "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
        "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
        "Current search engines are commonly tuned on human relevance judgments.",
        "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
        "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
        "Several research groups have evaluated the relationship between implicit measures and user interest.",
        "In these studies, both reading time and explicit ratings of interest are collected.",
        "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
        "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
        "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
        "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
        "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
        "The authors hypothesized correlations between a high degree of page activity and a users interest.",
        "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
        "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
        "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
        "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
        "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
        "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
        "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
        "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
        "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
        "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
        "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
        "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
        "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
        "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
        "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
        "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
        "Our general idea is to model the deviations from the expected user behavior.",
        "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
        "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
        "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
        "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
        "The data set is described in more detail in Section 5.2.",
        "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
        "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
        "We also have explicit relevance judgments for the top 10 results for each query.",
        "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
        "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
        "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
        "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
        "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
        "First we consider the distribution of clicks for the relevant documents for these queries.",
        "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
        "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
        "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
        "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
        "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
        "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
        "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
        "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
        "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
        "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
        "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
        "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
        "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
        "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
        "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
        "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
        "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
        "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
        "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
        "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
        "Furthermore, we include derived, distributional features computed as described above.",
        "The features we use to represent user search interactions are summarized in Table 3.1.",
        "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
        "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
        "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
        "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
        "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
        "These features are used to characterize interactions with pages beyond the results page.",
        "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
        "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
        "We include both the direct features and the derived features described above.",
        "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
        "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
        "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
        "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
        "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
        "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
        "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
        "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
        "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
        "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
        "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
        "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
        "More specifically, for each judged query we check if a result link has been judged.",
        "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
        "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
        "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
        "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
        "These models range from using no implicit user feedback to using all available implicit user feedback.",
        "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
        "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
        "At the same time, user interactions with a search engine provide a wealth of information.",
        "A commonly considered type of interaction is user clicks on search results.",
        "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
        "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
        "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
        "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
        "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
        "Hence, we will call this system Current for the subsequent discussion.",
        "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
        "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
        "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
        "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
        "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
        "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
        "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
        "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
        "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
        "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
        "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
        "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
        "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
        "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
        "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
        "The choice of d selects the tradeoff between recall and precision.",
        "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
        "However, for informational queries, multiple results may be clicked, with varying frequency.",
        "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
        "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
        "As in CD, the choice of m selects the tradeoff between recall and precision.",
        "The pairs may be preferred in the original order or in reverse of it.",
        "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
        "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
        "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
        "CDiff and CD are complimentary.",
        "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
        "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
        "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
        "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
        "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
        "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
        "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
        "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
        "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
        "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
        "EXPERIMENTAL SETUP We now describe our experimental setup.",
        "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
        "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
        "This allows us to compare to previous work [9,10].",
        "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
        "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
        "We discuss other applications of our models beyond web search ranking in Section 7.",
        "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
        "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
        "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
        "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
        "Specifically, we report the average query recall and precision.",
        "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
        "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
        "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
        "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
        "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
        "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
        "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
        "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
        "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
        "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
        "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
        "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
        "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
        "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
        "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
        "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
        "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
        "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
        "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
        "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
        "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
        "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
        "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
        "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
        "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
        "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
        "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
        "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
        "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
        "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
        "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
        "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
        "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
        "But first, we consider the best performing strategy, UserBehavior.",
        "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
        "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
        "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
        "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
        "Figure 6.2 reports Precision vs. Recall for each feature group.",
        "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
        "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
        "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
        "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
        "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
        "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
        "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
        "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
        "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
        "Not surprisingly, CD+CDiff improves with more clicks.",
        "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
        "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
        "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
        "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
        "In contrast, the current search engine always makes a prediction for every result for a given query.",
        "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
        "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
        "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
        "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
        "We now briefly summarize our experimental results.",
        "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
        "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
        "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
        "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
        "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
        "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
        "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
        "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
        "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
        "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
        "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
        "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
        "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
        "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
        "For example, the user behavior model on intranet search may be different from the web search behavior.",
        "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
        "A natural application of our preference prediction models is to improve web search ranking [1].",
        "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
        "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
        "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
        "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
        "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
        "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
        "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
        "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
        "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
        "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
        "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
        "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
        "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
        "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
        "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
        "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
        "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
        "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
        "Evaluating implicit measures to improve the search experience.",
        "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
        "Learning users interests by unobtrusively observing their normal behavior.",
        "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
        "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
        "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
        "GroupLens: Applying collaborative filtering to usenet news.",
        "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
        "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
        "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
        "Modeling information content using observable behavior.",
        "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
        "In Working with Technology in Mind: Brunswikian.",
        "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
        "Introduction to modern information retrieval.",
        "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
    ],
    "translated_text_sentences": [
        "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda.",
        "Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web.",
        "La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas.",
        "Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario.",
        "Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados.",
        "Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics.",
        "Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia.",
        "Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1.",
        "La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general.",
        "Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento.",
        "Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener.",
        "Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda.",
        "Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información.",
        "Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web.",
        "Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados.",
        "Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real.",
        "Una distinción significativa es que la búsqueda en la web no está controlada.",
        "Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar.",
        "Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web.",
        "Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web.",
        "Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real.",
        "Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario.",
        "Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda.",
        "Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda.",
        "Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6).",
        "Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo.",
        "ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información.",
        "Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20].",
        "Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características.",
        "Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana.",
        "Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación.",
        "Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación.",
        "Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario.",
        "En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés.",
        "Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios.",
        "Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens.",
        "Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación.",
        "Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa.",
        "Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web.",
        "Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios.",
        "Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario.",
        "Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario.",
        "Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas.",
        "Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito.",
        "Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web.",
        "Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda.",
        "Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página.",
        "Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación.",
        "Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics.",
        "Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio.",
        "Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12].",
        "Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real.",
        "En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3.",
        "MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias.",
        "Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables.",
        "Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente.",
        "Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario.",
        "Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta.",
        "Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario.",
        "Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente.",
        "En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia.",
        "El conjunto de datos se describe con más detalle en la Sección 5.2.",
        "Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas.",
        "Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas.",
        "También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta.",
        "La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado.",
        "La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p).",
        "Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1.",
        "La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados.",
        "Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas.",
        "Primero consideramos la distribución de clics para los documentos relevantes de estas consultas.",
        "La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR).",
        "Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante.",
        "Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas.",
        "Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas.",
        "Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable.",
        "Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados.",
        "Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios.",
        "Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda.",
        "Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo.",
        "Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar.",
        "Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes.",
        "De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1.",
        "Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL.",
        "Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios.",
        "En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos.",
        "Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web.",
        "Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos.",
        "Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web.",
        "Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7].",
        "Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario.",
        "Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente.",
        "Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1.",
        "Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación.",
        "Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original.",
        "Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento.",
        "Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc.",
        "Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados.",
        "Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados.",
        "Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta.",
        "Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas).",
        "Incluimos tanto las características directas como las características derivadas descritas anteriormente.",
        "Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda.",
        "Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4.",
        "Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba).",
        "Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1.",
        "Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario.",
        "Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones.",
        "Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio.",
        "Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario.",
        "El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas.",
        "Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado.",
        "Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda.",
        "Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados.",
        "Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado.",
        "Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda.",
        "Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento.",
        "RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos.",
        "PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario.",
        "Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible.",
        "La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información.",
        "La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1).",
        "Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información.",
        "Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda.",
        "Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes.",
        "También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2.",
        "Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3).",
        "Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente.",
        "Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web.",
        "Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior.",
        "Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página.",
        "Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10].",
        "Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas.",
        "Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next.",
        "Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado.",
        "El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual.",
        "Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente.",
        "Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente.",
        "Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo.",
        "Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio.",
        "Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales.",
        "El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar.",
        "Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo.",
        "Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC.",
        "Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics.",
        "La elección de d determina el equilibrio entre la sensibilidad y la precisión.",
        "Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada.",
        "Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables.",
        "Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente.",
        "Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m.",
        "Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión.",
        "Los pares pueden ser preferidos en el orden original o en su reverso.",
        "Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro.",
        "De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui.",
        "Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario.",
        "CDiff y CD son complementarios.",
        "CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD.",
        "Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff.",
        "Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas.",
        "Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web.",
        "Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3).",
        "La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1.",
        "Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4.",
        "Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias.",
        "Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá.",
        "Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario.",
        "CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental.",
        "Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1).",
        "Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados.",
        "Esto nos permite comparar con trabajos anteriores [9,10].",
        "Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9].",
        "La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario.",
        "Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7.",
        "Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas.",
        "Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales).",
        "Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20].",
        "Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar.",
        "Específicamente, informamos la recuperación promedio de consultas y la precisión.",
        "Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente.",
        "El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente.",
        "Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura.",
        "Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante).",
        "Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia.",
        "Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas.",
        "Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web.",
        "Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre.",
        "Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos.",
        "Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL).",
        "Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio.",
        "Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación.",
        "Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1).",
        "Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página.",
        "En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones.",
        "Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1).",
        "Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación.",
        "División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior.",
        "Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante.",
        "El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba).",
        "Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados.",
        "Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba.",
        "Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6.",
        "RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web.",
        "La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2).",
        "Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente.",
        "Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción.",
        "En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N).",
        "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1.",
        "Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08.",
        "Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos.",
        "Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas.",
        "Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada.",
        "Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior.",
        "Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados.",
        "La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics.",
        "Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base.",
        "Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual.",
        "La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características.",
        "Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar).",
        "Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario.",
        "Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs.",
        "Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente).",
        "Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente.",
        "Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff.",
        "Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios.",
        "Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta.",
        "La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta.",
        "No sorprende que CD+CDiff mejore con más clics.",
        "Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos.",
        "De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff.",
        "Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días).",
        "Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación.",
        "Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada.",
        "Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles.",
        "La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo.",
        "Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625).",
        "Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción.",
        "Ahora resumimos brevemente nuestros resultados experimentales.",
        "Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios.",
        "Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda).",
        "Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N.",
        "Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic.",
        "Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7.",
        "CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web.",
        "Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas.",
        "Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas.",
        "Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web.",
        "Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario.",
        "También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas.",
        "Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics.",
        "Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos.",
        "Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes.",
        "Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web.",
        "Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas.",
        "Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1].",
        "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios.",
        "Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia.",
        "Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda.",
        "Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas.",
        "Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas.",
        "Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora.",
        "Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos.",
        "Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas.",
        "Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web.",
        "Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados.",
        "Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web.",
        "REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan.",
        "Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos.",
        "En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala.",
        "En Actas de WWW7, 107-117, 1998. [4] C.J.C.",
        "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M.",
        "Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda.",
        "Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White.",
        "Evaluando medidas implícitas para mejorar la experiencia de búsqueda.",
        "En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick.",
        "Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal.",
        "En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico.",
        "Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía.",
        "En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl.",
        "GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet.",
        "En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia.",
        "En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim.",
        "Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim.",
        "Modelando el contenido de la información utilizando el comportamiento observable.",
        "En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web.",
        "Trabajando con la tecnología en mente: Brunswikiano.",
        "Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill.",
        "Introducción a la recuperación de información moderna.",
        "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001"
    ],
    "error_count": 4,
    "keys": {
        "relevance measurement": {
            "translated_key": "medición de la relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION <br>relevance measurement</br> is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "INTRODUCTION <br>relevance measurement</br> is crucial to web search and to information retrieval in general."
            ],
            "translated_annotated_samples": [
                "La <br>medición de la relevancia</br> es crucial para la búsqueda en la web y para la recuperación de información en general."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La <br>medición de la relevancia</br> es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "implicit feedback": {
            "translated_key": "retroalimentación implícita",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting <br>implicit feedback</br> is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published <br>implicit feedback</br> interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable <br>implicit feedback</br> through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether <br>implicit feedback</br> could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general <br>implicit feedback</br> interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published <br>implicit feedback</br> interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of <br>implicit feedback</br> modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other <br>implicit feedback</br> interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as <br>implicit feedback</br>, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, <br>implicit feedback</br> for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "<br>implicit feedback</br> for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from <br>implicit feedback</br>, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from <br>implicit feedback</br>, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Our key insight to improving robustness of interpreting <br>implicit feedback</br> is to model query-dependent deviations from the expected noisy user behavior.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published <br>implicit feedback</br> interpretation methods.",
                "At the same time, millions of people interact daily with web search engines, providing valuable <br>implicit feedback</br> through their interactions with the search results.",
                "Oard and Kim [15] studied whether <br>implicit feedback</br> could substitute for explicit ratings in recommender systems.",
                "We attempt to learn a general <br>implicit feedback</br> interpretation strategy automatically instead of relying on heuristics or insights."
            ],
            "translated_annotated_samples": [
                "Nuestra clave para mejorar la robustez en la interpretación de la <br>retroalimentación implícita</br> es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario.",
                "Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de <br>retroalimentación implícita</br> publicados.",
                "Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos <br>comentarios implícitos</br> a través de sus interacciones con los resultados de búsqueda.",
                "Oard y Kim [15] estudiaron si la <br>retroalimentación implícita</br> podría sustituir a las calificaciones explícitas en los sistemas de recomendación.",
                "Intentamos aprender una estrategia general de interpretación de <br>retroalimentación implícita</br> de forma automática en lugar de depender de heurísticas o percepciones."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la <br>retroalimentación implícita</br> es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de <br>retroalimentación implícita</br> publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos <br>comentarios implícitos</br> a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la <br>retroalimentación implícita</br> podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de <br>retroalimentación implícita</br> de forma automática en lugar de depender de heurísticas o percepciones. ",
            "candidates": [],
            "error": [
                [
                    "retroalimentación implícita",
                    "retroalimentación implícita",
                    "comentarios implícitos",
                    "retroalimentación implícita",
                    "retroalimentación implícita"
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to <br>information retrieval</br> in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving <br>information retrieval</br> systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the <br>information retrieval</br> community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information retrieval</br>.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in <br>information retrieval</br>.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern <br>information retrieval</br>.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Relevance measurement is crucial to web search and to <br>information retrieval</br> in general.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving <br>information retrieval</br> systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the <br>information retrieval</br> community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information retrieval</br>.",
                "Ranking search results to predict user preferences is a fundamental problem in <br>information retrieval</br>."
            ],
            "translated_annotated_samples": [
                "La medición de la relevancia es crucial para la búsqueda en la web y para la <br>recuperación de información</br> en general.",
                "Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los <br>sistemas de recuperación de información</br>.",
                "Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de <br>recuperación de información</br>, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web.",
                "ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la <br>recuperación de información</br>.",
                "La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la <br>recuperación de información</br>."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la <br>recuperación de información</br> en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los <br>sistemas de recuperación de información</br>. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de <br>recuperación de información</br>, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la <br>recuperación de información</br>. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la <br>recuperación de información</br>. ",
            "candidates": [],
            "error": [
                [
                    "recuperación de información",
                    "sistemas de recuperación de información",
                    "recuperación de información",
                    "recuperación de información",
                    "recuperación de información"
                ]
            ]
        },
        "clickthrough": {
            "translated_key": "interpretación de clics",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of <br>clickthrough</br> interpretation improves prediction accuracy over state-of-the-art <br>clickthrough</br> methods.",
                "We generalize our approach to model user behavior beyond <br>clickthrough</br>, which results in higher preference prediction accuracy than models based on <br>clickthrough</br> information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing <br>clickthrough</br> strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that <br>clickthrough</br> was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on <br>clickthrough</br> data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting <br>clickthrough</br> evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret <br>clickthrough</br> events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result <br>clickthrough</br>, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, <br>clickthrough</br> and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative <br>clickthrough</br> frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the <br>clickthrough</br> statistics and explicitly modeling post-search user behavior.",
                "Although <br>clickthrough</br> distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the <br>clickthrough</br> feature, if we subtract the background distribution (i.e., the expected <br>clickthrough</br> for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, <br>clickthrough</br>, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "<br>clickthrough</br> features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the <br>clickthrough</br>-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL <br>clickthrough</br> features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 <br>clickthrough</br> Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our <br>clickthrough</br> models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed <br>clickthrough</br> feature f as the deviation from the expected (background) <br>clickthrough</br> distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected <br>clickthrough</br> at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous <br>clickthrough</br> interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the <br>clickthrough</br> frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed <br>clickthrough</br> frequencies.",
                "As we discussed, <br>clickthrough</br> is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above <br>clickthrough</br> strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of <br>clickthrough</br> distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our <br>clickthrough</br> distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all <br>clickthrough</br>-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff <br>clickthrough</br> interpretation strategy can be improved upon by automatically learning to interpret the aggregated <br>clickthrough</br> evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and <br>clickthrough</br>-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features <br>clickthrough</br> Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over <br>clickthrough</br>-only features achieves substantially higher recall and precision than human-designed <br>clickthrough</br>-interpretation strategies described earlier.",
                "For example, the <br>clickthrough</br>-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our <br>clickthrough</br> and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our <br>clickthrough</br>-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff <br>clickthrough</br> strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering <br>clickthrough</br> alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting <br>clickthrough</br> evidence by aggregating across users and queries.",
                "Our methods result in <br>clickthrough</br> interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates <br>clickthrough</br>, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited <br>clickthrough</br> information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc <br>clickthrough</br> interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about <br>clickthrough</br> distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using <br>clickthrough</br> Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting <br>clickthrough</br> Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "We show that our model of <br>clickthrough</br> interpretation improves prediction accuracy over state-of-the-art <br>clickthrough</br> methods.",
                "We generalize our approach to model user behavior beyond <br>clickthrough</br>, which results in higher preference prediction accuracy than models based on <br>clickthrough</br> information alone.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing <br>clickthrough</br> strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "They found that <br>clickthrough</br> was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on <br>clickthrough</br> data to learn ranking functions."
            ],
            "translated_annotated_samples": [
                "Mostramos que nuestro modelo de <br>interpretación de clics</br> mejora la precisión de predicción sobre los métodos de clics más avanzados.",
                "Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del <br>clic</br>, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de <br>clic</br>s.",
                "Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de <br>clics</br> para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6).",
                "Encontraron que el <br>clic</br> fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página.",
                "Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en <br>datos de clics</br> para aprender funciones de clasificación."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de <br>interpretación de clics</br> mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del <br>clic</br>, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de <br>clic</br>s. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de <br>clics</br> para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el <br>clic</br> fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en <br>datos de clics</br> para aprender funciones de clasificación. ",
            "candidates": [],
            "error": [
                [
                    "interpretación de clics",
                    "clic",
                    "clic",
                    "clics",
                    "clic",
                    "datos de clics"
                ]
            ]
        },
        "position of top relevant document": {
            "translated_key": "posición del documento pertinente superior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying <br>position of top relevant document</br> (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (<br>position of top relevant document</br>). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Figure 3.2 reports the aggregated click distribution for queries with varying <br>position of top relevant document</br> (PTR).",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (<br>position of top relevant document</br>). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant)."
            ],
            "translated_annotated_samples": [
                "La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR).",
                "Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "top relevant document position": {
            "translated_key": "posición del documento más relevante",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "induce weight": {
            "translated_key": "pesos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to <br>induce weight</br>s for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Our general approach is to train a classifier to <br>induce weight</br>s for the user behavior features, and consequently derive a predictive model of user preferences."
            ],
            "translated_annotated_samples": [
                "Nuestro enfoque general es entrenar un clasificador para inducir <br>pesos</br> a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir <br>pesos</br> a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "predictive model": {
            "translated_key": "modelo predictivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a <br>predictive model</br> of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a <br>predictive model</br> of user preferences."
            ],
            "translated_annotated_samples": [
                "Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un <br>modelo predictivo</br> de las preferencias del usuario."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un <br>modelo predictivo</br> de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "user preference": {
            "translated_key": "preferencias de usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring <br>user preference</br>: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring <br>user preference</br>: A bibliography."
            ],
            "translated_annotated_samples": [
                "Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir <br>preferencias de usuario</br>: una bibliografía."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir <br>preferencias de usuario</br>: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "page dwell time": {
            "translated_key": "tiempo de permanencia en la página",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., <br>page dwell time</br>) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected <br>page dwell time</br> for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter <br>page dwell time</br> than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage <br>page dwell time</br> CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and <br>page dwell time</br> (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "We conjecture that other aspects of user behavior (e.g., <br>page dwell time</br>) are similarly distorted.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected <br>page dwell time</br> for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter <br>page dwell time</br> than transactional or informational queries).",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage <br>page dwell time</br> CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and <br>page dwell time</br> (Section 4.3)."
            ],
            "translated_annotated_samples": [
                "Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el <br>tiempo de permanencia en la página</br>) también están distorsionados de manera similar.",
                "Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta.",
                "Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un <br>tiempo de permanencia en la página</br> más corto que las consultas transaccionales o informativas).",
                "Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario.",
                "Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el <br>tiempo de permanencia en la página</br> (Sección 4.3)."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el <br>tiempo de permanencia en la página</br>) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un <br>tiempo de permanencia en la página</br> más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el <br>tiempo de permanencia en la página</br> (Sección 4.3). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "follow-up queries": {
            "translated_key": "consultas de seguimiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as <br>follow-up queries</br> and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as <br>follow-up queries</br> and page dwell time (Section 4.3)."
            ],
            "translated_annotated_samples": [
                "Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las <br>consultas de seguimiento</br> y el tiempo de permanencia en la página (Sección 4.3)."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las <br>consultas de seguimiento</br> y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "explicit relevance judgment": {
            "translated_key": "juicios de relevancia explícitos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and <br>explicit relevance judgment</br>s for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to <br>explicit relevance judgment</br>s and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have <br>explicit relevance judgment</br>s for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any <br>explicit relevance judgment</br>s available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with <br>explicit relevance judgment</br>s as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and <br>explicit relevance judgment</br>s for both individual queries and search sessions.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to <br>explicit relevance judgment</br>s and preferences.",
                "We also have <br>explicit relevance judgment</br>s for the top 10 results for each query.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any <br>explicit relevance judgment</br>s available for each query/result pair.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with <br>explicit relevance judgment</br>s as input to RankNet which learns an optimal weighting of features to predict preferences."
            ],
            "translated_annotated_samples": [
                "Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y <br>juicios explícitos de relevancia</br> tanto para consultas individuales como para sesiones de búsqueda.",
                "MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con <br>juicios de relevancia explícitos</br> y preferencias.",
                "También tenemos <br>juicios de relevancia explícitos</br> para los 10 mejores resultados de cada consulta.",
                "Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier <br>juicio de relevancia explícito</br> disponible para cada par consulta/resultado.",
                "Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con <br>juicios de relevancia explícitos</br> como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y <br>juicios explícitos de relevancia</br> tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con <br>juicios de relevancia explícitos</br> y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos <br>juicios de relevancia explícitos</br> para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier <br>juicio de relevancia explícito</br> disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con <br>juicios de relevancia explícitos</br> como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. ",
            "candidates": [],
            "error": [
                [
                    "juicios explícitos de relevancia",
                    "juicios de relevancia explícitos",
                    "juicios de relevancia explícitos",
                    "juicio de relevancia explícito",
                    "juicios de relevancia explícitos"
                ]
            ]
        },
        "predictive behavior model": {
            "translated_key": "modelo de comportamiento predictivo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a <br>predictive behavior model</br> Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a <br>predictive behavior model</br> we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a <br>predictive behavior model</br> Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "Recall that to learn a <br>predictive behavior model</br> we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences."
            ],
            "translated_annotated_samples": [
                "Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un <br>modelo de comportamiento predictivo</br> Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario.",
                "Recuerde que para aprender un <br>modelo de comportamiento predictivo</br> utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un <br>modelo de comportamiento predictivo</br> Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un <br>modelo de comportamiento predictivo</br> utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "recall measure": {
            "translated_key": "medida de recuerdo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "precision measure": {
            "translated_key": "medida de precisión",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "low recall": {
            "translated_key": "baja recuperación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but <br>low recall</br> (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but <br>low recall</br> (i.e., do not attempt to predict relevance of many search results)."
            ],
            "translated_annotated_samples": [
                "Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero <br>baja recuperación</br> (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda)."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero <br>baja recuperación</br> (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web search ranking": {
            "translated_key": "clasificación de búsqueda en la web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond <br>web search ranking</br> in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve <br>web search ranking</br> [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving <br>web search ranking</br> by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "We discuss other applications of our models beyond <br>web search ranking</br> in Section 7.",
                "A natural application of our preference prediction models is to improve <br>web search ranking</br> [1].",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving <br>web search ranking</br> by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan."
            ],
            "translated_annotated_samples": [
                "Discutimos otras aplicaciones de nuestros modelos más allá del <br>ranking de búsqueda</br> en la Sección 7.",
                "Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la <br>clasificación de búsqueda en la web</br> [1].",
                "REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la <br>clasificación de búsqueda en la web</br> mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del <br>ranking de búsqueda</br> en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la <br>clasificación de búsqueda en la web</br> [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la <br>clasificación de búsqueda en la web</br> mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    "ranking de búsqueda",
                    "clasificación de búsqueda en la web",
                    "clasificación de búsqueda en la web"
                ]
            ]
        },
        "click spam detection": {
            "translated_key": "detección de clics fraudulentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, <br>click spam detection</br>, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including <br>click spam detection</br>, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Accurate modeling and interpretation of user behavior has important applications to ranking, <br>click spam detection</br>, web search personalization, and other tasks.",
                "In addition, our work has many potential applications including <br>click spam detection</br>, search abuse detection, personalization, and domain-specific ranking."
            ],
            "translated_annotated_samples": [
                "La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, <br>detección de spam de clics</br>, personalización de la búsqueda web y otras tareas.",
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la <br>detección de clics fraudulentos</br>, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, <br>detección de spam de clics</br>, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la <br>detección de clics fraudulentos</br>, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    "detección de spam de clics",
                    "detección de clics fraudulentos"
                ]
            ]
        },
        "search abuse detection": {
            "translated_key": "detección de abuso en búsquedas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, <br>search abuse detection</br>, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "In addition, our work has many potential applications including click spam detection, <br>search abuse detection</br>, personalization, and domain-specific ranking."
            ],
            "translated_annotated_samples": [
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la <br>detección de abuso en búsquedas</br>, la personalización y la clasificación específica de dominios."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la <br>detección de abuso en búsquedas</br>, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "personalization": {
            "translated_key": "personalización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search <br>personalization</br>, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, <br>personalization</br>, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search <br>personalization</br>, and other tasks.",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, <br>personalization</br>, and domain-specific ranking."
            ],
            "translated_annotated_samples": [
                "La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, <br>personalización</br> de la búsqueda web y otras tareas.",
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la <br>personalización</br> y la clasificación específica de dominios."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, <br>personalización</br> de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la <br>personalización</br> y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "domain-specific ranking": {
            "translated_key": "clasificación específica de dominios",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and <br>domain-specific ranking</br>.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and <br>domain-specific ranking</br>."
            ],
            "translated_annotated_samples": [
                "Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la <br>clasificación específica de dominios</br>."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la <br>clasificación específica de dominios</br>. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "interpret implicit relevance feedback": {
            "translated_key": "interpretar la retroalimentación implícita de relevancia",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "user behavior model": {
            "translated_key": "modelo de comportamiento del usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust <br>user behavior model</br> Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General <br>user behavior model</br> The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned <br>user behavior model</br> described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the <br>user behavior model</br> on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically predict relevance preferences for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust <br>user behavior model</br> Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General <br>user behavior model</br> The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "Relative user preferences are then estimated using the learned <br>user behavior model</br> described in Section 3.4.",
                "For example, the <br>user behavior model</br> on intranet search may be different from the web search behavior."
            ],
            "translated_annotated_samples": [
                "Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 <br>Modelo Robusto de Comportamiento del Usuario</br> Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios.",
                "Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas.",
                "Las preferencias relativas de los usuarios se estiman luego utilizando el <br>modelo de comportamiento del usuario</br> aprendido descrito en la Sección 3.4.",
                "Por ejemplo, el <br>modelo de comportamiento del usuario</br> en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 <br>Modelo Robusto de Comportamiento del Usuario</br> Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el <br>modelo de comportamiento del usuario</br> aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el <br>modelo de comportamiento del usuario</br> en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    "Modelo Robusto de Comportamiento del Usuario",
                    "modelo de comportamiento del usuario",
                    "modelo de comportamiento del usuario"
                ]
            ]
        },
        "predict relevance preference": {
            "translated_key": "preferencias de relevancia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning User Interaction Models for Predicting Web Search Result Preferences Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com ABSTRACT Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance.",
                "We present a real-world study of modeling the behavior of web search users to predict web search result preferences.",
                "Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks.",
                "Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected noisy user behavior.",
                "We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods.",
                "We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone.",
                "We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Search process, relevance feedback.",
                "General Terms Algorithms, Measurement, Performance, Experimentation. 1.",
                "INTRODUCTION Relevance measurement is crucial to web search and to information retrieval in general.",
                "Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.",
                "However, explicit human ratings are expensive and difficult to obtain.",
                "At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.",
                "If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.",
                "Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.",
                "However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.",
                "Therefore, it is not clear whether these techniques will work for general real-world web search.",
                "A significant distinction is that web search is not controlled.",
                "Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.",
                "But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.",
                "By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.",
                "Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.",
                "Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.",
                "Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.",
                "We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.",
                "Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3). • Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4). • A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).",
                "We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "The most common approaches in the context of the web use both the similarity of the query to the page content, and the overall quality of a page [3, 20].",
                "A state-ofthe-art search engine may use hundreds of features to describe a candidate page, employing sophisticated algorithms to rank pages based on these features.",
                "Current search engines are commonly tuned on human relevance judgments.",
                "Human annotators rate a set of pages for a query according to perceived relevance, creating the gold standard against which different ranking algorithms can be evaluated.",
                "Reducing the dependence on explicit human judgments by using implicit relevance feedback has been an active topic of research.",
                "Several research groups have evaluated the relationship between implicit measures and user interest.",
                "In these studies, both reading time and explicit ratings of interest are collected.",
                "Morita and Shinoda [14] studied the amount of time that users spent reading Usenet news articles and found that reading time could predict a users interest levels.",
                "Konstan et al. [13] showed that reading time was a strong predictor of user interest in their GroupLens system.",
                "Oard and Kim [15] studied whether implicit feedback could substitute for explicit ratings in recommender systems.",
                "More recently, Oard and Kim [16] presented a framework for characterizing observable user behaviors using two dimensions-the underlying purpose of the observed behavior and the scope of the item being acted upon.",
                "Goecks and Shavlik [8] approximated human labels by collecting a set of page activity measures while users browsed the World Wide Web.",
                "The authors hypothesized correlations between a high degree of page activity and a users interest.",
                "While the results were promising, the sample size was small and the implicit measures were not tested against explicit judgments of user interest.",
                "Claypool et al. [6] studied how several implicit measures related to the interests of the user.",
                "They developed a custom browser called the Curious Browser to gather data, in a computer lab, about implicit interest indicators and to probe for explicit judgments of Web pages visited.",
                "Claypool et al. found that the time spent on a page, the amount of scrolling on a page, and the combination of time and scrolling have a strong positive relationship with explicit interest, while individual scrolling methods and mouse-clicks were not correlated with explicit interest.",
                "Fox et al. [7] explored the relationship between implicit and explicit measures in Web search.",
                "They built an instrumented browser to collect data and then developed Bayesian models to relate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "They found that clickthrough was the most important individual variable but that predictive accuracy could be improved by using additional variables, notably dwell time on a page.",
                "Joachims [9] developed valuable insights into the collection of implicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "More recently, Joachims et al. [10] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthrough events in a controlled, laboratory setting.",
                "A more comprehensive overview of studies of implicit measures is described in Kelly and Teevan [12].",
                "Unfortunately, the extent to which existing research applies to real-world web search is unclear.",
                "In this paper, we build on previous research to develop robust user behavior interpretation models for the real web search setting. 3.",
                "LEARNING USER BEHAVIOR MODELS As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.",
                "Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.",
                "Our main approach is to model user web search behavior as if it were generated by two components: a relevance component - queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.",
                "Our general idea is to model the deviations from the expected user behavior.",
                "Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.",
                "We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2). 3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.",
                "A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.",
                "In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.",
                "The data set is described in more detail in Section 5.2.",
                "For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.",
                "For these queries we aggregate click data over more than 120,000 searches performed over a three week period.",
                "We also have explicit relevance judgments for the top 10 results for each query.",
                "Figure 3.1 shows the relative clickthrough frequency as a function of result position.",
                "The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).",
                "These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.",
                "The resulting distribution agrees with previous observations that users click more often on top-ranked results.",
                "This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and noisewe attempt to separate these components in the analysis that follows. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.",
                "First we consider the distribution of clicks for the relevant documents for these queries.",
                "Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).",
                "While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.",
                "For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.",
                "Nevertheless, many users still click on the non-relevant results in position 1 for such queries.",
                "This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document). -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).",
                "If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.",
                "Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges. 3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.",
                "We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.",
                "Although clickthrough distributions are heavily biased towards top results, we have just shown how the relevance-driven click distribution can be recovered by correcting for the prior, background distribution.",
                "We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.",
                "Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.",
                "More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .",
                "In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.",
                "This aggregation gives additional robustness of not relying on individual noisy user interactions.",
                "In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.",
                "We now describe the actual features we use to represent user behavior. 3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.",
                "Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "This rich representation of user behavior is similar in many respects to the recent work by Fox et al. [7].",
                "An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general, queryindependent model of user behavior.",
                "Furthermore, we include derived, distributional features computed as described above.",
                "The features we use to represent user search interactions are summarized in Table 3.1.",
                "For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.",
                "Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.",
                "To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.",
                "These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.",
                "Browsing features: Simple aspects of the user web page interactions can be captured and quantified.",
                "These features are used to characterize interactions with pages beyond the results page.",
                "For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.",
                "These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).",
                "We include both the direct features and the derived features described above.",
                "Clickthrough features: Clicks are a special case of user interaction with the search engine.",
                "We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.",
                "For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component. well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).",
                "The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.",
                "Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.",
                "We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.",
                "We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.",
                "Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.",
                "The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.",
                "For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.",
                "We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.",
                "To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.",
                "More specifically, for each judged query we check if a result link has been judged.",
                "If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.",
                "These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.",
                "RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 4.",
                "PREDICTING USER PREFERENCES In our experiments, we explore several models for predicting user preferences.",
                "These models range from using no implicit user feedback to using all available implicit user feedback.",
                "Ranking search results to predict user preferences is a fundamental problem in information retrieval.",
                "Most traditional IR and web search approaches use a combination of page and link features to rank search results, and a representative state-ofthe-art ranking system will be used as our baseline ranker (Section 4.1).",
                "At the same time, user interactions with a search engine provide a wealth of information.",
                "A commonly considered type of interaction is user clicks on search results.",
                "Previous work [9], as described above, also examined which results were skipped (e.g., skip above and skip next) and other related strategies to induce preference judgments from the users skipping over results and not clicking on following results.",
                "We have also added refinements of these strategies to take into account the variability observed in realistic web scenarios.. We describe these strategies in Section 4.2.",
                "As clickthroughs are just one aspect of user interaction, we extend the relevance estimation by introducing a machine learning model that incorporates clicks as well as other aspects of user behavior, such as follow-up queries and page dwell time (Section 4.3).",
                "We conclude this section by briefly describing our baseline - a state-of-the-art ranking algorithm used by an operational web search engine. 4.1 Baseline Model A key question is whether browsing behavior can provide information absent from existing explicit judgments used to train an existing ranker.",
                "For our baseline system we use a state-of-theart page ranking system currently used by a major web search engine.",
                "Hence, we will call this system Current for the subsequent discussion.",
                "While the specific algorithms used by the search engine are beyond the scope of this paper, the algorithm ranks results based on hundreds of features such as query to document similarity, query to anchor text similarity, and intrinsic page quality.",
                "The Current web search engine rankings provide a strong system for comparison and experiments of the next two sections. 4.2 Clickthrough Model If we assume that every user click was motivated by a rational process that selected the most promising result summary, we can then interpret each click as described in Joachims et al.[10].",
                "By studying eye tracking and comparing clicks with explicit judgments, they identified a few basic strategies.",
                "We discuss the two strategies that performed best in their experiments, Skip Above and Skip Next.",
                "Strategy SA (Skip Above): For a set of results for a query and a clicked result at position p, all unclicked results ranked above p are predicted to be less relevant than the result at p. In addition to information about results above the clicked result, we also have information about the result immediately following the clicked one.",
                "Eye tracking study performed by Joachims et al. [10] showed that users usually consider the result immediately following the clicked result in current ranking.",
                "Their Skip Next strategy uses this observation to predict that a result following the clicked result at p is less relevant than the clicked result, with accuracy comparable to the SA strategy above.",
                "For better coverage, we combine the SA strategy with this extension to derive the Skip Above + Skip Next strategy: Strategy SA+N (Skip Above + Skip Next): This strategy predicts all un-clicked results immediately following a clicked result as less relevant than the clicked result, and combines these predictions with those of the SA strategy above.",
                "We experimented with variations of these strategies, and found that SA+N outperformed both SA and the original Skip Next strategy, so we will consider the SA and SA+N strategies in the rest of the paper.",
                "These strategies are motivated and empirically tested for individual users in a laboratory setting.",
                "As we will show, these strategies do not work as well in real web search setting due to inherent inconsistency and noisiness of individual users behavior.",
                "The general approach for using our clickthrough models directly is to filter clicks to those that reflect higher-than-chance click frequency.",
                "We then use the same SA and SA+N strategies, but only for clicks that have higher-than-expected frequency according to our model.",
                "For this, we estimate the relevance component rel(q,r,f) of the observed clickthrough feature f as the deviation from the expected (background) clickthrough distribution )( fC .",
                "Strategy CD (deviation d): For a given query, compute the observed click frequency distribution o(r, p) for all results r in positions p. The click deviation for a result r in position p, dev(r, p) is computed as: )(),(),( pCproprdev −= where C(p) is the expected clickthrough at position p. If dev(r,p)>d, retain the click as input to the SA+N strategy above, and apply SA+N strategy over the filtered set of click events.",
                "The choice of d selects the tradeoff between recall and precision.",
                "While the above strategy extends SA and SA+N, it still assumes that a (filtered) clicked result is preferred over all unclicked results presented to the user above a clicked position.",
                "However, for informational queries, multiple results may be clicked, with varying frequency.",
                "Hence, it is preferable to individually compare results for a query by considering the difference between the estimated relevance components of the click distribution of the corresponding query results.",
                "We now define a generalization of the previous clickthrough interpretation strategy: Strategy CDiff (margin m): Compute deviation dev(r,p) for each result r1...rn in position p. For each pair of results ri and rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.",
                "As in CD, the choice of m selects the tradeoff between recall and precision.",
                "The pairs may be preferred in the original order or in reverse of it.",
                "Given the margin, two results might be effectively indistinguishable, but only one can possibly be preferred over the other.",
                "Intuitively, CDiff generalizes the skip idea above to include cases where the user skipped (i.e., clicked less than expected) on uj and preferred (i.e., clicked more than expected) on ui.",
                "Furthermore, this strategy allows for differentiation within the set of clicked results, making it more appropriate to noisy user behavior.",
                "CDiff and CD are complimentary.",
                "CDiff is a generalization of the clickthrough frequency model of CD, but it ignores the positional information used in CD.",
                "Hence, combining the two strategies to improve coverage is a natural approach: Strategy CD+CDiff (deviation d, margin m): Union of CD and CDiff predictions.",
                "Other variations of the above strategies were considered, but these five methods cover the range of observed performance. 4.3 General User Behavior Model The strategies described in the previous section generate orderings based solely on observed clickthrough frequencies.",
                "As we discussed, clickthrough is just one, albeit important, aspect of user interactions with web search engine results.",
                "We now present our general strategy that relies on the automatically derived predictive user behavior models (Section 3).",
                "The UserBehavior Strategy: For a given query, each result is represented with the features in Table 3.1.",
                "Relative user preferences are then estimated using the learned user behavior model described in Section 3.4.",
                "Recall that to learn a predictive behavior model we used the features from Table 3.1 along with explicit relevance judgments as input to RankNet which learns an optimal weighting of features to predict preferences.",
                "This strategy models user interaction with the search engine, allowing it to benefit from the wisdom of crowds interacting with the results and the pages beyond.",
                "As our experiments in the subsequent sections demonstrate, modeling a richer set of user interactions beyond clickthroughs results in more accurate predictions of user preferences. 5.",
                "EXPERIMENTAL SETUP We now describe our experimental setup.",
                "We first describe the methodology used, including our evaluation metrics (Section 5.1).",
                "Then we describe the datasets (Section 5.2) and the methods we compared in this study (Section 5.3). 5.1 Evaluation Methodology and Metrics Our evaluation focuses on the pairwise agreement between preferences for results.",
                "This allows us to compare to previous work [9,10].",
                "Furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].",
                "The evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.",
                "We discuss other applications of our models beyond web search ranking in Section 7.",
                "To create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.",
                "To avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).",
                "In order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard Recall and Precision measures [20].",
                "While our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.",
                "Specifically, we report the average query recall and precision.",
                "For our task, Query Precision and Query Recall for a query q are defined as: • Query Precision: Fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment. • Query Recall: Fraction of preferences obtained from explicit human judgment for q that were correctly predicted.",
                "The overall Recall and Precision are computed as the average of Query Recall and Query Precision, respectively.",
                "A drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.",
                "We discuss this issue further when we consider extensions to the current work in Section 7. 5.2 Datasets For evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.",
                "For each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.",
                "In addition for these queries we also had user interaction data for more than 120,000 instances of these queries.",
                "The user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.",
                "This data collection was part of voluntary opt-in feedback submitted by users from October 11 through October 31.",
                "These three weeks (21 days) of user interaction data was filtered to include only the users in the English-U.S. market.",
                "In order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (Q1, Q10, and Q20) that contain different amounts of interaction data: • Q1: Human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-URL pairs) • Q10: Queries in Q1 with at least 10 clicks (1300 queries, 18,728 query-URL pairs). • Q20: Queries in Q1 with at least 20 clicks (1000 queries total, 12,922 query-URL pairs).",
                "These datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.",
                "Furthermore, the data size is order of magnitude larger than any study reported in the literature. 5.3 Methods Compared We considered a number of methods for comparison.",
                "We compared our UserBehavior model (Section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (Section 4.2), and to the current search engine ranking based on query and page features alone (Section 4.1).",
                "Specifically, we compare the following strategies: • SA: The Skip Above clickthrough strategy (Section 4.2) • SA+N: A more comprehensive extension of SA that takes better advantage of current search engine ranking. • CD: Our refinement of SA+N that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (Section 4.2). • CDiff: Our generalization of the CD strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (Section 4.2). • CD+CDiff: The strategy combining CD and CDiff as the union of predicted preferences from both (Section 4.2). • UserBehavior: We order predictions based on decreasing highest score of any page.",
                "In our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.",
                "This heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (Section 4.3) • Current: Current search engine ranking (section 4.1).",
                "Note that the Current ranker implementation was trained over a superset of the rated query/URL pairs in our datasets, but using the same truth labels as we do for our evaluation.",
                "Training/Test Split: The only strategy for which splitting the datasets into training and test was required was the UserBehavior method.",
                "To evaluate UserBehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.",
                "The sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).",
                "It is worth noting that both the ad-hoc SA and SA+N, as well as the distribution-based strategies (CD, CDiff, and CD+CDiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.",
                "Hence, all strategies except for UserBehavior were tested on the full set of queries and associated relevance preferences, while UserBehavior was tested on a randomly chosen hold-out subset of the queries as described above.",
                "To make sure we are not favoring UserBehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 6.",
                "RESULTS We now turn to experimental evaluation of predicting relevance preference of web search results.",
                "Figure 6.1 shows the recall-precision results over the Q1 query set (Section 5.2).",
                "The results indicate that previous click interpretation strategies, SA and SA+N perform suboptimally in this setting, exhibiting precision 0.627 and 0.638 respectively.",
                "Furthermore, there is no mechanism to do recall-precision trade-off with SA and SA+N, as they do not provide prediction confidence.",
                "In contrast, our clickthrough distribution-based techniques CD and CD+CDiff exhibit somewhat higher precision than SA and SA+N (0.648 and 0.717 at Recall of 0.08, maximum achieved by SA or SA+N).",
                "SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior, and Current relevance prediction methods over the Q1 dataset.",
                "Interestingly, CDiff alone exhibits precision equal to SA (0.627) at the same recall at 0.08.",
                "In contrast, by combining CD and CDiff strategies (CD+CDiff method) we achieve the best performance of all clickthrough-based strategies, exhibiting precision of above 0.66 for recall values up to 0.14, and higher at lower recall levels.",
                "Clearly, aggregating and intelligently interpreting clickthroughs, results in significant gain for realistic web search, than previously described strategies.",
                "However, even the CD+CDiff clickthrough interpretation strategy can be improved upon by automatically learning to interpret the aggregated clickthrough evidence.",
                "But first, we consider the best performing strategy, UserBehavior.",
                "Incorporating post-search navigation history in addition to clickthroughs (Browsing features) results in the highest recall and precision among all methods compared.",
                "Browse exhibits precision of above 0.7 at recall of 0.16, significantly outperforming our Baseline and clickthrough-only strategies.",
                "Furthermore, Browse is able to achieve high recall (as high as 0.43) while maintaining precision (0.67) significantly higher than the baseline ranking.",
                "To further analyze the value of different dimensions of implicit feedback modeled by the UserBehavior strategy, we consider each group of features in isolation.",
                "Figure 6.2 reports Precision vs. Recall for each feature group.",
                "Interestingly, Query-text alone has low accuracy (only marginally better than Random).",
                "Furthermore, Browsing features alone have higher precision (with lower maximum recall achieved) than considering all of the features in our UserBehavior model.",
                "Applying different machine learning methods for combining classifier predictions may increase performance of using all features for all recall values. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recall Precision All Features Clickthrough Query-text Browsing Figure 6.2: Precision vs. recall for predicting relevance with each group of features individually. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recall Precision CD+CDiff:Q1 UserBehavior:Q1 CD+CDiff:Q10 UserBehavior:Q10 CD+CDiff:Q20 UserBehavior:Q20 Figure 6.3: Recall vs.",
                "Precision of CD+CDiff and UserBehavior for query sets Q1, Q10, and Q20 (queries with at least 1, at least 10, and at least 20 clicks respectively).",
                "Interestingly, the ranker trained over Clickthrough-only features achieves substantially higher recall and precision than human-designed clickthrough-interpretation strategies described earlier.",
                "For example, the clickthrough-trained classifier achieves 0.67 precision at 0.42 Recall vs. the maximum recall of 0.14 achieved by the CD+CDiff strategy.",
                "Our clickthrough and user behavior interpretation strategies rely on extensive user interaction data.",
                "We consider the effects of having sufficient interaction data available for a query before proposing a re-ranking of results for that query.",
                "Figure 6.3 reports recall-precision curves for the CD+CDiff and UserBehavior methods for different test query sets with at least 1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per query.",
                "Not surprisingly, CD+CDiff improves with more clicks.",
                "This indicates that accuracy will improve as more user interaction histories become available, and more queries from the Q1 set will have comprehensive interaction histories.",
                "Similarly, the UserBehavior strategy performs better for queries with 10 and 20 clicks, although the improvement is less dramatic than for CD+CDiff.",
                "For queries with sufficient clicks, CD+CDiff exhibits precision comparable with Browse at lower recall. 0 0.05 0.1 0.15 0.2 7 12 17 21 Days of user interaction data harvested Recall CD+CDiff UserBehavior Figure 6.4: Recall of CD+CDiff and UserBehavior strategies at fixed minimum precision 0.7 for varying amounts of user activity data (7, 12, 17, 21 days).",
                "Our techniques often do not make relevance predictions for search results (i.e., if no interaction data is available for the lower-ranked results), consequently maintaining higher precision at the expense of recall.",
                "In contrast, the current search engine always makes a prediction for every result for a given query.",
                "As a consequence, the recall of Current is high (0.627) at the expense of lower precision As another dimension of acquiring training data we consider the learning curve with respect to amount (days) of training data available.",
                "Figure 6.4 reports the Recall of CD+CDiff and UserBehavior strategies for varying amounts of training data collected over time.",
                "We fixed minimum precision for both strategies at 0.7 as a point substantially higher than the baseline (0.625).",
                "As expected, Recall of both strategies improves quickly with more days of interaction data examined.",
                "We now briefly summarize our experimental results.",
                "We showed that by intelligently aggregating user clickthroughs across queries and users, we can achieve higher accuracy on predicting user preferences.",
                "Because of the skewed distribution of user clicks our clickthrough-only strategies have high precision, but low recall (i.e., do not attempt to predict relevance of many search results).",
                "Nevertheless, our CD+CDiff clickthrough strategy outperforms most recent state-of-the-art results by a large margin (0.72 precision for CD+CDiff vs. 0.64 for SA+N) at the highest recall level of SA+N.",
                "Furthermore, by considering the comprehensive UserBehavior features that model user interactions after the search and beyond the initial click, we can achieve substantially higher precision and recall than considering clickthrough alone.",
                "Our UserBehavior strategy achieves recall of over 0.43 with precision of over 0.67 (with much higher precision at lower recall levels), substantially outperforms the current search engine preference ranking and all other implicit feedback interpretation methods. 7.",
                "CONCLUSIONS AND FUTURE WORK Our paper is the first, to our knowledge, to interpret postsearch user behavior to estimate user preferences in a real web search setting.",
                "We showed that our robust models result in higher prediction accuracy than previously published techniques.",
                "We introduced new, robust, probabilistic techniques for interpreting clickthrough evidence by aggregating across users and queries.",
                "Our methods result in clickthrough interpretation substantially more accurate than previously published results not specifically designed for web search scenarios.",
                "Our methods predictions of relevance preferences are substantially more accurate than the current state-of-the-art search result ranking that does not consider user interactions.",
                "We also presented a general model for interpreting post-search user behavior that incorporates clickthrough, browsing, and query features.",
                "By considering the complete search experience after the initial query and click, we demonstrated prediction accuracy far exceeding that of interpreting only the limited clickthrough information.",
                "Furthermore, we showed that automatically learning to interpret user behavior results in substantially better performance than the human-designed ad-hoc clickthrough interpretation strategies.",
                "Another benefit of automatically learning to interpret user behavior is that such methods can adapt to changing conditions and changing user profiles.",
                "For example, the user behavior model on intranet search may be different from the web search behavior.",
                "Our general UserBehavior method would be able to adapt to these changes by automatically learning to map new behavior patterns to explicit relevance ratings.",
                "A natural application of our preference prediction models is to improve web search ranking [1].",
                "In addition, our work has many potential applications including click spam detection, search abuse detection, personalization, and domain-specific ranking.",
                "For example, our automatically derived behavior models could be trained on examples of search abuse or click spam behavior instead of relevance labels.",
                "Alternatively, our models could be used directly to detect anomalies in user behavior - either due to abuse or to operational problems with the search engine.",
                "While our techniques perform well on average, our assumptions about clickthrough distributions (and learning the user behavior models) may not hold equally well for all queries.",
                "For example, queries with divergent access patterns (e.g., for ambiguous queries with multiple meanings) may result in behavior inconsistent with the model learned for all queries.",
                "Hence, clustering queries and learning different predictive models for each query type is a promising research direction.",
                "Query distributions also change over time, and it would be productive to investigate how that affects the predictive ability of these models.",
                "Furthermore, some predicted preferences may be more valuable than others, and we plan to investigate different metrics to capture the utility of the predicted preferences.",
                "As we showed in this paper, using the wisdom of crowds can give us accurate interpretation of user interactions even in the inherently noisy web search setting.",
                "Our techniques allow us to automatically <br>predict relevance preference</br>s for web search results with accuracy greater than the previously published methods.",
                "The predicted relevance preferences can be used for automatic relevance evaluation and tuning, for deploying search in new settings, and ultimately for improving the overall web search experience. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking by Incorporating User Behavior, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan.",
                "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents.",
                "In Proceedings of TREC 2003, 24-37, 2004. [3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine,.",
                "In Proceedings of WWW7, 107-117, 1998. [4] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning (ICML), 2005 [5] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest, in IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [8] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, in Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [11] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [14] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [15] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems. in Proceedings of AAAI Workshop on Recommender Systems. 1998 [16] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback, in Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2005 [19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in the ICML Workshop on Learning in Web Search, 2005 [20] G. Salton and M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Overview of TREC, 2001"
            ],
            "original_annotated_samples": [
                "Our techniques allow us to automatically <br>predict relevance preference</br>s for web search results with accuracy greater than the previously published methods."
            ],
            "translated_annotated_samples": [
                "Nuestras técnicas nos permiten predecir automáticamente las <br>preferencias de relevancia</br> para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados."
            ],
            "translated_text": "Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las <br>preferencias de relevancia</br> para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}