{
    "id": "H-60",
    "original_text": "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ). We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events. By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative. The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1. INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects. With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications. Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone. This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term. The idf alone works better than the tf alone does. An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms. We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document. We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term. The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document. Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf . In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf . From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation. The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g.: informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved. An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1]. For example, consider a normalisation based on the maximal idf -value. Let T be the set of terms occurring in a collection. Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents. Can we interpret Pfreq , the normalised idf , as the probability that the term is informative? When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events. These observations are reported in section 3. We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events. In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term. The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory. In section 4, we link the results of the previous sections to probability theory. We show the steps from possible worlds to binomial distribution and Poisson distribution. In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf . Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2. BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers. In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model. The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability. The Poisson model was here applied to the term frequency of a term in a document. We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively. Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates. The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson. Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf . We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference. A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events. The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions. Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related. The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences. The Pareto distribution is used by [2] for term frequency normalisation. The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do. This makes Pareto interesting since Poisson is felt to be too radical on frequent events. We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based. This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3. FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments. We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0. The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise. P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal. The document containment probability reflect the chance that a document occurs in a collection. This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment. Containment could be derived, for example, from the size, quality, age, links, etc. of a document. For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability. We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document. P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0. Next, we define the frequency-based noise probability and the total noise probability for disjoint documents. We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection. Definition 1. The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2. The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf . Theorem 1. IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability. Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof. The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf . The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events. From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate. Still, idf works well. This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents. From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise. The occurrence and containment can be term specific. For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N). We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c). We obtain P(t∧d|c) = 1/ND(c). In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document. This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability. The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product. The disjointness assumption is typical for frequency-based probabilities. From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf . But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document? In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment. P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2. For independent documents, we use now the conjunction of negated events. Definition 3. The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N. Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability. We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2. The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof. The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function). With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents. Therefore, a term with n = N has the largest noise probability. For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents. In the disjoint case, the noise probability is one for a term that occurs in all documents. If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents. Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents. Alternatively, we can assume a constant containment and a term-dependent occurrence. If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document. The common assumption is that the average containment or occurrence probability is proportional to n(t). However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t). For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ . In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition. Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 . This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3. The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal. The entropy of a maximal informative signal is Hmax = e−1 . Proof. The probability and entropy follow from the derivation above. The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal. We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ . We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy. By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory. Next, we link independent documents to probability theory. 4. THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world. For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events. With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit. The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal. This shows the relationship of the Poisson distribution and information theory. After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability. First, we define the Poisson noise probability: Definition 4. The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k! For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true. Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy. Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5. THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1. We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection. Let ND := |D| and NT := |T| be the number of documents and terms, respectively. We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document. The document dimension in a collection space corresponds to the location (position) dimension in a document space. The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document. For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment. We have highlighted in this section the duality between the collection space and the document space. We concentrate in this paper on the probability of a term to be noisy and informative. Those probabilities are defined in the collection space. However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document. Also, the results can be applied to containment of documents and locations. 6. THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities. In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions. Definition 5. The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5). Definition 6. The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1. Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ . We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t). The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ. Consider the illustration of the noise and informativeness definitions in figure 1. The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs. The frequency-based noise corresponds to the linear solid curve in the noise figure. With an independence assumption, we obtain the curve in the lower triangle of the noise figure. By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve. The noise figure shows the lifting for the value λ := ln N ≈ 9.2. The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ. Whether we can draw more conclusions from this setting is an open question. We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy. On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative. The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ. For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one. This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution. Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5. The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy. For the informativeness, we observe that the radical behaviour of Poisson is preserved. The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5. The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 . That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero. This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little. The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness. The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do. However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next. For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance. For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t). In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does. Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters. The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters. The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit. The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents. Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number. This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms. In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability. This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7. SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches. We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint. By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document. We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability. By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson. From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative. The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively. We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson. The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper. Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively. Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document. This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models. The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems. Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces. My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent. This work was funded by a research fellowship from Queen Mary University of London. 8. REFERENCES [1] A. Aizawa. An information-theoretic perspective of tf-idf measures. Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen. Term frequency normalization via Pareto distributions. In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew. Finding out about. Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson. Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein. Taschenbuch der Mathematik. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale. Poisson mixtures. Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale. Inverse document frequency: A measure of deviations from poisson. In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel. Links between information construction and information gain: Entropy and bibliometric distribution. Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis. N-poisson document modelling. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994. Springer-Verlag. [11] S. Wong and Y. Yao. An information-theoric measure of term specificity. Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao. On modeling information retrieval with probabilistic inference. ACM Transactions on Information Systems, 13(1):38-68, 1995. 234",
    "original_translation": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia. Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7. La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson. A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo. Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento. Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de recuperación de información. Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron. Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres. REFERENCIAS [1] A. Aizawa. Una perspectiva de teoría de la información de las medidas tf-idf. Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de la frecuencia de términos a través de distribuciones de Pareto. En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew. Descubriendo acerca de. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para indexación automática. Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein. Manual de matemáticas. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa de documentos: Una medida de desviaciones de Poisson. En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica. Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994. Springer-Verlag. [11] S. Wong y Y. Yao. Una medida de especificidad de términos basada en la teoría de la información. Revista de la Sociedad Americana de Ciencia de la Información, 43(1):54-61, 1992. [12] S. Wong y Y. Yao. En modelado de recuperación de información con inferencia probabilística. ACM Transactions on Information Systems, 13(1):38-68, 1995. 234\n\nTraducción: ACM Transactions on Information Systems, 13(1):38-68, 1995. 234",
    "original_sentences": [
        "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
        "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
        "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
        "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
        "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
        "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
        "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
        "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
        "The idf alone works better than the tf alone does.",
        "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
        "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
        "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
        "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
        "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
        "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
        "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
        "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
        "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
        "For example, consider a normalisation based on the maximal idf -value.",
        "Let T be the set of terms occurring in a collection.",
        "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
        "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
        "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
        "These observations are reported in section 3.",
        "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
        "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
        "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
        "In section 4, we link the results of the previous sections to probability theory.",
        "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
        "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
        "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
        "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
        "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
        "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
        "The Poisson model was here applied to the term frequency of a term in a document.",
        "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
        "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
        "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
        "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
        "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
        "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
        "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
        "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
        "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
        "The Pareto distribution is used by [2] for term frequency normalisation.",
        "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
        "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
        "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
        "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
        "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
        "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
        "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
        "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
        "The document containment probability reflect the chance that a document occurs in a collection.",
        "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
        "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
        "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
        "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
        "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
        "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
        "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
        "Definition 1.",
        "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
        "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
        "Theorem 1.",
        "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
        "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
        "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
        "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
        "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
        "Still, idf works well.",
        "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
        "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
        "The occurrence and containment can be term specific.",
        "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
        "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
        "We obtain P(t∧d|c) = 1/ND(c).",
        "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
        "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
        "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
        "The disjointness assumption is typical for frequency-based probabilities.",
        "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
        "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
        "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
        "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
        "For independent documents, we use now the conjunction of negated events.",
        "Definition 3.",
        "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
        "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
        "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
        "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
        "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
        "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
        "Therefore, a term with n = N has the largest noise probability.",
        "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
        "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
        "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
        "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
        "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
        "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
        "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
        "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
        "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
        "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
        "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
        "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
        "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
        "The entropy of a maximal informative signal is Hmax = e−1 .",
        "Proof.",
        "The probability and entropy follow from the derivation above.",
        "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
        "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
        "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
        "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
        "Next, we link independent documents to probability theory. 4.",
        "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
        "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
        "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
        "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
        "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
        "This shows the relationship of the Poisson distribution and information theory.",
        "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
        "First, we define the Poisson noise probability: Definition 4.",
        "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
        "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
        "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
        "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
        "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
        "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
        "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
        "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
        "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
        "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
        "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
        "We have highlighted in this section the duality between the collection space and the document space.",
        "We concentrate in this paper on the probability of a term to be noisy and informative.",
        "Those probabilities are defined in the collection space.",
        "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
        "Also, the results can be applied to containment of documents and locations. 6.",
        "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
        "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
        "Definition 5.",
        "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
        "Definition 6.",
        "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
        "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
        "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
        "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
        "Consider the illustration of the noise and informativeness definitions in figure 1.",
        "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
        "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
        "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
        "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
        "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
        "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
        "Whether we can draw more conclusions from this setting is an open question.",
        "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
        "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
        "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
        "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
        "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
        "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
        "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
        "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
        "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
        "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
        "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
        "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
        "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
        "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
        "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
        "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
        "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
        "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
        "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
        "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
        "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
        "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
        "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
        "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
        "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
        "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
        "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
        "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
        "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
        "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
        "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
        "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
        "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
        "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
        "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
        "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
        "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
        "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
        "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
        "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
        "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
        "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
        "This work was funded by a research fellowship from Queen Mary University of London. 8.",
        "REFERENCES [1] A. Aizawa.",
        "An information-theoretic perspective of tf-idf measures.",
        "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
        "Term frequency normalization via Pareto distributions.",
        "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
        "Finding out about.",
        "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
        "Probabilistic models for automatic indexing.",
        "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
        "Taschenbuch der Mathematik.",
        "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
        "Poisson mixtures.",
        "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
        "Inverse document frequency: A measure of deviations from poisson.",
        "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
        "Links between information construction and information gain: Entropy and bibliometric distribution.",
        "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
        "N-poisson document modelling.",
        "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
        "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
        "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
        "Springer-Verlag. [11] S. Wong and Y. Yao.",
        "An information-theoric measure of term specificity.",
        "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
        "On modeling information retrieval with probabilistic inference.",
        "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
    ],
    "translated_text_sentences": [
        "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf).",
        "Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos.",
        "Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos.",
        "El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1.",
        "INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados.",
        "Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones.",
        "Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf.",
        "Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término.",
        "El idf por sí solo funciona mejor que lo hace el tf por sí solo.",
        "Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos.",
        "Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento.",
        "Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término.",
        "El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento.",
        "Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf.",
        "En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo.",
        "Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística.",
        "La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia.",
        "Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1].",
        "Por ejemplo, considera una normalización basada en el valor máximo de idf.",
        "Sea T el conjunto de términos que ocurren en una colección.",
        "La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos.",
        "¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo?",
        "Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos.",
        "Estas observaciones se informan en la sección 3.",
        "Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos.",
        "En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término.",
        "El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información.",
        "En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad.",
        "Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson.",
        "En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf.",
        "Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2.",
        "ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores.",
        "En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico.",
        "La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal.",
        "El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento.",
        "Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente.",
        "Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas.",
        "Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson.",
        "Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf.",
        "En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística.",
        "Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos.",
        "La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas.",
        "La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados.",
        "La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf.",
        "La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos.",
        "La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson.",
        "Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes.",
        "Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas.",
        "Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3.",
        "De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos.",
        "Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0.",
        "La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario.",
        "P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual.",
        "La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección.",
        "La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención.",
        "La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento.",
        "Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N.",
        "Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento.",
        "P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0.",
        "A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos.",
        "Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección.",
        "Definición 1.",
        "La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2.",
        "La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico.",
        "Teorema 1.",
        "Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia.",
        "Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba.",
        "Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf.",
        "Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos.",
        "A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta.",
        "Sin embargo, el idf funciona bien.",
        "Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes.",
        "Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario.",
        "La ocurrencia y contención pueden ser específicas del término.",
        "Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N).",
        "Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c).",
        "Obtenemos P(t∧d|c) = 1/ND(c).",
        "En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento.",
        "Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia.",
        "La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf.",
        "La suposición de la disyunción es típica para las probabilidades basadas en frecuencia.",
        "Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico.",
        "¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento?",
        "En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento.",
        "P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2.",
        "Para documentos independientes, ahora utilizamos la conjunción de eventos negados.",
        "Definición 3.",
        "El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N.",
        "Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio.",
        "Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2.",
        "El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba.",
        "El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler).",
        "Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos.",
        "Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido.",
        "Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos.",
        "En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos.",
        "Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños.",
        "Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños.",
        "Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término.",
        "Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento.",
        "La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t).",
        "Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos.",
        "Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ.",
        "En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía.",
        "Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1.",
        "Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3.",
        "La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima.",
        "La entropía de una señal máximamente informativa es Hmax = e−1.",
        "Prueba.",
        "La probabilidad y la entropía se derivan de lo anterior.",
        "El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa.",
        "Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ.",
        "Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1.",
        "Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información.",
        "A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4.",
        "EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible.",
        "Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos.",
        "Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
        "El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite.",
        "El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima.",
        "Esto muestra la relación entre la distribución de Poisson y la teoría de la información.",
        "Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente.",
        "Primero, definimos la probabilidad de ruido de Poisson: Definición 4.",
        "La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k!",
        "Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero.",
        "Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso.",
        "Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5.",
        "EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1.",
        "Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección.",
        "Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente.",
        "Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento.",
        "La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento.",
        "La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento.",
        "Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención.",
        "Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos.",
        "Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo.",
        "Esas probabilidades están definidas en el espacio de la colección.",
        "Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento.",
        "Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6.",
        "La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson.",
        "En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones.",
        "Definición 5.",
        "La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5).",
        "Definición 6.",
        "La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1.",
        "Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ.",
        "Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande.",
        "La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ.",
        "Considera la ilustración de las definiciones de ruido e informatividad en la figura 1.",
        "Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad.",
        "El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido.",
        "Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido.",
        "Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia.",
        "El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2.",
        "El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ.",
        "Si podemos sacar más conclusiones de esta configuración es una pregunta abierta.",
        "Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos.",
        "Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos.",
        "Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ.",
        "Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno.",
        "Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional.",
        "La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
        "El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5.",
        "La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos.",
        "Para la informatividad, observamos que el comportamiento radical de Poisson se conserva.",
        "El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5.",
        "El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000.",
        "Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero.",
        "Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo.",
        "El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad.",
        "Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido.",
        "Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación.",
        "Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia.",
        "Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes.",
        "En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia.",
        "Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros.",
        "El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros.",
        "Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes.",
        "La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos.",
        "La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño.",
        "Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos).",
        "Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia.",
        "Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7.",
        "La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística.",
        "Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos.",
        "Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento.",
        "Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento.",
        "Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson.",
        "A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo.",
        "La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente.",
        "Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional.",
        "La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo.",
        "Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente.",
        "Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento.",
        "Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística.",
        "Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de recuperación de información.",
        "Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron.",
        "Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica.",
        "Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres.",
        "REFERENCIAS [1] A. Aizawa.",
        "Una perspectiva de teoría de la información de las medidas tf-idf.",
        "Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen.",
        "Normalización de la frecuencia de términos a través de distribuciones de Pareto.",
        "En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew.",
        "Descubriendo acerca de.",
        "Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson.",
        "Modelos probabilísticos para indexación automática.",
        "Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein.",
        "Manual de matemáticas.",
        "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale.",
        "Mezclas de Poisson.",
        "Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale.",
        "Frecuencia inversa de documentos: Una medida de desviaciones de Poisson.",
        "En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel.",
        "Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica.",
        "Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis.",
        "Modelado de documentos N-Poisson.",
        "En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker.",
        "Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística.",
        "En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994.",
        "Springer-Verlag. [11] S. Wong y Y. Yao.",
        "Una medida de especificidad de términos basada en la teoría de la información.",
        "Revista de la Sociedad Americana de Ciencia de la Información, 43(1):54-61, 1992. [12] S. Wong y Y. Yao.",
        "En modelado de recuperación de información con inferencia probabilística.",
        "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234\n\nTraducción: ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
    ],
    "error_count": 2,
    "keys": {
        "probability function": {
            "translated_key": "función de probabilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based <br>probability function</br> for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based <br>probability function</br> covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a <br>probability function</br> as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a <br>probability function</br> as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the <br>probability function</br> of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial <br>probability function</br> yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson <br>probability function</br> is the limit of the binomial <br>probability function</br>. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness <br>probability function</br> Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "We show that an intuitive idf -based <br>probability function</br> for the probability of a term being informative assumes disjoint document events.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based <br>probability function</br> covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "We refer to a <br>probability function</br> as binary if for all events the probability is either 1.0 or 0.0.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a <br>probability function</br> as constant if for all events the probability is equal.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the <br>probability function</br> of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial <br>probability function</br> yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events."
            ],
            "translated_annotated_samples": [
                "Mostramos que una <br>función de probabilidad</br> basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos.",
                "La <br>función de probabilidad</br> basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta <br>función de probabilidad</br> basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos.",
                "Nos referimos a una <br>función de probabilidad</br> como binaria si para todos los eventos la probabilidad es 1.0 o 0.0.",
                "P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una <br>función de probabilidad</br> como constante si para todos los eventos la probabilidad es igual.",
                "Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la <br>función de probabilidad</br> de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La <br>función de probabilidad</br> binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una <br>función de probabilidad</br> basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La <br>función de probabilidad</br> basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta <br>función de probabilidad</br> basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una <br>función de probabilidad</br> como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una <br>función de probabilidad</br> como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la <br>función de probabilidad</br> de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La <br>función de probabilidad</br> binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "inverse document frequency": {
            "translated_key": "frecuencia inversa del documento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the <br>inverse document frequency</br> (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The <br>inverse document frequency</br> (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the <br>inverse document frequency</br> can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "<br>inverse document frequency</br>: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the <br>inverse document frequency</br> (idf ).",
                "INTRODUCTION AND BACKGROUND The <br>inverse document frequency</br> (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "We showed that the classical definition of the noise (document frequency) in the <br>inverse document frequency</br> can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "<br>inverse document frequency</br>: A measure of deviations from poisson."
            ],
            "translated_annotated_samples": [
                "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la <br>frecuencia inversa del documento</br> (idf).",
                "INTRODUCCIÓN Y ANTECEDENTES La <br>frecuencia inversa de documentos</br> (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados.",
                "Mostramos que la definición clásica del ruido (frecuencia del documento) en la <br>frecuencia inversa del documento</br> puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos.",
                "Frecuencia inversa de documentos: Una medida de desviaciones de Poisson."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la <br>frecuencia inversa del documento</br> (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La <br>frecuencia inversa de documentos</br> (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia. Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7. La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la <br>frecuencia inversa del documento</br> puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson. A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo. Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento. Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de recuperación de información. Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron. Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres. REFERENCIAS [1] A. Aizawa. Una perspectiva de teoría de la información de las medidas tf-idf. Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de la frecuencia de términos a través de distribuciones de Pareto. En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew. Descubriendo acerca de. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para indexación automática. Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein. Manual de matemáticas. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa de documentos: Una medida de desviaciones de Poisson. En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica. Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994. Springer-Verlag. [11] S. Wong y Y. Yao. Una medida de especificidad de términos basada en la teoría de la información. Revista de la Sociedad Americana de Ciencia de la Información, 43(1):54-61, 1992. [12] S. Wong y Y. Yao. En modelado de recuperación de información con inferencia probabilística. ACM Transactions on Information Systems, 13(1):38-68, 1995. 234\n\nTraducción: ACM Transactions on Information Systems, 13(1):38-68, 1995. 234 ",
            "candidates": [],
            "error": [
                [
                    "frecuencia inversa del documento",
                    "frecuencia inversa de documentos",
                    "frecuencia inversa del documento"
                ]
            ]
        },
        "frequency-based probability": {
            "translated_key": "probabilidad basada en frecuencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This <br>frequency-based probability</br> function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear <br>frequency-based probability</br>.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The <br>frequency-based probability</br> of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the <br>frequency-based probability</br> of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the <br>frequency-based probability</br> and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The <br>frequency-based probability</br> can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The <br>frequency-based probability</br> is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This <br>frequency-based probability</br> function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear <br>frequency-based probability</br>.",
                "The <br>frequency-based probability</br> of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the <br>frequency-based probability</br> of being informative (see definition 5).",
                "The parallel definitions of the <br>frequency-based probability</br> and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The <br>frequency-based probability</br> can be explained by binary occurrence, constant containment and disjointness of documents."
            ],
            "translated_annotated_samples": [
                "La función de <br>probabilidad basada en frecuencia</br> Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de <br>probabilidad basada en frecuencia</br> cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos.",
                "La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una <br>probabilidad basada en frecuencia</br> lineal.",
                "La <br>probabilidad basada en la frecuencia</br> de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la <br>probabilidad basada en la frecuencia</br> de ser informativo (ver definición 5).",
                "Las definiciones paralelas de la <br>probabilidad basada en la frecuencia</br> y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes.",
                "La <br>probabilidad basada en frecuencia</br> puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de <br>probabilidad basada en frecuencia</br> Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de <br>probabilidad basada en frecuencia</br> cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una <br>probabilidad basada en frecuencia</br> lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La <br>probabilidad basada en la frecuencia</br> de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la <br>probabilidad basada en la frecuencia</br> de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la <br>probabilidad basada en la frecuencia</br> y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La <br>probabilidad basada en frecuencia</br> puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. ",
            "candidates": [],
            "error": [
                [
                    "probabilidad basada en frecuencia",
                    "probabilidad basada en frecuencia",
                    "probabilidad basada en frecuencia",
                    "probabilidad basada en la frecuencia",
                    "probabilidad basada en la frecuencia",
                    "probabilidad basada en la frecuencia",
                    "probabilidad basada en frecuencia"
                ]
            ]
        },
        "poisson-based probability": {
            "translated_key": "probabilidad basada en Poisson",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a <br>poisson-based probability</br> of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a <br>poisson-based probability</br> of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the <br>poisson-based probability</br> of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The <br>poisson-based probability</br> of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the <br>poisson-based probability</br> of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "By assuming documents to be independent rather than disjoint, we arrive at a <br>poisson-based probability</br> of being informative.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a <br>poisson-based probability</br> of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the <br>poisson-based probability</br> of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "The <br>poisson-based probability</br> of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "The parallel definitions of the frequency-based probability and the <br>poisson-based probability</br> of being informative made the underlying assumptions explicit."
            ],
            "translated_annotated_samples": [
                "Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una <br>probabilidad basada en Poisson</br> de ser informativos.",
                "Hemos definido una probabilidad basada en la frecuencia y una <br>probabilidad basada en Poisson</br> de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso.",
                "La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la <br>probabilidad basada en Poisson</br> de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5).",
                "La <br>probabilidad basada en Poisson</br> de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1.",
                "Las definiciones paralelas de la probabilidad basada en la frecuencia y la <br>probabilidad basada en Poisson</br> de ser informativas hicieron explícitas las suposiciones subyacentes."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una <br>probabilidad basada en Poisson</br> de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una <br>probabilidad basada en Poisson</br> de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la <br>probabilidad basada en Poisson</br> de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La <br>probabilidad basada en Poisson</br> de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la <br>probabilidad basada en Poisson</br> de ser informativas hicieron explícitas las suposiciones subyacentes. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "disjointness of document": {
            "translated_key": "disyunción de eventos de contención de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and <br>disjointness of document</br> containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and <br>disjointness of document</br> containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and <br>disjointness of document</br> containment events.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and <br>disjointness of document</br> containments."
            ],
            "translated_annotated_samples": [
                "Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y <br>disyunción de eventos de contención de documentos</br>.",
                "De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y <br>disyunción de contenciones de documentos</br>."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y <br>disyunción de eventos de contención de documentos</br>. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y <br>disyunción de contenciones de documentos</br>. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia. Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7. La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson. A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo. Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento. Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de recuperación de información. Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron. Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres. REFERENCIAS [1] A. Aizawa. Una perspectiva de teoría de la información de las medidas tf-idf. Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de la frecuencia de términos a través de distribuciones de Pareto. En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew. Descubriendo acerca de. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para indexación automática. Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein. Manual de matemáticas. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa de documentos: Una medida de desviaciones de Poisson. En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica. Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994. Springer-Verlag. [11] S. Wong y Y. Yao. Una medida de especificidad de términos basada en la teoría de la información. Revista de la Sociedad Americana de Ciencia de la Información, 43(1):54-61, 1992. [12] S. Wong y Y. Yao. En modelado de recuperación de información con inferencia probabilística. ACM Transactions on Information Systems, 13(1):38-68, 1995. 234\n\nTraducción: ACM Transactions on Information Systems, 13(1):38-68, 1995. 234 ",
            "candidates": [],
            "error": [
                [
                    "disyunción de eventos de contención de documentos",
                    "disyunción de contenciones de documentos"
                ]
            ]
        },
        "document disjointness": {
            "translated_key": "La disyunción de documentos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "noise probability": {
            "translated_key": "probabilidad de ruido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based <br>noise probability</br> n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the <br>noise probability</br> of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based <br>noise probability</br> (definition 1), the total <br>noise probability</br> for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based <br>noise probability</br> and the total <br>noise probability</br> for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the <br>noise probability</br> P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the <br>noise probability</br> corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term <br>noise probability</br>: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term <br>noise probability</br> for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the <br>noise probability</br> for disjoint documents is equal to the frequency-based <br>noise probability</br>.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the <br>noise probability</br> for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The <br>noise probability</br> can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the <br>noise probability</br> led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term <br>noise probability</br> for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the <br>noise probability</br> depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the <br>noise probability</br>. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent <br>noise probability</br> follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest <br>noise probability</br>.",
                "For a collection with infinite many documents, the upper bound of the <br>noise probability</br> for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the <br>noise probability</br> is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal <br>noise probability</br> is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the <br>noise probability</br> of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term <br>noise probability</br>.",
                "First, we define the Poisson <br>noise probability</br>: Definition 4.",
                "The Poisson term <br>noise probability</br>: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term <br>noise probability</br> is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the <br>noise probability</br> for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the <br>noise probability</br> (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "We show in section 3.1 that the frequency-based <br>noise probability</br> n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the <br>noise probability</br> of a term.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based <br>noise probability</br> (definition 1), the total <br>noise probability</br> for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "Next, we define the frequency-based <br>noise probability</br> and the total <br>noise probability</br> for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the <br>noise probability</br> P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the <br>noise probability</br> corresponds to the occurrence probability of a term in a collection."
            ],
            "translated_annotated_samples": [
                "Mostramos en la sección 3.1 que la <br>probabilidad de ruido</br> basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos.",
                "En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la <br>probabilidad de ruido</br> de un término.",
                "De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la <br>probabilidad de ruido</br> basada en frecuencia (definición 1), la <br>probabilidad de ruido</br> total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos.",
                "A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos.",
                "Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la <br>probabilidad de ruido</br> P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la <br>probabilidad de ruido</br> corresponde a la probabilidad de ocurrencia de un término en una colección."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la <br>probabilidad de ruido</br> basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la <br>probabilidad de ruido</br> de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la <br>probabilidad de ruido</br> basada en frecuencia (definición 1), la <br>probabilidad de ruido</br> total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la <br>probabilidad de ruido</br> P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la <br>probabilidad de ruido</br> corresponde a la probabilidad de ocurrencia de un término en una colección. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which <br>information retrieval</br> models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in <br>information retrieval</br>. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised <br>information retrieval</br> systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling <br>information retrieval</br> with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which <br>information retrieval</br> models are formalised based on probabilistic inference.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in <br>information retrieval</br>. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised <br>information retrieval</br> systems.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 232-241, London, et al., 1994."
            ],
            "translated_annotated_samples": [
                "En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de <br>recuperación de información</br> se formalizan basados en la inferencia probabilística.",
                "Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la <br>recuperación de información</br>. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas.",
                "Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de <br>recuperación de información</br>.",
                "En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker.",
                "En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de <br>recuperación de información</br> se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la <br>recuperación de información</br>. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia. Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7. La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson. A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo. Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento. Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de <br>recuperación de información</br>. Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron. Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres. REFERENCIAS [1] A. Aizawa. Una perspectiva de teoría de la información de las medidas tf-idf. Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de la frecuencia de términos a través de distribuciones de Pareto. En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew. Descubriendo acerca de. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para indexación automática. Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein. Manual de matemáticas. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa de documentos: Una medida de desviaciones de Poisson. En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica. Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "probability theory": {
            "translated_key": "teoría de la probabilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to <br>probability theory</br>.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a <br>probability theory</br> point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to <br>probability theory</br>. 4.",
                "THE LINK TO <br>probability theory</br> We review for independent documents three concepts of <br>probability theory</br>: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "In section 4, we link the results of the previous sections to <br>probability theory</br>.",
                "From a <br>probability theory</br> point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "Next, we link independent documents to <br>probability theory</br>. 4.",
                "THE LINK TO <br>probability theory</br> We review for independent documents three concepts of <br>probability theory</br>: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world."
            ],
            "translated_annotated_samples": [
                "En la sección 4, vinculamos los resultados de las secciones anteriores con la <br>teoría de la probabilidad</br>.",
                "Desde el punto de vista de la <br>teoría de la probabilidad</br>, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico.",
                "A continuación, vinculamos documentos independientes a la <br>teoría de la probabilidad</br>. 4.",
                "EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la <br>teoría de la probabilidad</br> para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la <br>teoría de la probabilidad</br>. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la <br>teoría de la probabilidad</br>, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la <br>teoría de la probabilidad</br>. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la <br>teoría de la probabilidad</br> para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia. Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7. La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson. A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo. Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento. Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de recuperación de información. Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron. Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres. REFERENCIAS [1] A. Aizawa. Una perspectiva de teoría de la información de las medidas tf-idf. Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de la frecuencia de términos a través de distribuciones de Pareto. En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew. Descubriendo acerca de. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para indexación automática. Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein. Manual de matemáticas. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa de documentos: Una medida de desviaciones de Poisson. En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica. Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994. Springer-Verlag. [11] S. Wong y Y. Yao. Una medida de especificidad de términos basada en la teoría de la información. Revista de la Sociedad Americana de Ciencia de la Información, 43(1):54-61, 1992. [12] S. Wong y Y. Yao. En modelado de recuperación de información con inferencia probabilística. ACM Transactions on Information Systems, 13(1):38-68, 1995. 234\n\nTraducción: ACM Transactions on Information Systems, 13(1):38-68, 1995. 234 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "collection space": {
            "translated_key": "espacio de colección",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the <br>collection space</br> and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the <br>collection space</br> as well as to the the document space. 5.",
                "THE <br>collection space</br> AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a <br>collection space</br> D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a <br>collection space</br> corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the <br>collection space</br> and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the <br>collection space</br>.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , <br>collection space</br> and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the <br>collection space</br> and the document space, respectively.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the <br>collection space</br> as well as to the the document space. 5.",
                "THE <br>collection space</br> AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a <br>collection space</br> D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "The document dimension in a <br>collection space</br> corresponds to the location (position) dimension in a document space."
            ],
            "translated_annotated_samples": [
                "Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el <br>espacio de la colección</br> y el espacio del documento, respectivamente.",
                "Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al <br>espacio de colección</br> como al espacio de documentos. 5.",
                "EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1.",
                "Asociamos un <br>espacio de colección</br> D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección.",
                "La dimensión del documento en un <br>espacio de colección</br> corresponde a la dimensión de ubicación en un espacio de documento."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el <br>espacio de la colección</br> y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al <br>espacio de colección</br> como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un <br>espacio de colección</br> D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un <br>espacio de colección</br> corresponde a la dimensión de ubicación en un espacio de documento. ",
            "candidates": [],
            "error": [
                [
                    "espacio de la colección",
                    "espacio de colección",
                    "espacio de colección",
                    "espacio de colección"
                ]
            ]
        },
        "informativeness": {
            "translated_key": "informatividad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (<br>informativeness</br>) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : <br>informativeness</br> of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) <br>informativeness</br> − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between <br>informativeness</br> and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and <br>informativeness</br> apply to their dual counterparts: term occurrence and <br>informativeness</br> in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and <br>informativeness</br> Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson <br>informativeness</br> is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and <br>informativeness</br> definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and <br>informativeness</br> graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based <br>informativeness</br> have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and <br>informativeness</br> Probability function Noise <br>informativeness</br> Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the <br>informativeness</br>, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the <br>informativeness</br> will be only close to one for very little noise, whereas for a bit of noise, <br>informativeness</br> will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the <br>informativeness</br>.",
                "The indepence-based and frequency-based <br>informativeness</br> functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average <br>informativeness</br> by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based <br>informativeness</br>, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (<br>informativeness</br>) of a term, whereas the tf reflects the occurrence of a term.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : <br>informativeness</br> of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) <br>informativeness</br> − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between <br>informativeness</br> and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "However, the results regarding the term noise and <br>informativeness</br> apply to their dual counterparts: term occurrence and <br>informativeness</br> in a document.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and <br>informativeness</br> Poisson by starting the sum from 0, since eλ >> 1."
            ],
            "translated_annotated_samples": [
                "Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (<br>informatividad</br>) de un término, mientras que el tf refleja la ocurrencia de un término.",
                "La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: <br>informatividad</br> de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia.",
                "Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) <br>informatividad</br> − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre <br>informatividad</br> y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención.",
                "Sin embargo, los resultados sobre el término ruido y <br>la informatividad</br> se aplican a sus contrapartes duales: la ocurrencia del término y <br>la informatividad</br> en un documento.",
                "La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la <br>informatividad</br> de Poisson comenzando la suma desde 0, ya que eλ >> 1."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (<br>informatividad</br>) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: <br>informatividad</br> de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) <br>informatividad</br> − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre <br>informatividad</br> y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y <br>la informatividad</br> se aplican a sus contrapartes duales: la ocurrencia del término y <br>la informatividad</br> en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la suposición de independencia conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la <br>informatividad</br> de Poisson comenzando la suma desde 0, ya que eλ >> 1. ",
            "candidates": [],
            "error": [
                [
                    "informatividad",
                    "informatividad",
                    "informatividad",
                    "informatividad",
                    "la informatividad",
                    "la informatividad",
                    "informatividad"
                ]
            ]
        },
        "probabilistic information retrieval": {
            "translated_key": "Recuperación de información probabilística",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "idf": {
            "translated_key": "idf",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (<br>idf</br> ).",
                "We show that an intuitive <br>idf</br> -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (<br>idf</br> ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the <br>idf</br> is defined as follows: <br>idf</br>(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and <br>idf</br> works better than <br>idf</br> alone.",
                "This approach is known as tf-<br>idf</br> , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The <br>idf</br> reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The <br>idf</br> alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-<br>idf</br> approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low <br>idf</br> .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas <br>idf</br> has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of <br>idf</br> is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of <br>idf</br> such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal <br>idf</br> -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := <br>idf</br>(t) maxidf maxidf := max({<br>idf</br>(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised <br>idf</br> , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised <br>idf</br> , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic <br>idf</br> -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both <br>idf</br> and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of <br>idf</br> and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of <br>idf</br> are put into context and a notion of noise is defined, where noise is viewed as the complement of <br>idf</br> .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-<br>idf</br> and information theory and the theoretical research on the meaning of tf-<br>idf</br> clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical <br>idf</br> and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the <br>idf</br> definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical <br>idf</br> .",
                "Theorem 1.",
                "<br>idf</br> assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical <br>idf</br> .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that <br>idf</br> (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, <br>idf</br> works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, <br>idf</br> means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-<br>idf</br> -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the <br>idf</br> with a simple P(t|d), since the combined tf-<br>idf</br> contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-<br>idf</br> -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical <br>idf</br> .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based <br>idf</br> , the gradient is monotonously decreasing and we obtain for different collections the same distances of <br>idf</br> -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value <br>idf</br>(tn+1) of a term tn+1 that occurs in n+1 documents, and the value <br>idf</br>(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the <br>idf</br> into a probabilistic interpretation, and we can use the <br>idf</br> -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of <br>idf</br> does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of <br>idf</br> - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of <br>idf</br> and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of <br>idf</br> , information theory and Poisson described in this paper may lead to variable probabilistic <br>idf</br> and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-<br>idf</br> measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (<br>idf</br> ).",
                "We show that an intuitive <br>idf</br> -based probability function for the probability of a term being informative assumes disjoint document events.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (<br>idf</br> ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the <br>idf</br> is defined as follows: <br>idf</br>(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and <br>idf</br> works better than <br>idf</br> alone."
            ],
            "translated_annotated_samples": [
                "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la <br>frecuencia inversa del documento</br> (idf).",
                "Mostramos que una función de probabilidad basada en <br>idf</br> intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos.",
                "INTRODUCCIÓN Y ANTECEDENTES La <br>frecuencia inversa de documentos</br> (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados.",
                "Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el <br>idf</br> se define de la siguiente manera: <br>idf</br>(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones.",
                "Además, es bien sabido que la combinación de un peso específico del término del documento y el <br>idf</br> funciona mejor que solo el <br>idf</br>."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la <br>frecuencia inversa del documento</br> (idf). Mostramos que una función de probabilidad basada en <br>idf</br> intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La <br>frecuencia inversa de documentos</br> (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el <br>idf</br> se define de la siguiente manera: <br>idf</br>(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el <br>idf</br> funciona mejor que solo el <br>idf</br>. ",
            "candidates": [],
            "error": [
                [
                    "frecuencia inversa del documento",
                    "idf",
                    "frecuencia inversa de documentos",
                    "idf",
                    "idf",
                    "idf",
                    "idf"
                ]
            ]
        },
        "poisson distribution": {
            "translated_key": "distribución de Poisson",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and <br>poisson distribution</br>.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the <br>poisson distribution</br> in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the <br>poisson distribution</br> focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the <br>poisson distribution</br> in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and <br>poisson distribution</br>. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 <br>poisson distribution</br> For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the <br>poisson distribution</br> and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the <br>poisson distribution</br> as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the <br>poisson distribution</br> approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional <br>poisson distribution</br>.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "We show the steps from possible worlds to binomial distribution and <br>poisson distribution</br>.",
                "In this background section, we focus on work that investigates the application of the <br>poisson distribution</br> in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "Our discussion of the <br>poisson distribution</br> focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The Pareto distribution compares to the <br>poisson distribution</br> in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and <br>poisson distribution</br>. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world."
            ],
            "translated_annotated_samples": [
                "Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la <br>distribución de Poisson</br>.",
                "En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la <br>distribución de Poisson</br> en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico.",
                "Nuestra discusión sobre la <br>distribución de Poisson</br> se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas.",
                "La distribución de Pareto se compara con la <br>distribución de Poisson</br> en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson.",
                "EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y <br>distribución de Poisson</br>. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la <br>distribución de Poisson</br>. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la <br>distribución de Poisson</br> en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la <br>distribución de Poisson</br> se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la <br>distribución de Poisson</br> en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y <br>distribución de Poisson</br>. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information theory": {
            "translated_key": "teoría de la información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to <br>information theory</br>.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and <br>information theory</br> (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to <br>information theory</br>. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and <br>information theory</br> and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the independence assumption. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to <br>information theory</br>. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and <br>information theory</br>.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and <br>information theory</br>.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the independence assumption leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an independence assumption, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), <br>information theory</br> and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , <br>information theory</br> and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to <br>information theory</br>.",
                "BACKGROUND The relationship between frequencies, probabilities and <br>information theory</br> (entropy) has been the focus of many researchers.",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to <br>information theory</br>. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and <br>information theory</br> and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "In the next section, we relate the value e−λ to <br>information theory</br>. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition."
            ],
            "translated_annotated_samples": [
                "El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la <br>teoría de la información</br>.",
                "ANTECEDENTES La relación entre las frecuencias, las probabilidades y la <br>teoría de la información</br> (entropía) ha sido el foco de muchos investigadores.",
                "En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la <br>teoría de la información</br>. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística.",
                "Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la <br>teoría de la información</br> y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas.",
                "En la siguiente sección, relacionamos el valor e−λ con la <br>teoría de la información</br>. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la <br>teoría de la información</br>. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la <br>teoría de la información</br> (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la <br>teoría de la información</br>. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la <br>teoría de la información</br> y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la suposición de independencia. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la <br>teoría de la información</br>. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "independence assumption": {
            "translated_key": "suposición de independencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative Thomas Roelleke Department of Computer Science Queen Mary University of London thor@dcs.qmul.ac.uk ABSTRACT This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ).",
                "We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events.",
                "By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative.",
                "The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Theory 1.",
                "INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects.",
                "With N being the total number of documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.",
                "Also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.",
                "This approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. The idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.",
                "The idf alone works better than the tf alone does.",
                "An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.",
                "We use the notion of noisy terms rather than frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as withindocument frequency) of a term in a document.",
                "We associate noise with the document frequency of a term in a collection, and we associate occurrence with the withindocument frequency of a term.",
                "The tf of a noisy term might be high in a document, but noisy terms are not good candidates for representing a document.",
                "Therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .",
                "In a tf-idf approach, the removal of stopwords is conceptually obsolete, if stopwords are just words with a low idf .",
                "From a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.",
                "The missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g. : informativeness of terms, structure of documents, quality of documents, age of documents, etc.) such that a good estimate of the probability of relevance is achieved.",
                "An intuitive solution is a normalisation of idf such that we obtain values in the interval [0; 1].",
                "For example, consider a normalisation based on the maximal idf -value.",
                "Let T be the set of terms occurring in a collection.",
                "Pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t is informative) ≤ 1.0 This frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.",
                "Can we interpret Pfreq , the normalised idf , as the probability that the term is informative?",
                "When investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to disjointness and independence of document events.",
                "These observations are reported in section 3.",
                "We show in section 3.1 that the frequency-based noise probability n(t) N used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.",
                "In section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.",
                "The value e−1 is related to the logarithm and we investigate in section 3.3 the link to information theory.",
                "In section 4, we link the results of the previous sections to probability theory.",
                "We show the steps from possible worlds to binomial distribution and Poisson distribution.",
                "In section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .",
                "Finally, in section 6, we base the definition of the probability of being informative on the results of the previous sections and compare frequency-based and Poisson-based definitions. 2.",
                "BACKGROUND The relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.",
                "In this background section, we focus on work that investigates the application of the Poisson distribution in IR since a main part of the work presented in this paper addresses the underlying assumptions of Poisson. [4] proposes a 2-Poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords). [9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-Poisson model. [10] uses a 2-Poisson model for including term frequency-based probabilities in the probabilistic retrieval model.",
                "The non-linear scaling of the Poisson function showed significant improvement compared to a linear frequency-based probability.",
                "The Poisson model was here applied to the term frequency of a term in a document.",
                "We will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the document space, respectively.",
                "Our discussion of the Poisson distribution focuses on the document frequency in a collection rather than on the term frequency in a document. [7] and [6] address the deviation of idf and Poisson, and apply Poisson mixtures to achieve better Poisson-based estimates.",
                "The results proved again experimentally that a onedimensional Poisson does not work for rare terms, therefore Poisson mixtures and additional parameters are proposed. [3], section 3.3, illustrates and summarises comprehensively the relationships between frequencies, probabilities and Poisson.",
                "Different definitions of idf are put into context and a notion of noise is defined, where noise is viewed as the complement of idf .",
                "We use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events. [11], [12], [8] and [1] link frequencies and probability estimation to information theory. [12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.",
                "A key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.",
                "The probability of being informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12]. [8] address entropy and bibliometric distributions.",
                "Entropy is maximal if all events are equiprobable and the frequency-based Lotka law (N/iλ is the number of scientists that have written i publications, where N and λ are distribution parameters), Zipf and the Pareto distribution are related.",
                "The Pareto distribution is the continuous case of the Lotka and Lotka and Zipf show equivalences.",
                "The Pareto distribution is used by [2] for term frequency normalisation.",
                "The Pareto distribution compares to the Poisson distribution in the sense that Pareto is fat-tailed, i. e. Pareto assigns larger probabilities to large numbers of events than Poisson distributions do.",
                "This makes Pareto interesting since Poisson is felt to be too radical on frequent events.",
                "We restrict in this paper to the discussion of Poisson, however, our results show that indeed a smoother distribution than Poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval. [1] establishes a theoretical link between tf-idf and information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.",
                "This motivation matches the motivation of our paper: We investigate theoretically the assumptions of classical idf and Poisson for a better understanding of parameter estimation and combination. 3.",
                "FROM DISJOINT TO INDEPENDENT We define and discuss in this section three probabilities: The frequency-based noise probability (definition 1), the total noise probability for disjoint documents (definition 2). and the noise probability for independent documents (definition 3). 3.1 Binary occurrence, constant containment and disjointness of documents We show in this section, that the frequency-based noise probability n(t) N in the idf definition can be explained as a total probability with binary term occurrence, constant document containment and disjointness of document containments.",
                "We refer to a probability function as binary if for all events the probability is either 1.0 or 0.0.",
                "The occurrence probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and P(t|d) is equal to 0.0, otherwise.",
                "P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 We refer to a probability function as constant if for all events the probability is equal.",
                "The document containment probability reflect the chance that a document occurs in a collection.",
                "This containment probability is constant if we have no information about the document containment or we ignore that documents differ in containment.",
                "Containment could be derived, for example, from the size, quality, age, links, etc. of a document.",
                "For a constant containment in a collection with N documents, 1 N is often assumed as the containment probability.",
                "We generalise this definition and introduce the constant λ where 0 ≤ λ ≤ N. The containment of a document d depends on the collection c, this is reflected by the notation P(d|c) used for the containment 228 of a document.",
                "P(d|c) is constant : ⇐⇒ ∀d : P(d|c) = λ N For disjoint documents that cover the whole event space, we set λ = 1 and obtain Èd P(d|c) = 1.0.",
                "Next, we define the frequency-based noise probability and the total noise probability for disjoint documents.",
                "We introduce the event notation t is noisy and t occurs for making the difference between the noise probability P(t is noisy|c) in a collection and the occurrence probability P(t occurs|d) in a document more explicit, thereby keeping in mind that the noise probability corresponds to the occurrence probability of a term in a collection.",
                "Definition 1.",
                "The frequency-based term noise probability: Pfreq (t is noisy|c) := n(t) N Definition 2.",
                "The total term noise probability for disjoint documents: Pdis (t is noisy|c) := d P(t occurs|d) · P(d|c) Now, we can formulate a theorem that makes assumptions explicit that explain the classical idf .",
                "Theorem 1.",
                "IDF assumptions: If the occurrence probability P(t|d) of term t over documents d is binary, and the containment probability P(d|c) of documents d is constant, and document containments are disjoint events, then the noise probability for disjoint documents is equal to the frequency-based noise probability.",
                "Pdis (t is noisy|c) = Pfreq (t is noisy|c) Proof.",
                "The assumptions are: ∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 We obtain: Pdis (t is noisy|c) = d|t∈d 1 N = n(t) N = Pfreq (t is noisy|c) The above result is not a surprise but it is a mathematical formulation of assumptions that can be used to explain the classical idf .",
                "The assumptions make explicit that the different types of term occurrence in documents (frequency of a term, importance of a term, position of a term, document part where the term occurs, etc.) and the different types of document containment (size, quality, age, etc.) are ignored, and document containments are considered as disjoint events.",
                "From the assumptions, we can conclude that idf (frequencybased noise, respectively) is a relatively simple but strict estimate.",
                "Still, idf works well.",
                "This could be explained by a leverage effect that justifies the binary occurrence and constant containment: The term occurrence for small documents tends to be larger than for large documents, whereas the containment for small documents tends to be smaller than for large documents.",
                "From that point of view, idf means that P(t ∧ d|c) is constant for all d in which t occurs, and P(t ∧ d|c) is zero otherwise.",
                "The occurrence and containment can be term specific.",
                "For example, set P(t∧d|c) = 1/ND(c) if t occurs in d, where ND(c) is the number of documents in collection c (we used before just N).",
                "We choose a document-dependent occurrence P(t|d) := 1/NT (d), i. e. the occurrence probability is equal to the inverse of NT (d), which is the total number of terms in document d. Next, we choose the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where NT (d)/NT (c) is a document length normalisation (number of terms in document d divided by the number of terms in collection c), and NT (c)/ND(c) is a constant factor of the collection (number of terms in collection c divided by the number of documents in collection c).",
                "We obtain P(t∧d|c) = 1/ND(c).",
                "In a tf-idf -retrieval function, the tf -component reflects the occurrence probability of a term in a document.",
                "This is a further explanation why we can estimate the idf with a simple P(t|d), since the combined tf-idf contains the occurrence probability.",
                "The containment probability corresponds to a document normalisation (document length normalisation, pivoted document length) and is normally attached to the tf -component or the tf-idf -product.",
                "The disjointness assumption is typical for frequency-based probabilities.",
                "From a probability theory point of view, we can consider documents as disjoint events, in order to achieve a sound theoretical model for explaining the classical idf .",
                "But does disjointness reflect the real world where the containment of a document appears to be independent of the containment of another document?",
                "In the next section, we replace the disjointness assumption by the <br>independence assumption</br>. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c) For disjoint documents, this view of the noise probability led to definition 2.",
                "For independent documents, we use now the conjunction of negated events.",
                "Definition 3.",
                "The term noise probability for independent documents: Pin (t is noisy|c) := d (1 − P(t occurs|d) · P(d|c)) With binary occurrence and a constant containment P(d|c) := λ/N, we obtain the term noise of a term t that occurs in n(t) documents: Pin (t is noisy|c) = 1 − 1 − λ N n(t) 229 For binary occurrence and disjoint documents, the containment probability was 1/N.",
                "Now, with independent documents, we can use λ as a collection parameter that controls the average containment probability.",
                "We show through the next theorem that the upper bound of the noise probability depends on λ. Theorem 2.",
                "The upper bound of being noisy: If the occurrence P(t|d) is binary, and the containment P(d|c) is constant, and document containments are independent events, then 1 − e−λ is the upper bound of the noise probability. ∀t : Pin (t is noisy|c) < 1 − e−λ Proof.",
                "The upper bound of the independent noise probability follows from the limit limN→∞(1 + x N )N = ex (see any comprehensive math book, for example, [5], for the convergence equation of the Euler function).",
                "With x = −λ, we obtain: lim N→∞ 1 − λ N N = e−λ For the term noise, we have: Pin (t is noisy|c) = 1 − 1 − λ N n(t) Pin (t is noisy|c) is strictly monotonous: The noise of a term tn is less than the noise of a term tn+1, where tn occurs in n documents and tn+1 occurs in n + 1 documents.",
                "Therefore, a term with n = N has the largest noise probability.",
                "For a collection with infinite many documents, the upper bound of the noise probability for terms tN that occur in all documents becomes: lim N→∞ Pin (tN is noisy) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ By applying an independence rather a disjointness assumption, we obtain the probability e−1 that a term is not noisy even if the term does occur in all documents.",
                "In the disjoint case, the noise probability is one for a term that occurs in all documents.",
                "If we view P(d|c) := λ/N as the average containment, then λ is large for a term that occurs mostly in large documents, and λ is small for a term that occurs mostly in small documents.",
                "Thus, the noise of a term t is large if t occurs in n(t) large documents and the noise is smaller if t occurs in small documents.",
                "Alternatively, we can assume a constant containment and a term-dependent occurrence.",
                "If we assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as the average probability that t represents a document.",
                "The common assumption is that the average containment or occurrence probability is proportional to n(t).",
                "However, here is additional potential: The statistical laws (see [3] on Luhn and Zipf) indicate that the average probability could follow a normal distribution, i. e. small probabilities for small n(t) and large n(t), and larger probabilities for medium n(t).",
                "For the monotonous case we investigate here, the noise of a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and the noise of a term with n(t) = N is close to 1− e−λ .",
                "In the next section, we relate the value e−λ to information theory. 3.3 The probability of a maximal informative signal The probability e−1 is special in the sense that a signal with that probability is a signal with maximal information as derived from the entropy definition.",
                "Consider the definition of the entropy contribution H(t) of a signal t. H(t) := P(t) · − ln P(t) We form the first derivation for computing the optimum. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)) For obtaining optima, we use: 0 = −(1 + ln P(t)) The entropy contribution H(t) is maximal for P(t) = e−1 .",
                "This result does not depend on the base of the logarithm as we see next: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b We summarise this result in the following theorem: Theorem 3.",
                "The probability of a maximal informative signal: The probability Pmax = e−1 ≈ 0.37 is the probability of a maximal informative signal.",
                "The entropy of a maximal informative signal is Hmax = e−1 .",
                "Proof.",
                "The probability and entropy follow from the derivation above.",
                "The complement of the maximal noise probability is e−λ and we are looking now for a generalisation of the entropy definition such that e−λ is the probability of a maximal informative signal.",
                "We can generalise the entropy definition by computing the integral of λ+ ln P(t), i. e. this derivation is zero for e−λ .",
                "We obtain a generalised entropy: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) The generalised entropy corresponds for λ = 1 to the classical entropy.",
                "By moving from disjoint to independent documents, we have established a link between the complement of the noise probability of a term that occurs in all documents and information theory.",
                "Next, we link independent documents to probability theory. 4.",
                "THE LINK TO PROBABILITY THEORY We review for independent documents three concepts of probability theory: possible worlds, binomial distribution and Poisson distribution. 4.1 Possible Worlds Each conjunction of document events (for each document, we consider two document events: the document can be true or false) is associated with a so-called possible world.",
                "For example, consider the eight possible worlds for three documents (N = 3). 230 world w conjunction w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 With each world w, we associate a probability µ(w), which is equal to the product of the single probabilities of the document events. world w probability µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 The sum over the possible worlds in which k documents are true and N −k documents are false is equal to the probability function of the binomial distribution, since the binomial coefficient yields the number of possible worlds in which k documents are true. 4.2 Binomial distribution The binomial probability function yields the probability that k of N events are true where each event is true with the single event probability p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k The single event probability is usually defined as p := λ/N, i. e. p is inversely proportional to N, the total number of events.",
                "With this definition of p, we obtain for an infinite number of documents the following limit for the product of the binomial coefficient and pk : lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k!",
                "The limit is close to the actual value for k << N. For large k, the actual value is smaller than the limit.",
                "The limit of (1−p)N −k follows from the limit limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Again, the limit is close to the actual value for k << N. For large k, the actual value is larger than the limit. 4.3 Poisson distribution For an infinite number of events, the Poisson probability function is the limit of the binomial probability function. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ The probability poisson(0, 1) is equal to e−1 , which is the probability of a maximal informative signal.",
                "This shows the relationship of the Poisson distribution and information theory.",
                "After seeing the convergence of the binomial distribution, we can choose the Poisson distribution as an approximation of the independent term noise probability.",
                "First, we define the Poisson noise probability: Definition 4.",
                "The Poisson term noise probability: Ppoi (t is noisy|c) := e−λ · n(t) k=1 λk k!",
                "For independent documents, the Poisson distribution approximates the probability of the disjunction for large n(t), since the independent term noise probability is equal to the sum over the binomial probabilities where at least one of n(t) document containment events is true.",
                "Pin (t is noisy|c) = n(t) k=1 n(t) k pk (1 − p)N −k Pin (t is noisy|c) ≈ Ppoi (t is noisy|c) We have defined a frequency-based and a Poisson-based probability of being noisy, where the latter is the limit of the independence-based probability of being noisy.",
                "Before we present in the final section the usage of the noise probability for defining the probability of being informative, we emphasise in the next section that the results apply to the collection space as well as to the the document space. 5.",
                "THE COLLECTION SPACE AND THE DOCUMENT SPACE Consider the dual definitions of retrieval parameters in table 1.",
                "We associate a collection space D × T with a collection c where D is the set of documents and T is the set of terms in the collection.",
                "Let ND := |D| and NT := |T| be the number of documents and terms, respectively.",
                "We consider a document as a subset of T and a term as a subset of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that occur in the document d, and let nD(t) := |{d|t ∈ d}| be the number of documents that contain the term t. In a dual way, we associate a document space L × T with a document d where L is the set of locations (also referred to as positions, however, we use the letters L and l and not P and p for avoiding confusion with probabilities) and T is the set of terms in the document.",
                "The document dimension in a collection space corresponds to the location (position) dimension in a document space.",
                "The definition makes explicit that the classical notion of term frequency of a term in a document (also referred to as the within-document term frequency) actually corresponds to the location frequency of a term in a document.",
                "For the 231 space collection document dimensions documents and terms locations and terms document/location frequency nD(t, c): Number of documents in which term t occurs in collection c nL(t, d): Number of locations (positions) at which term t occurs in document d ND(c): Number of documents in collection c NL(d): Number of locations (positions) in document d term frequency nT (d, c): Number of terms that document d contains in collection c nT (l, d): Number of terms that location l contains in document d NT (c): Number of terms in collection c NT (d): Number of terms in document d noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence) containment P(d|c) (document) P(l|d) (location) informativeness − ln P(t|c) − ln P(t|d) conciseness − ln P(d|c) − ln P(l|d) P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Table 1: Retrieval parameters actual term frequency value, it is common to use the maximal occurrence (number of locations; let lf be the location frequency). tf(t, d):=lf(t, d):= Pfreq (t occurs|d) Pfreq (tmax occurs|d) = nL(t, d) nL(tmax , d) A further duality is between informativeness and conciseness (shortness of documents or locations): informativeness is based on occurrence (noise), conciseness is based on containment.",
                "We have highlighted in this section the duality between the collection space and the document space.",
                "We concentrate in this paper on the probability of a term to be noisy and informative.",
                "Those probabilities are defined in the collection space.",
                "However, the results regarding the term noise and informativeness apply to their dual counterparts: term occurrence and informativeness in a document.",
                "Also, the results can be applied to containment of documents and locations. 6.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the <br>independence assumption</br> leads to Poisson probabilities.",
                "In this section, we formulate a frequency-based definition and a Poisson-based definition of the probability of being informative and then we compare the two definitions.",
                "Definition 5.",
                "The frequency-based probability of being informative: Pfreq (t is informative|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N We define the Poisson-based probability of being informative analogously to the frequency-based probability of being informative (see definition 5).",
                "Definition 6.",
                "The Poisson-based probability of being informative: Ppoi (t is informative|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ For the sum expression, the following limit holds: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 For λ >> 1, we can alter the noise and informativeness Poisson by starting the sum from 0, since eλ >> 1.",
                "Then, the minimal Poisson informativeness is poisson(0, λ) = e−λ .",
                "We obtain a simplified Poisson probability of being informative: Ppoi (t is informative|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ The computation of the Poisson sum requires an optimisation for large n(t).",
                "The implementation for this paper exploits the nature of the Poisson density: The Poisson density yields only values significantly greater than zero in an interval around λ.",
                "Consider the illustration of the noise and informativeness definitions in figure 1.",
                "The probability functions displayed are summarised in figure 2 where the simplified Poisson is used in the noise and informativeness graphs.",
                "The frequency-based noise corresponds to the linear solid curve in the noise figure.",
                "With an <br>independence assumption</br>, we obtain the curve in the lower triangle of the noise figure.",
                "By changing the parameter p := λ/N of the independence probability, we can lift or lower the independence curve.",
                "The noise figure shows the lifting for the value λ := ln N ≈ 9.2.",
                "The setting λ = ln N is special in the sense that the frequency-based and the Poisson-based informativeness have the same denominator, namely ln N, and the Poisson sum converges to λ.",
                "Whether we can draw more conclusions from this setting is an open question.",
                "We can conclude, that the lifting is desirable if we know for a collection that terms that occur in relatively few doc232 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeingnoisy n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 0 0.2 0.4 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilityofbeinginformative n(t): Number of documents with term t frequency independence: 1/N independence: ln(N)/N poisson: 1000 poisson: 2000 poisson: 1000,2000 Figure 1: Noise and Informativeness Probability function Noise Informativeness Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N) Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0 Independence Pin Def 1 − (1 − p)n(t) ln(1 − (1 − p)n(t) )/ ln(p) Interval p ≤ Pin < 1 − e−λ ln(p) ≤ Pin ≤ 1.0 Poisson Ppoi Def e−λ Èn(t) k=1 λk k! (λ − ln Èn(t) k=1 λk k! )/(λ − ln λ) Interval e−λ · λ ≤ Ppoi < 1 − e−λ (λ − ln(eλ − 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0 Poisson Ppoi simplified Def e−λ Èn(t) k=0 λk k! (λ − ln Èn(t) k=0 λk k! )/λ Interval e−λ ≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0 Figure 2: Probability functions uments are no guarantee for finding relevant documents, i. e. we assume that rare terms are still relatively noisy.",
                "On the opposite, we could lower the curve when assuming that frequent terms are not too noisy, i. e. they are considered as being still significantly discriminative.",
                "The Poisson probabilities approximate the independence probabilities for large n(t); the approximation is better for larger λ.",
                "For n(t) < λ, the noise is zero whereas for n(t) > λ the noise is one.",
                "This radical behaviour can be smoothened by using a multi-dimensional Poisson distribution.",
                "Figure 1 shows a Poisson noise based on a two-dimensional Poisson: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k!",
                "The two dimensional Poisson shows a plateau between λ1 = 1000 and λ2 = 2000, we used here π = 0.5.",
                "The idea behind this setting is that terms that occur in less than 1000 documents are considered to be not noisy (i.e. they are informative), that terms between 1000 and 2000 are half noisy, and that terms with more than 2000 are definitely noisy.",
                "For the informativeness, we observe that the radical behaviour of Poisson is preserved.",
                "The plateau here is approximately at 1/6, and it is important to realise that this plateau is not obtained with the multi-dimensional Poisson noise using π = 0.5.",
                "The logarithm of the noise is normalised by the logarithm of a very small number, namely 0.5 · e−1000 + 0.5 · e−2000 .",
                "That is why the informativeness will be only close to one for very little noise, whereas for a bit of noise, informativeness will drop to zero.",
                "This effect can be controlled by using small values for π such that the noise in the interval [λ1; λ2] is still very little.",
                "The setting π = e−2000/6 leads to noise values of approximately e−2000/6 in the interval [λ1; λ2], the logarithms lead then to 1/6 for the informativeness.",
                "The indepence-based and frequency-based informativeness functions do not differ as much as the noise functions do.",
                "However, for the indepence-based probability of being informative, we can control the average informativeness by the definition p := λ/N whereas the control on the frequencybased is limited as we address next.",
                "For the frequency-based idf , the gradient is monotonously decreasing and we obtain for different collections the same distances of idf -values, i. e. the parameter N does not affect the distance.",
                "For an illustration, consider the distance between the value idf(tn+1) of a term tn+1 that occurs in n+1 documents, and the value idf(tn) of a term tn that occurs in n documents. idf(tn+1) − idf(tn) = ln n n + 1 The first three values of the distance function are: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 For the Poisson-based informativeness, the gradient decreases first slowly for small n(t), then rapidly near n(t) ≈ λ and then it grows again slowly for large n(t).",
                "In conclusion, we have seen that the Poisson-based definition provides more control and parameter possibilities than 233 the frequency-based definition does.",
                "Whereas more control and parameter promises to be positive for the personalisation of retrieval systems, it bears at the same time the danger of just too many parameters.",
                "The framework presented in this paper raises the awareness about the probabilistic and information-theoretic meanings of the parameters.",
                "The parallel definitions of the frequency-based probability and the Poisson-based probability of being informative made the underlying assumptions explicit.",
                "The frequency-based probability can be explained by binary occurrence, constant containment and disjointness of documents.",
                "Independence of documents leads to Poisson, where we have to be aware that Poisson approximates the probability of a disjunction for a large number of events, but not for a small number.",
                "This theoretical result explains why experimental investigations on Poisson (see [7]) show that a Poisson estimation does work better for frequent (bad, noisy) terms than for rare (good, informative) terms.",
                "In addition to the collection-wide parameter setting, the framework presented here allows for document-dependent settings, as explained for the independence probability.",
                "This is in particular interesting for heterogeneous and structured collections, since documents are different in nature (size, quality, root document, sub document), and therefore, binary occurrence and constant containment are less appropriate than in relatively homogeneous collections. 7.",
                "SUMMARY The definition of the probability of being informative transforms the informative interpretation of the idf into a probabilistic interpretation, and we can use the idf -based probability in probabilistic retrieval approaches.",
                "We showed that the classical definition of the noise (document frequency) in the inverse document frequency can be explained by three assumptions: the term within-document occurrence probability is binary, the document containment probability is constant, and the document containment events are disjoint.",
                "By explicitly and mathematically formulating the assumptions, we showed that the classical definition of idf does not take into account parameters such as the different nature (size, quality, structure, etc.) of documents in a collection, or the different nature of terms (coverage, importance, position, etc.) in a document.",
                "We discussed that the absence of those parameters is compensated by a leverage effect of the within-document term occurrence probability and the document containment probability.",
                "By applying an independence rather a disjointness assumption for the document containment, we could establish a link between the noise probability (term occurrence in a collection), information theory and Poisson.",
                "From the frequency-based and the Poisson-based probabilities of being noisy, we derived the frequency-based and Poisson-based probabilities of being informative.",
                "The frequency-based probability is relatively smooth whereas the Poisson probability is radical in distinguishing between noisy or not noisy, and informative or not informative, respectively.",
                "We showed how to smoothen the radical behaviour of Poisson with a multidimensional Poisson.",
                "The explicit and mathematical formulation of idf - and Poisson-assumptions is the main result of this paper.",
                "Also, the paper emphasises the duality of idf and tf , collection space and document space, respectively.",
                "Thus, the result applies to term occurrence and document containment in a collection, and it applies to term occurrence and position containment in a document.",
                "This theoretical framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.",
                "The links between indepence-based noise as document frequency, probabilistic interpretation of idf , information theory and Poisson described in this paper may lead to variable probabilistic idf and tf definitions and combinations as required in advanced and personalised information retrieval systems.",
                "Acknowledgment: I would like to thank Mounia Lalmas, Gabriella Kazai and Theodora Tsikrika for their comments on the as they said heavy pieces.",
                "My thanks also go to the meta-reviewer who advised me to improve the presentation to make it less formidable and more accessible for those without a theoretic bent.",
                "This work was funded by a research fellowship from Queen Mary University of London. 8.",
                "REFERENCES [1] A. Aizawa.",
                "An information-theoretic perspective of tf-idf measures.",
                "Information Processing and Management, 39:45-65, January 2003. [2] G. Amati and C. J. Rijsbergen.",
                "Term frequency normalization via Pareto distributions.",
                "In 24th BCS-IRSG European Colloquium on IR Research, Glasgow, Scotland, 2002. [3] R. K. Belew.",
                "Finding out about.",
                "Cambridge University Press, 2000. [4] A. Bookstein and D. Swanson.",
                "Probabilistic models for automatic indexing.",
                "Journal of the American Society for Information Science, 25:312-318, 1974. [5] I. N. Bronstein.",
                "Taschenbuch der Mathematik.",
                "Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Natural Language Engineering, 1(2):163-190, 1995. [7] K. W. Church and W. A. Gale.",
                "Inverse document frequency: A measure of deviations from poisson.",
                "In Third Workshop on Very Large Corpora, ACL Anthology, 1995. [8] T. Lafouge and C. Michel.",
                "Links between information construction and information gain: Entropy and bibliometric distribution.",
                "Journal of Information Science, 27(1):39-49, 2001. [9] E. Margulis.",
                "N-poisson document modelling.",
                "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 177-189, 1992. [10] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232-241, London, et al., 1994.",
                "Springer-Verlag. [11] S. Wong and Y. Yao.",
                "An information-theoric measure of term specificity.",
                "Journal of the American Society for Information Science, 43(1):54-61, 1992. [12] S. Wong and Y. Yao.",
                "On modeling information retrieval with probabilistic inference.",
                "ACM Transactions on Information Systems, 13(1):38-68, 1995. 234"
            ],
            "original_annotated_samples": [
                "In the next section, we replace the disjointness assumption by the <br>independence assumption</br>. 3.2 The upper bound of the noise probability for independent documents For independent documents, we compute the probability of a disjunction as usual, namely as the complement of the probability of the conjunction of the negated events: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)) The noise probability can be considered as the conjunction of the term occurrence and the document containment.",
                "THE PROBABILITY OF BEING INFORMATIVE We showed in the previous sections that the disjointness assumption leads to frequency-based probabilities and that the <br>independence assumption</br> leads to Poisson probabilities.",
                "With an <br>independence assumption</br>, we obtain the curve in the lower triangle of the noise figure."
            ],
            "translated_annotated_samples": [
                "En la siguiente sección, reemplazamos la suposición de disyunción por la <br>suposición de independencia</br>. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento.",
                "La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la <br>suposición de independencia</br> conduce a probabilidades de Poisson.",
                "Con una <br>suposición de independencia</br>, obtenemos la curva en el triángulo inferior de la figura de ruido."
            ],
            "translated_text": "Una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativo Thomas Roelleke Departamento de Ciencias de la Computación Universidad Queen Mary de Londres thor@dcs.qmul.ac.uk RESUMEN Este artículo informa sobre investigaciones teóricas acerca de las suposiciones subyacentes a la frecuencia inversa del documento (idf). Mostramos que una función de probabilidad basada en idf intuitiva para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Teoría 1. INTRODUCCIÓN Y ANTECEDENTES La frecuencia inversa de documentos (idf) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de los objetos recuperados. Con N siendo el número total de documentos, y n(t) siendo el número de documentos en los que aparece el término t, el idf se define de la siguiente manera: idf(t) := − log n(t) N , 0 <= idf(t) < ∞ El ranking basado en la suma de los valores de idf de los términos de la consulta que aparecen en los documentos recuperados funciona bien, esto ha sido demostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso específico del término del documento y el idf funciona mejor que solo el idf. Este enfoque se conoce como tf-idf, donde tf(t, d) (0 <= tf(t, d) <= 1) es la llamada frecuencia del término t en el documento d. El idf refleja el poder discriminatorio (informatividad) de un término, mientras que el tf refleja la ocurrencia de un término. El idf por sí solo funciona mejor que lo hace el tf por sí solo. Una explicación podría ser el problema de tf con términos que aparecen en muchos documentos; llamemos a esos términos términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes dejan abierta la posibilidad de si nos referimos a la frecuencia del término en un conjunto de documentos o a la llamada frecuencia del término (también conocida como frecuencia dentro del documento) de un término en un documento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia dentro del documento de un término. El tf de un término ruidoso puede ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocida como eliminación de stopwords) es esencial al aplicar tf. En un enfoque tf-idf, la eliminación de palabras vacías es conceptualmente obsoleta, si las palabras vacías son simplemente palabras con un idf bajo. Desde un punto de vista probabilístico, tf es un valor con una interpretación probabilística basada en la frecuencia, mientras que idf tiene una interpretación informativa en lugar de probabilística. La interpretación probabilística faltante de idf es un problema en modelos de recuperación probabilística donde combinamos conocimiento incierto de diferentes dimensiones (por ejemplo: informatividad de términos, estructura de documentos, calidad de documentos, antigüedad de documentos, etc.) de manera que se logre una buena estimación de la probabilidad de relevancia. Una solución intuitiva es la normalización de idf de tal manera que obtengamos valores en el intervalo [0; 1]. Por ejemplo, considera una normalización basada en el valor máximo de idf. Sea T el conjunto de términos que ocurren en una colección. La función de probabilidad basada en frecuencia Pfreq (t es informativo) := idf(t) maxidf maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N) minidf := min({idf(t)|t ∈ T}), minidf >= 0 minidf maxidf ≤ Pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0; 1] si el idf mínimo es igual a cero, lo cual es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar Pfreq, el idf normalizado, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística del idf normalizado de 227, hicimos varias observaciones relacionadas con la disyunción y la independencia de los eventos de los documentos. Estas observaciones se informan en la sección 3. Mostramos en la sección 3.1 que la probabilidad de ruido basada en frecuencia n(t) N utilizada en la definición clásica de idf puede explicarse mediante tres suposiciones: ocurrencia de términos binarios, contención constante de documentos y disyunción de eventos de contención de documentos. En la sección 3.2 mostramos que al asumir la independencia de los documentos, obtenemos 1 − e−1 ≈ 1 − 0.37 como el límite superior de la probabilidad de ruido de un término. El valor e−1 está relacionado con el logaritmo y en la sección 3.3 investigamos el vínculo con la teoría de la información. En la sección 4, vinculamos los resultados de las secciones anteriores con la teoría de la probabilidad. Mostramos los pasos desde los mundos posibles hasta la distribución binomial y la distribución de Poisson. En la sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para idf como para tf. Finalmente, en la sección 6, basamos la definición de la probabilidad de ser informativo en los resultados de las secciones anteriores y comparamos las definiciones basadas en frecuencia y en Poisson. 2. ANTECEDENTES La relación entre las frecuencias, las probabilidades y la teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección introductoria, nos enfocamos en trabajos que investigan la aplicación de la distribución de Poisson en IR, ya que una parte principal del trabajo presentado en este documento aborda las suposiciones subyacentes de Poisson. [4] propone un modelo de 2-Poisson que tiene en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras funcionales, stopwords). [9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo de n-Poisson de baja dimensión. [10] utiliza un modelo de 2-Poisson para incluir probabilidades basadas en la frecuencia de términos en el modelo de recuperación probabilístico. La escala no lineal de la función de Poisson mostró una mejora significativa en comparación con una probabilidad basada en frecuencia lineal. El modelo de Poisson fue aplicado aquí a la frecuencia de términos en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia del término son parámetros duales en el espacio de la colección y el espacio del documento, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en la frecuencia del término en un documento. [7] y [6] abordan la desviación de idf y Poisson, y aplican mezclas de Poisson para obtener estimaciones basadas en Poisson más precisas. Los resultados demostraron nuevamente experimentalmente que un Poisson unidimensional no funciona para términos raros, por lo tanto se proponen mezclas de Poisson y parámetros adicionales. [3], la sección 3.3, ilustra y resume de manera exhaustiva las relaciones entre frecuencias, probabilidades y Poisson. Diferentes definiciones de idf se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de idf. En nuestro artículo utilizamos una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término basado en la independencia de los eventos del documento. Los enlaces [11], [12], [8] y [1] vinculan las frecuencias y la estimación de la probabilidad con la teoría de la información. [12] establece un marco en el que los modelos de recuperación de información se formalizan basados en la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco principalmente utiliza términos como eventos disjuntos. La probabilidad de ser informativo definida en nuestro artículo puede ser vista como la probabilidad de los términos disjuntos en el espacio de términos de [12]. [8] aborda la entropía y las distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en frecuencias (N/iλ es el número de científicos que han escrito i publicaciones, donde N y λ son parámetros de distribución), Zipf y la distribución de Pareto están relacionados. La distribución de Pareto es el caso continuo de las equivalencias mostradas por Lotka y Lotka y Zipf. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de términos. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto tiene colas gruesas, es decir, Pareto asigna probabilidades más altas a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace interesante a Pareto ya que se considera que Poisson es demasiado radical en eventos frecuentes. Nos limitamos en este artículo a la discusión de Poisson, sin embargo, nuestros resultados muestran que efectivamente una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de probabilidades en la recuperación de información. [1] establece un vínculo teórico entre tf-idf y la teoría de la información y la investigación teórica sobre el significado de tf-idf aclara el modelo estadístico en el que se basan comúnmente las diferentes medidas. Esta motivación coincide con la motivación de nuestro artículo: Investigamos teóricamente las suposiciones del idf clásico y de Poisson para una mejor comprensión de la estimación de parámetros y combinación. 3. De DISJUNTO A INDEPENDIENTE Definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en frecuencia (definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2) y la probabilidad de ruido para documentos independientes (definición 3). 3.1 Ocurrencia binaria, contención constante y disyunción de documentos Mostramos en esta sección que la probabilidad de ruido basada en frecuencia n(t) N en la definición de idf puede explicarse como una probabilidad total con ocurrencia binaria de términos, contención constante de documentos y disyunción de contenciones de documentos. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P(t|d) es binaria, siendo igual a 1.0 si t ∈ d, y siendo igual a 0.0 en caso contrario. P(t|d) es binario: ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento aparezca en una colección. La probabilidad de contención es constante si no tenemos información sobre la contención del documento o si ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con N documentos, a menudo se asume que la probabilidad de contención es de 1/N. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento d depende de la colección c, esto se refleja en la notación P(d|c) utilizada para la contención de un documento. P(d|c) es constante : ⇐⇒ ∀d : P(d|c) = λ N Para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos Èd P(d|c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Introducimos la notación del evento t es ruidoso y t ocurre para hacer más explícita la diferencia entre la probabilidad de ruido P(t es ruidoso|c) en una colección y la probabilidad de ocurrencia P(t ocurre|d) en un documento, teniendo en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido basada en la frecuencia: Pfreq (t es ruidoso|c) := n(t) N Definición 2. La probabilidad total de ruido de término para documentos disjuntos: Pdis (t es ruidoso|c) := d P(t ocurre|d) · P(d|c) Ahora, podemos formular un teorema que haga explícitas las suposiciones que explican el idf clásico. Teorema 1. Supuestos del IDF: Si la probabilidad de ocurrencia P(t|d) del término t en los documentos d es binaria, y la probabilidad de contención P(d|c) de los documentos d es constante, y las contenciones de documentos son eventos disjuntos, entonces la probabilidad de ruido para documentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. Pdis (t es ruidoso|c) = Pfreq (t es ruidoso|c) Prueba. Las suposiciones son: ∀d: (P(t ocurre|d) = 1 ∨ P(t ocurre|d) = 0) ∧ P(d|c) = λ N ∧ d P(d|c) = 1.0 Obtenemos: Pdis (t es ruidoso|c) = d|t∈d 1 N = n(t) N = Pfreq (t es ruidoso|c) El resultado anterior no es una sorpresa, pero es una formulación matemática de suposiciones que se pueden utilizar para explicar el clásico idf. Las suposiciones hacen explícito que se ignoran los diferentes tipos de ocurrencia de términos en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad, antigüedad, etc.), considerando las contenciones de documentos como eventos disjuntos. A partir de las suposiciones, podemos concluir que el idf (ruido basado en la frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Sin embargo, el idf funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y el constante contenido: El término de ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que el contenido para documentos pequeños tiende a ser menor que para documentos grandes. Desde ese punto de vista, idf significa que P(t ∧ d|c) es constante para todos los d en los que t ocurre, y P(t ∧ d|c) es cero de lo contrario. La ocurrencia y contención pueden ser específicas del término. Por ejemplo, establezca P(t∧d|c) = 1/ND(c) si t ocurre en d, donde ND(c) es el número de documentos en la colección c (anteriormente usamos solo N). Elegimos una ocurrencia dependiente del documento P(t|d) := 1/NT (d), es decir, la probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d. A continuación, elegimos la contención P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) donde NT (d)/NT (c) es una normalización de la longitud del documento (número de términos en el documento d dividido por el número de términos en la colección c), y NT (c)/ND(c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P(t∧d|c) = 1/ND(c). En una función de recuperación tf-idf, el componente tf refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional de por qué podemos estimar el idf con un simple P(t|d), ya que el tf-idf combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización de documentos (normalización de longitud de documentos, longitud de documentos pivotada) y normalmente se adjunta al componente tf o al producto tf-idf. La suposición de la disyunción es típica para las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, con el fin de lograr un modelo teórico sólido para explicar el idf clásico. ¿Pero la falta de conexión refleja el mundo real donde la inclusión de un documento parece ser independiente de la inclusión de otro documento? En la siguiente sección, reemplazamos la suposición de disyunción por la <br>suposición de independencia</br>. 3.2 El límite superior de la probabilidad de ruido para documentos independientes. Para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, es decir, como el complemento de la probabilidad de la conjunción de los eventos negados: P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN ) = 1 − d (1 − P(d)). La probabilidad de ruido puede considerarse como la conjunción de la ocurrencia del término y la contención del documento. P(t es ruidoso|c) := P(t ocurre ∧ (d1 ∨ . . . ∨ dN )|c) Para documentos disjuntos, esta visión de la probabilidad de ruido llevó a la definición 2. Para documentos independientes, ahora utilizamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: Pin (t es ruidoso|c) := d (1 − P(t ocurre|d) · P(d|c)) Con ocurrencia binaria y una contención constante P(d|c) := λ/N, obtenemos el término ruido de un término t que ocurre en n(t) documentos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) 229 Para ocurrencia binaria y documentos disjuntos, la probabilidad de contención fue de 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Demostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ. Teorema 2. El límite superior de ser ruidoso: Si la ocurrencia P(t|d) es binaria, y la contención P(d|c) es constante, y las contenciones de documentos son eventos independientes, entonces 1 − e−λ es el límite superior de la probabilidad de ruido. ∀t : Pin (t es ruidoso|c) < 1 − e−λ Prueba. El límite superior de la probabilidad de ruido independiente se deriva del límite limN→∞(1 + x N )N = ex (consulte cualquier libro de matemáticas completo, por ejemplo, [5], para la ecuación de convergencia de la función de Euler). Con x = −λ, obtenemos: lim N→∞ 1 − λ N N = e−λ Para el término ruido, tenemos: Pin (t es ruidoso|c) = 1 − 1 − λ N n(t) Pin (t es ruidoso|c) es estrictamente monótono: El ruido de un término tn es menor que el ruido de un término tn+1, donde tn ocurre en n documentos y tn+1 ocurre en n + 1 documentos. Por lo tanto, un término con n = N tiene la mayor probabilidad de ruido. Para una colección con un número infinito de documentos, el límite superior de la probabilidad de ruido para los términos tN que ocurren en todos los documentos es: lim N→∞ Pin (tN es ruidoso) = lim N→∞ 1 − 1 − λ N N = 1 − e−λ Al aplicar una suposición de independencia en lugar de una de disyunción, obtenemos la probabilidad e−1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es uno para un término que aparece en todos los documentos. Si consideramos P(d|c) := λ/N como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n(t) documentos grandes y el ruido es menor si t ocurre en documentos pequeños. Alternativamente, podemos asumir un confinamiento constante y una ocurrencia dependiente del término. Si asumimos que P(d|c) := 1, entonces P(t|d) := λ/N se puede interpretar como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a n(t). Sin embargo, aquí hay un potencial adicional: Las leyes estadísticas (ver [3] en Luhn y Zipf) indican que la probabilidad promedio podría seguir una distribución normal, es decir, probabilidades pequeñas para n(t) pequeños y grandes, y probabilidades mayores para n(t) medianos. Para el caso monótono que investigamos aquí, el ruido de un término con n(t) = 1 es igual a 1 − (1 − λ/N) = λ/N y el ruido de un término con n(t) = N es cercano a 1− e−λ. En la siguiente sección, relacionamos el valor e−λ con la teoría de la información. 3.3 La probabilidad de una señal máximamente informativa La probabilidad e−1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima según se deriva de la definición de entropía. Considera la definición de la contribución de entropía H(t) de una señal t. H(t) := P(t) · − ln P(t). Formamos la primera derivada para calcular el óptimo. ∂H(t) ∂P(t) = − ln P(t) + −1 P(t) · P(t) = −(1 + ln P(t)). Para obtener el óptimo, usamos: 0 = −(1 + ln P(t)). La contribución de entropía H(t) es máxima para P(t) = e−1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂H(t) ∂P(t) = − logb P(t) + −1 P(t) · ln b · P(t) = − 1 ln b + logb P(t) = − 1 + ln P(t) ln b Resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: La probabilidad Pmax = e−1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal máximamente informativa es Hmax = e−1. Prueba. La probabilidad y la entropía se derivan de lo anterior. El complemento de la probabilidad de ruido máxima es e^−λ y ahora estamos buscando una generalización de la definición de entropía tal que e^−λ sea la probabilidad de una señal máximamente informativa. Podemos generalizar la definición de entropía calculando la integral de λ+ ln P(t), es decir, esta derivación es cero para e−λ. Obtenemos una entropía generalizada: −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)). La entropía generalizada corresponde a la entropía clásica para λ = 1. Al pasar de documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes a la teoría de la probabilidad. 4. EL VÍNCULO CON LA TEORÍA DE LA PROBABILIDAD Revisamos tres conceptos de la teoría de la probabilidad para documentos independientes: mundos posibles, distribución binomial y distribución de Poisson. 4.1 Mundos Posibles Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documento: el documento puede ser verdadero o falso) está asociada con un llamado mundo posible. Por ejemplo, considera los ocho posibles mundos para tres documentos (N = 3). 230 mundo w conjunción w7 d1 ∧ d2 ∧ d3 w6 d1 ∧ d2 ∧ ¬d3 w5 d1 ∧ ¬d2 ∧ d3 w4 d1 ∧ ¬d2 ∧ ¬d3 w3 ¬d1 ∧ d2 ∧ d3 w2 ¬d1 ∧ d2 ∧ ¬d3 w1 ¬d1 ∧ ¬d2 ∧ d3 w0 ¬d1 ∧ ¬d2 ∧ ¬d3 Con cada mundo w, asociamos una probabilidad µ(w), que es igual al producto de las probabilidades individuales de los eventos de los documentos. mundo w probabilidad µ(w) w7  λ N ¡3 ·  1 − λ N ¡0 w6  λ N ¡2 ·  1 − λ N ¡1 w5  λ N ¡2 ·  1 − λ N ¡1 w4  λ N ¡1 ·  1 − λ N ¡2 w3  λ N ¡2 ·  1 − λ N ¡1 w2  λ N ¡1 ·  1 − λ N ¡2 w1  λ N ¡1 ·  1 − λ N ¡2 w0  λ N ¡0 ·  1 − λ N ¡3 La suma de los posibles mundos en los que k documentos son verdaderos y N −k documentos son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial da el número de posibles mundos en los que k documentos son verdaderos. 4.2 Distribución binomial La función de probabilidad binomial da la probabilidad de que k de N eventos sean verdaderos donde cada evento es verdadero con la probabilidad de evento única p. P(k) := binom(N, k, p) := N k pk (1 − p)N −k La probabilidad de evento única suele definirse como p := λ/N, es decir, p es inversamente proporcional a N, el número total de eventos. Con esta definición de p, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y pk: lim N→∞ N k pk = = lim N→∞ N · (N −1) · . . . · (N −k +1) k! λ N k = λk k! El límite está cerca del valor real para k << N. Para valores grandes de k, el valor real es menor que el límite. El límite de (1−p)N −k se sigue del límite limN→∞(1+ x N )N = ex . lim N→∞ (1 − p)N−k = lim N→∞ 1 − λ N N −k = lim N→∞ e−λ · 1 − λ N −k = e−λ Nuevamente, el límite se acerca al valor real para k << N. Para k grande, el valor real es mayor que el límite. 4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial. lim N→∞ binom(N, k, p) = λk k! · e−λ P(k) = poisson(k, λ) := λk k! · e−λ La probabilidad poisson(0, 1) es igual a e−1 , que es la probabilidad de una señal informativa máxima. Esto muestra la relación entre la distribución de Poisson y la teoría de la información. Después de observar la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido del término independiente. Primero, definimos la probabilidad de ruido de Poisson: Definición 4. La probabilidad de ruido del término de Poisson: Ppoi (t es ruidoso|c) := e−λ · n(t) k=1 λk k! Para documentos independientes, la distribución de Poisson aproxima la probabilidad de la disyunción para grandes n(t), ya que la probabilidad de ruido del término independiente es igual a la suma de las probabilidades binomiales donde al menos uno de los eventos de contención de documentos n(t) es verdadero. Hemos definido una probabilidad basada en la frecuencia y una probabilidad basada en Poisson de ser ruidoso, donde esta última es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican tanto al espacio de colección como al espacio de documentos. 5. EL ESPACIO DE COLECCIÓN Y EL ESPACIO DE DOCUMENTOS Considere las definiciones duales de parámetros de recuperación en la tabla 1. Asociamos un espacio de colección D × T con una colección c donde D es el conjunto de documentos y T es el conjunto de términos en la colección. Sea ND := |D| y NT := |T| el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de T y un término como un subconjunto de D. Sea nT(d) := |{t|d ∈ t}| el número de términos que ocurren en el documento d, y sea nD(t) := |{d|t ∈ d}| el número de documentos que contienen el término t. De manera dual, asociamos un espacio de documentos L × T con un documento d donde L es el conjunto de ubicaciones (también conocido como posiciones, sin embargo, usamos las letras L y l y no P y p para evitar confusiones con probabilidades) y T es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación en un espacio de documento. La definición deja claro que la noción clásica de frecuencia de término de un término en un documento (también conocida como frecuencia de término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el documento de colección de espacio 231, dimensiones de documentos y términos, ubicaciones y términos de documentos y frecuencia de documentos/ubicaciones nD(t, c): Número de documentos en los que el término t ocurre en la colección c nL(t, d): Número de ubicaciones (posiciones) en las que el término t ocurre en el documento d ND(c): Número de documentos en la colección c NL(d): Número de ubicaciones (posiciones) en el documento d frecuencia de términos nT (d, c): Número de términos que el documento d contiene en la colección c nT (l, d): Número de términos que la ubicación l contiene en el documento d NT (c): Número de términos en la colección c NT (d): Número de términos en el documento d ruido/ocurrencia P(t|c) (ruido de término) P(t|d) (ocurrencia de término) contención P(d|c) (documento) P(l|d) (ubicación) informatividad − ln P(t|c) − ln P(t|d) concisión − ln P(d|c) − ln P(l|d) P(informativo) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d)) P(conciso) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d)) Tabla 1: Parámetros de recuperación valor de frecuencia de término actual, es común usar la ocurrencia máxima (número de ubicaciones; sea lf la frecuencia de ubicación). tf(t, d):=lf(t, d):= Pfreq (t ocurre|d) Pfreq (tmax ocurre|d) = nL(t, d) nL(tmax , d) Una dualidad adicional es entre informatividad y concisión (brevedad de documentos o ubicaciones): la informatividad se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos resaltado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este artículo en la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades están definidas en el espacio de la colección. Sin embargo, los resultados sobre el término ruido y la informatividad se aplican a sus contrapartes duales: la ocurrencia del término y la informatividad en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones. 6. La probabilidad de ser informativo. Mostramos en las secciones anteriores que la suposición de disyunción conduce a probabilidades basadas en frecuencias y que la <br>suposición de independencia</br> conduce a probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencias y una definición basada en Poisson de la probabilidad de ser informativo y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en la frecuencia de ser informativo: Pfreq (t es informativo|c) := − ln n(t) N − ln 1 N = − logN n(t) N = 1 − logN n(t) = 1 − ln n(t) ln N Definimos la probabilidad basada en Poisson de ser informativo de manera análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativo: Ppoi (t es informativo|c) := − ln e−λ · Èn(t) k=1 λk k! − ln(e−λ · λ) = λ − ln Èn(t) k=1 λk k! λ − ln λ Para la expresión de la suma, se cumple el siguiente límite: lim n(t)→∞ n(t) k=1 λk k! = eλ − 1 Para λ >> 1, podemos alterar el ruido y la informatividad de Poisson comenzando la suma desde 0, ya que eλ >> 1. Entonces, la informatividad mínima de Poisson es poisson(0, λ) = e−λ. Obtenemos una probabilidad de Poisson simplificada de ser informativa: Ppoi (t es informativo|c) ≈ λ − ln Èn(t) k=0 λk k! λ = 1 − ln Èn(t) k=0 λk k! λ. El cálculo de la suma de Poisson requiere una optimización para n(t) grande. La implementación de este artículo explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considera la ilustración de las definiciones de ruido e informatividad en la figura 1. Las funciones de probabilidad mostradas se resumen en la figura 2, donde se utiliza la distribución de Poisson simplificada en los gráficos de ruido e informatividad. El ruido basado en frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una <br>suposición de independencia</br>, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p := λ/N de la probabilidad de independencia, podemos elevar o disminuir la curva de independencia. El factor de ruido muestra el aumento para el valor λ := ln N ≈ 9.2. El valor λ = ln N es especial en el sentido de que la informatividad basada en frecuencias y la informatividad basada en Poisson tienen el mismo denominador, es decir, ln N, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos que para una colección los términos que ocurren en relativamente pocos documentos no son una garantía para encontrar documentos relevantes, es decir, asumimos que los términos raros siguen siendo relativamente ruidosos. Por el contrario, podríamos reducir la curva al asumir que los términos frecuentes no son demasiado ruidosos, es decir, se consideran aún significativamente discriminativos. Las probabilidades de Poisson aproximan las probabilidades de independencia para valores grandes de n(t); la aproximación es mejor para valores mayores de λ. Para n(t) < λ, el ruido es cero, mientras que para n(t) > λ el ruido es uno. Este comportamiento radical puede ser suavizado utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: poisson(k, λ1, λ2) := π · e−λ1 · λk 1 k! + (1 − π) · e−λ2 · λk 2 k! El Poisson bidimensional muestra un plateau entre λ1 = 1000 y λ2 = 2000, aquí usamos π = 0.5. La idea detrás de esta configuración es que los términos que aparecen en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la informatividad, observamos que el comportamiento radical de Poisson se conserva. El plateau aquí está aproximadamente en 1/6, y es importante darse cuenta de que este plateau no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza por el logaritmo de un número muy pequeño, a saber, 0.5 · e−1000 + 0.5 · e−2000. Por eso, la informatividad será cercana a uno solo para muy poco ruido, mientras que para un poco de ruido, la informatividad caerá a cero. Este efecto se puede controlar utilizando valores pequeños para π de manera que el ruido en el intervalo [λ1; λ2] siga siendo muy bajo. El ajuste π = e−2000/6 conduce a valores de ruido de aproximadamente e−2000/6 en el intervalo [λ1; λ2], los logaritmos conducen entonces a 1/6 para la informatividad. Las funciones de informatividad basadas en la independencia y en la frecuencia no difieren tanto como lo hacen las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativa, podemos controlar la informatividad promedio mediante la definición p := λ/N, mientras que el control sobre la basada en la frecuencia es limitado, como abordaremos a continuación. Para el idf basado en frecuencia, el gradiente disminuye monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de idf, es decir, el parámetro N no afecta la distancia. Para una ilustración, considera la distancia entre el valor idf(tn+1) de un término tn+1 que ocurre en n+1 documentos, y el valor idf(tn) de un término tn que ocurre en n documentos. idf(tn+1) − idf(tn) = ln n n + 1 Los primeros tres valores de la función de distancia son: idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69 idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41 idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29 Para la informatividad basada en Poisson, el gradiente disminuye primero lentamente para n(t) pequeños, luego rápidamente cerca de n(t) ≈ λ y luego vuelve a crecer lentamente para n(t) grandes. En conclusión, hemos visto que la definición basada en Poisson proporciona más control y posibilidades de parámetros que la definición basada en frecuencia. Si bien un mayor control y promesas de parámetros parecen ser positivos para la personalización de los sistemas de recuperación, al mismo tiempo conlleva el peligro de tener simplemente demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos e informacionales de los parámetros. Las definiciones paralelas de la probabilidad basada en la frecuencia y la probabilidad basada en Poisson de ser informativas hicieron explícitas las suposiciones subyacentes. La probabilidad basada en frecuencia puede ser explicada por la ocurrencia binaria, la contención constante y la disyunción de documentos. La independencia de los documentos conduce a Poisson, donde debemos tener en cuenta que Poisson aproxima la probabilidad de una disyunción para un gran número de eventos, pero no para un número pequeño. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros en toda la colección, el marco presentado aquí permite ajustes dependientes del documento, como se explica para la probabilidad de independencia. Esto es particularmente interesante para colecciones heterogéneas y estructuradas, ya que los documentos son diferentes en naturaleza (tamaño, calidad, documento raíz, subdocumento), y por lo tanto, la ocurrencia binaria y la contención constante son menos apropiadas que en colecciones relativamente homogéneas. 7. La definición de la probabilidad de ser informativo transforma la interpretación informativa del idf en una interpretación probabilística, y podemos utilizar la probabilidad basada en idf en enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse mediante tres suposiciones: la probabilidad de ocurrencia del término dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente las suposiciones, demostramos que la definición clásica de idf no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de los documentos en una colección, o la naturaleza diferente de los términos (cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una suposición de independencia en lugar de una de disyunción para la contención de documentos, podríamos establecer un vínculo entre la probabilidad de ruido (aparición de términos en una colección), la teoría de la información y la distribución de Poisson. A partir de las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser ruidoso, derivamos las probabilidades basadas en la frecuencia y en la distribución de Poisson de ser informativo. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical al distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las suposiciones de idf y Poisson es el resultado principal de este artículo. Además, el artículo enfatiza la dualidad de idf y tf, espacio de colección y espacio de documento, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia de términos y la contención de documentos en una colección, y se aplica a la ocurrencia de términos y la contención de posiciones en un documento. Este marco teórico es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Los vínculos entre el ruido basado en la independencia como la frecuencia del documento, la interpretación probabilística de idf, la teoría de la información y la distribución de Poisson descritos en este documento pueden llevar a definiciones variables de idf y tf probabilísticos y combinaciones según lo requerido en sistemas avanzados y personalizados de recuperación de información. Agradecimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas, tal como dijeron. Mis agradecimientos también van para el meta-revisor que me aconsejó mejorar la presentación para que sea menos intimidante y más accesible para aquellos sin inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres. REFERENCIAS [1] A. Aizawa. Una perspectiva de teoría de la información de las medidas tf-idf. Procesamiento y Gestión de la Información, 39:45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de la frecuencia de términos a través de distribuciones de Pareto. En el 24º Coloquio Europeo de Investigación en Recuperación de Información BCS-IRSG, Glasgow, Escocia, 2002. [3] R. K. Belew. Descubriendo acerca de. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para indexación automática. Revista de la Sociedad Americana de Ciencia de la Información, 25:312-318, 1974. [5] I. N. Bronstein. Manual de matemáticas. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1(2):163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa de documentos: Una medida de desviaciones de Poisson. En el Tercer Taller sobre Corpora Muy Grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Vínculos entre la construcción de información y la ganancia de información: Entropía y distribución bibliométrica. Revista de Ciencia de la Información, 27(1):39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 232-241, Londres, et al., 1994. Springer-Verlag. [11] S. Wong y Y. Yao. Una medida de especificidad de términos basada en la teoría de la información. Revista de la Sociedad Americana de Ciencia de la Información, 43(1):54-61, 1992. [12] S. Wong y Y. Yao. En modelado de recuperación de información con inferencia probabilística. ACM Transactions on Information Systems, 13(1):38-68, 1995. 234\n\nTraducción: ACM Transactions on Information Systems, 13(1):38-68, 1995. 234 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}