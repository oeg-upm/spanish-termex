{
    "id": "I-77",
    "original_text": "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC). We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness). The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions. The balance is the difference between these matrices. A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future. The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1. INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters. The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14]. We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical. Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results. In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them. These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels). These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows. We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process. Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level. It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension. In this sense, humans aim at a general sense of fairness in an interaction. In Section 2 we outline the aspects of human negotiation modelling that we cover in this work. Then, in Section 3 we introduce the negotiation language. Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation. Section 5 contains a description of the different metrics used in the agent model including intimacy. Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2. HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy. What information is relevant to the negotiation process? What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options. What are the possible agreements we can accept? • Goals. What are the underlying things we need or care about? What are our goals? • Independence. What will we do if the negotiation fails? What alternatives have we got? • Commitment. What outstanding commitments do we have? Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . . A key part of any negotiation process is to build a model of our opponent(s) along these dimensions. All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments. Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them. For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain. These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships. The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable). However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need. Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby). For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension. The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level). In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need. We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved. According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used. This might be part of our social evolution. There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce. In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity). In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods. See Table 1 for some examples of desired balances along the LOGIC dimensions. The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators. For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close. Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3. COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1. C is a finite set of concept symbols (including basic data types); 2. R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found. This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure. Usually, food is accumulated at the shelter for future use. Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required. We will omit this here. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy. To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on). R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues. The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation. Semantic distance plays a fundamental role in strategies for information-based agency. How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships. A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts). Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions. First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated. Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts. A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively. Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed. Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour. Norms, contracts, and information have an obvious temporal dimension. Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time. The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place. In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions. C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a. Representing an ontology as a set predicates in Prolog is simple. The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y. Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4. AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2]. The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment. In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation. The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents. Agents have a probabilistic first-order internal language L used to represent a world model, Mt . A generic information-based architecture is described in detail in [15]. The LOGIC agent architecture is shown in Figure 1. Agent α acts in response to a need that is expressed in terms of the ontology. A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires. Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 . The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here. For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term. Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt . Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer. We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2. Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2. The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6. We now describe two of the distributions in Mt that support offer exchange. Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability. The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15]. Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space. The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space. We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space. Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6. The parameters g and h are independent. We can imagine a relationship that begins with g = 1 and h = 0. Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1. The basis for the acceptance criterion has thus developed from equity to equality, and then to need. Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses. For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}. In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible. This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed. All observations about the world are received as utterances from an all-truthful institution agent ξ. For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger. So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received. We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility. For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions. In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data. In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi). For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution. Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi). Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution. We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations. We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows. Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj . This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11]. Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution. The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation. The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease. We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context. A relationship, Ψ∗t , is a sequence of dialogues. We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation). Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures. Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories. In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14]. Ideal observations. Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe. This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e). Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ). That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have. This equation measures confidence for a single statement ϕ. It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ. Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations. The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ. Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ. Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation. Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement. If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before. Computational Note. The various measures given above involve extensive calculations. For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ . We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ). The extent of this calculation is controlled by the parameter η. An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β. This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}. Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own. In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated. The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define. This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach. For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain. Attaching a utilitarian measure to this utterance may not be so simple. We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements. Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ). The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ . If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction. Dx is the reputation of agent x. The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β . In particular, the intimacy determines values for the parameters g and h in Equation 1. As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases. The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues. In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6. STRATEGIES AND TACTICS Each negotiation has to achieve two goals. First it may be intended to achieve some contractual outcome. Second it will aim to contribute to the growth, or decline, of the relationship intimacy. We now describe in greater detail the contents of the Negotiation box in Figure 1. The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships. The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy. The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases. The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions. The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy. The negotiation strategy is concerned with maintaining a working set of Options. If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position. In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options. This is achieved by increasing the value of s in Equation 1. The following strategy uses the machinery described in Section 4. Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances. Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation. Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ . Suppose that the relationship target is (T∗t α , T∗t β ). Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g. Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.). It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6. Nα(Ψt ) is what α hopes to receive from β during Ψt . This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions. A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance. A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue. If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7. DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies. It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic. Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment. Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures. The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next. We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).) Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8. REFERENCES [1] Adams, J. S. Inequity in social exchange. In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2. New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J. A., and Sierra, C. Environment engineering for multiagent systems. Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B. Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives. Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation. Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering. American Institute of Physics, Melville, NY, USA, 2004, ch. On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J. Bargaining with information. In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B. Getting to Yes: Negotiating agreements without giving in. Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory. In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources. IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003. [12] Paris, J. Common sense and maximum entropy. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J. An information-based model for trust. In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J. Trust and honour in information-based agency. In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency. In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship. Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A. Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations. In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037",
    "original_translation": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037",
    "original_sentences": [
        "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
        "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
        "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
        "The balance is the difference between these matrices.",
        "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
        "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
        "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
        "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
        "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
        "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
        "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
        "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
        "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
        "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
        "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
        "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
        "In this sense, humans aim at a general sense of fairness in an interaction.",
        "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
        "Then, in Section 3 we introduce the negotiation language.",
        "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
        "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
        "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
        "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
        "What information is relevant to the negotiation process?",
        "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
        "What are the possible agreements we can accept? • Goals.",
        "What are the underlying things we need or care about?",
        "What are our goals? • Independence.",
        "What will we do if the negotiation fails?",
        "What alternatives have we got? • Commitment.",
        "What outstanding commitments do we have?",
        "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
        "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
        "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
        "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
        "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
        "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
        "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
        "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
        "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
        "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
        "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
        "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
        "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
        "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
        "This might be part of our social evolution.",
        "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
        "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
        "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
        "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
        "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
        "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
        "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
        "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
        "C is a finite set of concept symbols (including basic data types); 2.",
        "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
        "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
        "Usually, food is accumulated at the shelter for future use.",
        "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
        "We will omit this here.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
        "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
        "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
        "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
        "Semantic distance plays a fundamental role in strategies for information-based agency.",
        "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
        "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
        "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
        "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
        "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
        "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
        "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
        "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
        "Norms, contracts, and information have an obvious temporal dimension.",
        "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
        "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
        "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
        "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
        "Representing an ontology as a set predicates in Prolog is simple.",
        "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
        "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
        "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
        "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
        "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
        "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
        "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
        "A generic information-based architecture is described in detail in [15].",
        "The LOGIC agent architecture is shown in Figure 1.",
        "Agent α acts in response to a need that is expressed in terms of the ontology.",
        "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
        "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
        "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
        "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
        "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
        "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
        "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
        "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
        "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
        "We now describe two of the distributions in Mt that support offer exchange.",
        "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
        "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
        "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
        "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
        "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
        "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
        "The parameters g and h are independent.",
        "We can imagine a relationship that begins with g = 1 and h = 0.",
        "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
        "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
        "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
        "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
        "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
        "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
        "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
        "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
        "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
        "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
        "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
        "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
        "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
        "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
        "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
        "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
        "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
        "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
        "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
        "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
        "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
        "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
        "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
        "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
        "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
        "A relationship, Ψ∗t , is a sequence of dialogues.",
        "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
        "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
        "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
        "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
        "Ideal observations.",
        "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
        "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
        "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
        "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
        "This equation measures confidence for a single statement ϕ.",
        "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
        "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
        "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
        "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
        "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
        "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
        "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
        "Computational Note.",
        "The various measures given above involve extensive calculations.",
        "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
        "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
        "The extent of this calculation is controlled by the parameter η.",
        "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
        "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
        "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
        "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
        "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
        "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
        "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
        "Attaching a utilitarian measure to this utterance may not be so simple.",
        "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
        "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
        "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
        "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
        "Dx is the reputation of agent x.",
        "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
        "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
        "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
        "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
        "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
        "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
        "First it may be intended to achieve some contractual outcome.",
        "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
        "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
        "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
        "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
        "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
        "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
        "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
        "The negotiation strategy is concerned with maintaining a working set of Options.",
        "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
        "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
        "This is achieved by increasing the value of s in Equation 1.",
        "The following strategy uses the machinery described in Section 4.",
        "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
        "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
        "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
        "Suppose that the relationship target is (T∗t α , T∗t β ).",
        "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
        "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
        "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
        "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
        "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
        "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
        "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
        "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
        "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
        "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
        "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
        "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
        "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
        "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
        "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
        "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
        "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
        "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
        "A., and Sierra, C. Environment engineering for multiagent systems.",
        "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
        "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
        "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
        "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
        "American Institute of Physics, Melville, NY, USA, 2004, ch.",
        "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
        "Bargaining with information.",
        "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
        "Getting to Yes: Negotiating agreements without giving in.",
        "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
        "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
        "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
        "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
        "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
        "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
        "Cambridge University Press, 2003. [12] Paris, J.",
        "Common sense and maximum entropy.",
        "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
        "An information-based model for trust.",
        "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
        "Trust and honour in information-based agency.",
        "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
        "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
        "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
        "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
        "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
        "JAI Press, 1995, pp. 65-94.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
    ],
    "translated_text_sentences": [
        "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC).",
        "Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad).",
        "La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA.",
        "El equilibrio es la diferencia entre estas matrices.",
        "Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro.",
        "La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1.",
        "INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos.",
        "El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14].",
        "Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego.",
        "Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información.",
        "En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable.",
        "Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad).",
        "Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta.",
        "No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación.",
        "Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado.",
        "Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión.",
        "En este sentido, los humanos buscan un sentido general de equidad en una interacción.",
        "En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo.",
        "Luego, en la Sección 3 introducimos el lenguaje de negociación.",
        "La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación.",
        "La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad.",
        "Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2.",
        "NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad.",
        "¿Qué información es relevante para el proceso de negociación?",
        "¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones.",
        "¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas.",
        "¿Cuáles son las cosas subyacentes que necesitamos o nos importan?",
        "¿Cuáles son nuestros objetivos? • Independencia.",
        "¿Qué haremos si la negociación falla?",
        "¿Qué alternativas tenemos? • Compromiso.",
        "¿Qué compromisos pendientes tenemos?",
        "Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares...",
        "Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones.",
        "Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos.",
        "Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial.",
        "Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada.",
        "Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación.",
        "La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas).",
        "Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad.",
        "La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé).",
        "Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos.",
        "La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad).",
        "En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad.",
        "Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas.",
        "Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad.",
        "Esto podría ser parte de nuestra evolución social.",
        "Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba.",
        "En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad).",
        "En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes.",
        "Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA.",
        "El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores.",
        "Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana.",
        "Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3.",
        "MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1.",
        "C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2.",
        "En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran.",
        "Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos.",
        "Por lo general, la comida se acumula en el refugio para uso futuro.",
        "Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones.",
        "Omitiremos esto aquí.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un.",
        "Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente).",
        "R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas.",
        "La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤.",
        "La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información.",
        "Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales.",
        "Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos).",
        "La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales.",
        "Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas.",
        "En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos.",
        "Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente.",
        "Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida.",
        "Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente.",
        "Normas, contratos e información tienen una dimensión temporal obvia.",
        "Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo.",
        "El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar.",
        "De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones.",
        "C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a.",
        "Representar una ontología como un conjunto de predicados en Prolog es sencillo.",
        "El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y.",
        "Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4.",
        "La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2].",
        "El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso.",
        "En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior.",
        "El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes.",
        "Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt.",
        "Una arquitectura genérica basada en la información se describe en detalle en [15].",
        "La arquitectura del agente LOGIC se muestra en la Figura 1.",
        "El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología.",
        "Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita.",
        "Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación.",
        "La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí.",
        "Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo.",
        "Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt.",
        "A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente.",
        "No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading.",
        "Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2.",
        "La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6.",
        "Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas.",
        "Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad.",
        "La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15].",
        "Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico.",
        "La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito.",
        "También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración.",
        "Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6.",
        "Los parámetros g y h son independientes.",
        "Podemos imaginar una relación que comienza con g = 1 y h = 0.",
        "Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1.",
        "La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad.",
        "Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β.",
        "Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}.",
        "En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible.",
        "Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983.",
        "La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente.",
        "Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ.",
        "Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre.",
        "Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas.",
        "Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional.",
        "Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β.",
        "En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos.",
        "En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi).",
        "Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución.",
        "Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi).",
        "Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α.",
        "Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones.",
        "Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera.",
        "Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj.",
        "Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11].",
        "Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad.",
        "El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación.",
        "El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir.",
        "Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
        "Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto.",
        "Una relación, Ψ∗t, es una secuencia de diálogos.",
        "Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación).",
        "Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza.",
        "Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA.",
        "En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14].",
        "Observaciones ideales.",
        "Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar.",
        "Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e).",
        "Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ).",
        "Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener.",
        "Esta ecuación mide la confianza para una única declaración ϕ.",
        "Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ.",
        "De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas.",
        "La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ.",
        "Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ.",
        "Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación.",
        "Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original.",
        "Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes.",
        "Nota computacional.",
        "Las diversas medidas mencionadas anteriormente implican cálculos extensos.",
        "Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ.",
        "Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ).",
        "La magnitud de este cálculo está controlada por el parámetro η.",
        "Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β.",
        "Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}.",
        "Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio.",
        "En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular.",
        "Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir.",
        "Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información.",
        "Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información.",
        "Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple.",
        "Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes.",
        "La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl.",
        "En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt).",
        "La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´.",
        "Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción.",
        "Dx es la reputación del agente x.",
        "El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β.",
        "En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1.",
        "Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta.",
        "La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados.",
        "En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6.",
        "ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos.",
        "Primero puede tener la intención de lograr algún resultado contractual.",
        "En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación.",
        "Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1.",
        "La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas.",
        "La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura.",
        "La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad.",
        "La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α.",
        "El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad.",
        "La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo.",
        "Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O).",
        "De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales.",
        "Esto se logra aumentando el valor de s en la Ecuación 1.",
        "La estrategia siguiente utiliza la maquinaria descrita en la Sección 4.",
        "Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos.",
        "Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación.",
        "Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´.",
        "Supongamos que el objetivo de la relación es (T∗t α , T∗t β ).",
        "Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo,",
        "Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc.",
        "Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6.",
        "Nα(Ψt) es lo que α espera recibir de β durante Ψt.",
        "Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA.",
        "Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura.",
        "Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado.",
        "Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7.",
        "DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología.",
        "Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación.",
        "La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso.",
        "Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad.",
        "Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación.",
        "Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).)",
        "Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8.",
        "REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social.",
        "En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2.",
        "Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J.",
        "Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C.",
        "Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B.",
        "Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas.",
        "Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación.",
        "Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería.",
        "Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap.",
        "Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J.",
        "Negociando con información.",
        "En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B.",
        "Llegar a Sí: Negociar acuerdos sin ceder.",
        "Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información.",
        "En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science.",
        "Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación.",
        "McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
        "A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información.",
        "IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje.",
        "Cambridge University Press, 2003. [12] París, J.",
        "Sentido común y entropía máxima.",
        "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J.",
        "Un modelo basado en la información para la confianza.",
        "En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J.",
        "Confianza y honor en una agencia basada en la información.",
        "En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información.",
        "En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación.",
        "Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A.",
        "Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones.",
        "En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5.",
        "JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037"
    ],
    "error_count": 2,
    "keys": {
        "successive negotiation encounter": {
            "translated_key": "encuentros de negociación sucesivos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on <br>successive negotiation encounter</br>s.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on <br>successive negotiation encounter</br>s."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en <br>encuentros de negociación sucesivos</br>."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en <br>encuentros de negociación sucesivos</br>. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "long term relationship": {
            "translated_key": "relaciones a largo plazo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with <br>long term relationship</br>s that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "INTRODUCTION In this paper we propose a new negotiation model to deal with <br>long term relationship</br>s that are founded on successive negotiation encounters."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar <br>relaciones a largo plazo</br> que se basan en encuentros de negociación sucesivos."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar <br>relaciones a largo plazo</br> que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "utterance": {
            "translated_key": "expresión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an <br>utterance</br> and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing <br>utterance</br> μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an <br>utterance</br> accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between <br>utterance</br>, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an <br>utterance</br> μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the <br>utterance</br> μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each <br>utterance</br>, the difference between what is said (the <br>utterance</br>) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the <br>utterance</br> Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this <br>utterance</br> may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each <br>utterance</br> μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "In Section 4.1 this enables us to measure the difference between an <br>utterance</br> and a subsequent observation.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing <br>utterance</br> μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an <br>utterance</br> accurately describes what will subsequently be observed.",
                "We represent the relationship between <br>utterance</br>, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "Suppose that α receives an <br>utterance</br> μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution."
            ],
            "translated_annotated_samples": [
                "En la Sección 4.1 esto nos permite medir la diferencia entre una <br>expresión</br> y una observación posterior.",
                "Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el <br>enunciado</br> μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt.",
                "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una <br>enunciación</br> describe con precisión lo que se observará posteriormente.",
                "Representamos la relación entre la <br>emisión</br>, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional.",
                "Supongamos que α recibe una <br>expresión</br> μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una <br>expresión</br> y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el <br>enunciado</br> μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una <br>enunciación</br> describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la <br>emisión</br>, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una <br>expresión</br> μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. ",
            "candidates": [],
            "error": [
                [
                    "expresión",
                    "enunciado",
                    "enunciación",
                    "emisión",
                    "expresión"
                ]
            ]
        },
        "utilitarian interpretation": {
            "translated_key": "interpretación utilitaria",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a <br>utilitarian interpretation</br> in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "Also, several utterances can have a <br>utilitarian interpretation</br> in the sense that an agent can associate a preferential gain to them."
            ],
            "translated_annotated_samples": [
                "Además, varias expresiones pueden tener una <br>interpretación utilitaria</br> en el sentido de que un agente puede asociarles una ganancia preferencial."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una <br>interpretación utilitaria</br> en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ontology": {
            "translated_key": "ontología",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 <br>ontology</br> In order to define a language to structure agent dialogues we need an <br>ontology</br> that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an <br>ontology</br> depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an <br>ontology</br> as a set predicates in Prolog is simple.",
                "The set term contains instances of the <br>ontology</br> concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the <br>ontology</br>.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the <br>ontology</br> described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the <br>ontology</br> at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the <br>ontology</br> at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An <br>ontology</br>-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "COMMUNICATION MODEL 3.1 <br>ontology</br> In order to define a language to structure agent dialogues we need an <br>ontology</br> that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "The semantic distance between concepts within an <br>ontology</br> depends on how far away they are in the structure defined by the ≤ relation.",
                "Representing an <br>ontology</br> as a set predicates in Prolog is simple.",
                "The set term contains instances of the <br>ontology</br> concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Agent α acts in response to a need that is expressed in terms of the <br>ontology</br>."
            ],
            "translated_annotated_samples": [
                "MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una <br>ontología</br> que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos <br>ontología</br>s siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1.",
                "La distancia semántica entre conceptos dentro de una <br>ontología</br> depende de qué tan lejos estén en la estructura definida por la relación ≤.",
                "Representar una <br>ontología</br> como un conjunto de predicados en Prolog es sencillo.",
                "El término establecido contiene instancias de los conceptos y relaciones de la <br>ontología</br>. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y.",
                "El agente α actúa en respuesta a una necesidad que se expresa en términos de la <br>ontología</br>."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una <br>ontología</br> que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos <br>ontología</br>s siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una <br>ontología</br> depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una <br>ontología</br> como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la <br>ontología</br>. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la <br>ontología</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "set predicate": {
            "translated_key": "conjunto de predicados",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a <br>set predicate</br>s in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "Representing an ontology as a <br>set predicate</br>s in Prolog is simple."
            ],
            "translated_annotated_samples": [
                "Representar una ontología como un <br>conjunto de predicados</br> en Prolog es sencillo."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un <br>conjunto de predicados</br> en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multiagent system": {
            "translated_key": "sistema multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A <br>multiagent system</br> {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "AGENT ARCHITECTURE A <br>multiagent system</br> {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2]."
            ],
            "translated_annotated_samples": [
                "La arquitectura de agentes de un <br>sistema multiagente</br> {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un <br>sistema multiagente</br> {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "logic agent architecture": {
            "translated_key": "arquitectura del agente lógico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The <br>logic agent architecture</br> 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The <br>logic agent architecture</br> is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The <br>logic agent architecture</br> 4.",
                "The <br>logic agent architecture</br> is shown in Figure 1."
            ],
            "translated_annotated_samples": [
                "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4.",
                "La arquitectura del agente LOGIC se muestra en la Figura 1."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "negotiation strategy": {
            "translated_key": "estrategia de negociación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The <br>negotiation strategy</br> maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The <br>negotiation strategy</br> then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The <br>negotiation strategy</br> is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a <br>negotiation strategy</br> and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "The <br>negotiation strategy</br> maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "The <br>negotiation strategy</br> then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "The <br>negotiation strategy</br> is concerned with maintaining a working set of Options.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a <br>negotiation strategy</br> and tactic."
            ],
            "translated_annotated_samples": [
                "La <br>estrategia de negociación</br> mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada.",
                "La <br>estrategia de negociación</br> determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6.",
                "La <br>estrategia de negociación</br> se preocupa por mantener un conjunto de opciones de trabajo.",
                "Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una <br>estrategia y táctica de negociación</br>."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La <br>estrategia de negociación</br> mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La <br>estrategia de negociación</br> determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La <br>estrategia de negociación</br> se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una <br>estrategia y táctica de negociación</br>. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    "estrategia de negociación",
                    "estrategia de negociación",
                    "estrategia de negociación",
                    "estrategia y táctica de negociación"
                ]
            ]
        },
        "view of acceptability": {
            "translated_key": "vista de aceptabilidad",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "acceptability view": {
            "translated_key": "vista de aceptabilidad",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "acceptance criterion": {
            "translated_key": "criterio de aceptación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the <br>acceptance criterion</br> has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "The basis for the <br>acceptance criterion</br> has thus developed from equity to equality, and then to need."
            ],
            "translated_annotated_samples": [
                "La base para el <br>criterio de aceptación</br> ha evolucionado de la equidad a la igualdad, y luego a la necesidad."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el <br>criterio de aceptación</br> ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "component dialogue": {
            "translated_key": "diálogos componentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its <br>component dialogue</br>s. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "Finally we define the intimacy of a relationship as an aggregation of the value of its <br>component dialogue</br>s. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories."
            ],
            "translated_annotated_samples": [
                "Finalmente definimos la intimidad de una relación como una agregación del valor de sus <br>diálogos componentes</br>. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las medidas de confianza. Finalmente definimos la intimidad de una relación como una agregación del valor de sus <br>diálogos componentes</br>. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican medidas de confianza para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "confidence measure": {
            "translated_key": "medidas de confianza",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC Negotiation Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that negotiation is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under negotiation, but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process.",
                "Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human negotiation modelling that we cover in this work.",
                "Then, in Section 3 we introduce the negotiation language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the negotiation.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN NEGOTIATION Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the negotiation process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the negotiation fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "Negotiation dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any negotiation process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a negotiation give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their negotiation relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in negotiation or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the negotiation by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each negotiation consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human negotiation, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The negotiation strategy then determines the current set of Options {δi}, and then the tactics, guided by the negotiation target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent negotiation terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the <br>confidence measure</br>s.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 <br>confidence measure</br>s are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing negotiation dialogues Suppose that a negotiation commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This negotiation dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a negotiation dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each negotiation round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each negotiation has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the Negotiation box in Figure 1.",
                "The negotiation literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the negotiation stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The negotiation strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the negotiation. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the negotiation α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a negotiation target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a negotiation balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to negotiation that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a negotiation strategy and tactic.",
                "Negotiation is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of Negotiation.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in Negotiation in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the <br>confidence measure</br>s.",
                "In Section 5.2 <br>confidence measure</br>s are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14]."
            ],
            "translated_annotated_samples": [
                "Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las <br>medidas de confianza</br>.",
                "En la Sección 5.2 se aplican <br>medidas de confianza</br> para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de negociación basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de negociación mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la negociación hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de negociación para tratar relaciones a largo plazo que se basan en encuentros de negociación sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la negociación es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. Creemos que si los agentes quieren tener éxito en dominios de aplicación reales, deben conciliar ambas perspectivas: la informativa y la teórica del juego. Nuestro objetivo es modelar escenarios de negociación donde los agentes representen a sus principales humanos, y por lo tanto queremos que su comportamiento sea comprensible para los humanos y respete los procedimientos habituales de negociación humana, al mismo tiempo que sea consistente con, y de alguna manera extienda, los resultados teóricos de juegos e información. En este sentido, los agentes no solo buscan maximizar la utilidad, sino que también buscan construir relaciones duraderas con niveles crecientes de intimidad que determinan qué equilibrio en el intercambio de información y recursos les resulta aceptable. Estos dos conceptos, intimidad y equilibrio, son clave en el modelo, y nos permiten entender la teoría de juegos competitiva y cooperativa como dos teorías particulares de las relaciones entre agentes (es decir, en diferentes niveles de intimidad). Estas dos teorías son demasiado específicas y distintas para describir cómo una relación (comercial) podría crecer, ya que las interacciones tienen algunos aspectos de estos dos extremos en un continuo en el que, por ejemplo, los agentes revelan cantidades crecientes de información privada a medida que su intimidad aumenta. No seguimos el enfoque de Co-Opetition [4] donde la cooperación y la competencia dependen del tema en negociación, sino que creemos que la disposición a cooperar/competir afecta todos los aspectos en el proceso de negociación. Las estrategias de negociación pueden ser vistas naturalmente como procedimientos que seleccionan tácticas utilizadas para lograr un acuerdo exitoso y alcanzar un nivel de intimidad deseado. Es común en entornos humanos utilizar tácticas que compensen desequilibrios en una dimensión de una negociación con desequilibrios en otra dimensión. En este sentido, los humanos buscan un sentido general de equidad en una interacción. En la Sección 2 delineamos los aspectos del modelado de la negociación humana que cubrimos en este trabajo. Luego, en la Sección 3 introducimos el lenguaje de negociación. La sección 4 explica de manera general la arquitectura y los conceptos de intimidad y equilibrio, y cómo influyen en la negociación. La sección 5 contiene una descripción de las diferentes métricas utilizadas en el modelo de agente, incluida la intimidad. Finalmente, la Sección 6 describe cómo las estrategias y tácticas utilizan el marco lógico, la intimidad y el equilibrio. 2. NEGOCIACIÓN HUMANA Antes de que comience una negociación, los negociadores humanos preparan los intercambios dialógicos que se pueden realizar a lo largo de las cinco dimensiones LÓGICAS [7]: • Legitimidad. ¿Qué información es relevante para el proceso de negociación? ¿Cuáles son los argumentos persuasivos sobre la equidad de las opciones? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Opciones. ¿Cuáles son los acuerdos posibles que podemos aceptar? • Metas. ¿Cuáles son las cosas subyacentes que necesitamos o nos importan? ¿Cuáles son nuestros objetivos? • Independencia. ¿Qué haremos si la negociación falla? ¿Qué alternativas tenemos? • Compromiso. ¿Qué compromisos pendientes tenemos? Los diálogos de negociación, en este contexto, intercambian movimientos dialógicos, es decir, mensajes, con la intención de obtener información sobre el oponente o revelar información sobre nosotros a lo largo de estas cinco dimensiones: solicitud de información, proponer opciones, informar sobre intereses, hacer promesas, apelar a estándares... Una parte clave de cualquier proceso de negociación es construir un modelo de nuestro(s) oponente(s) a lo largo de estas dimensiones. Todas las expresiones que los agentes hacen durante una negociación revelan información sobre su modelo de LÓGICA actual, es decir, sobre su legitimidad, opciones, objetivos, independencia y compromisos. Además, varias expresiones pueden tener una interpretación utilitaria en el sentido de que un agente puede asociarles una ganancia preferencial. Por ejemplo, una oferta puede informar a nuestro oponente de negociación sobre nuestra disposición a firmar un contrato en los términos expresados en la oferta, y al mismo tiempo el oponente puede calcular cuál es su ganancia utilitaria esperada asociada. Estas dos perspectivas: basada en la información y basada en la utilidad, son centrales en el modelo propuesto en este documento. 2.1 Intimidad y equilibrio en las relaciones. Existen evidencias de estudios psicológicos que indican que los seres humanos buscan un equilibrio en sus relaciones de negociación. La visión clásica es que las personas perciben las asignaciones de recursos como distributivamente justas (es decir, bien equilibradas) si son proporcionales a los insumos o contribuciones (es decir, equitativas). Sin embargo, estudios más recientes [16, 17] muestran que los humanos siguen un conjunto más amplio de normas de justicia distributiva dependiendo de su nivel de intimidad: equidad, igualdad y necesidad. La equidad es la asignación proporcional al esfuerzo (por ejemplo, las ganancias de una empresa van a los accionistas en proporción a su inversión), la igualdad es la asignación en cantidades iguales (por ejemplo, dos amigos comen la misma cantidad de un pastel cocinado por uno de ellos), y la necesidad es la asignación proporcional a la necesidad del recurso (por ejemplo, en caso de escasez de alimentos, una madre le da toda la comida a su bebé). Por ejemplo, si nos encontramos en un entorno puramente económico (baja intimidad), podríamos solicitar equidad para la dimensión de Opciones pero podríamos aceptar igualdad en la dimensión de Objetivos. La percepción de que una relación está en equilibrio (es decir, justa) depende en gran medida de la naturaleza de las relaciones sociales entre individuos (es decir, el nivel de intimidad). En relaciones puramente económicas (por ejemplo, negocios), la equidad se percibe como más justa; en relaciones donde la acción conjunta o el fomento de relaciones sociales son el objetivo (por ejemplo, amistades), la igualdad se percibe como más justa; y en situaciones donde el desarrollo personal o el bienestar personal son el objetivo (por ejemplo, familia), las asignaciones suelen basarse en la necesidad. Creemos que la percepción del equilibrio en los diálogos (en negociaciones u otros contextos) se basa en las relaciones sociales, y que cada dimensión de una interacción entre humanos puede correlacionarse con la cercanía social o intimidad entre las partes involucradas. Según los estudios previos, cuanto mayor sea la intimidad en las cinco dimensiones de la LÓGICA, más se utiliza la norma de necesidad, y cuanto menor sea la intimidad, más se utiliza la norma de equidad. Esto podría ser parte de nuestra evolución social. Hay amplias evidencias de que cuando las sociedades humanas evolucionaron de una estructura de cazadores-recolectores a una basada en refugios, la probabilidad de supervivencia aumentó cuando la comida escaseaba. En este contexto, podemos ver claramente que, por ejemplo, las familias intercambian no solo bienes, sino también información y conocimiento basado en la necesidad, y que pocas familias considerarían sus relaciones como desequilibradas y, por lo tanto, injustas, cuando hay una fuerte asimetría en los intercambios (una madre explicando todo a sus hijos, o comprando juguetes, no espera reciprocidad). En el caso de las parejas, hay algunas pruebas [3] de que las asignaciones de bienes y cargas (es decir, utilidades positivas y negativas) se perciben como justas, o en equilibrio, basadas en la equidad para las cargas y la igualdad para los bienes. Consulte la Tabla 1 para ver algunos ejemplos de balances deseados a lo largo de las dimensiones de LÓGICA. El equilibrio percibido en un diálogo de negociación permite a los negociadores inferir información sobre su oponente, sobre su postura lógica, y comparar sus relaciones con todos los negociadores. Por ejemplo, si percibimos que cada vez que solicitamos información esta es proporcionada, y que no se devuelven preguntas significativas, o no se presentan quejas sobre no recibir información, entonces probablemente significa que nuestro oponente percibe nuestra relación social como muy cercana. Alternativamente, podemos detectar qué problemas están causando una carga a nuestro oponente observando un desequilibrio en la información o en los sentidos utilitarios sobre ese tema. 3. MODELO DE COMUNICACIÓN 3.1 Ontología Para definir un lenguaje que estructure los diálogos de los agentes, necesitamos una ontología que incluya un repertorio (mínimo) de elementos: un conjunto de conceptos (por ejemplo, cantidad, calidad, material) organizados en una jerarquía de es-un (por ejemplo, el ornitorrinco es un mamífero, el dólar australiano es una moneda), y un conjunto de relaciones sobre estos conceptos (por ejemplo, precio(cerveza, AUD)). Modelamos ontologías siguiendo un enfoque algebraico [8] de la siguiente manera: Una ontología es una tupla O = (C, R, ≤, σ) donde: 1. C es un conjunto finito de símbolos conceptuales (incluyendo tipos de datos básicos); 2. En su forma más pura, los individuos en estas sociedades recolectan alimentos y los consumen cuando y donde se encuentran. Esta es una participación equitativa pura de los recursos, la ganancia es proporcional al esfuerzo. En estas sociedades existen unidades familiares, alrededor de un refugio, que representan la estructura básica de compartir alimentos. Por lo general, la comida se acumula en el refugio para uso futuro. Entonces, la ingesta de alimentos depende más de la necesidad de los miembros. Por lo general, también se requiere un conjunto de axiomas definidos sobre los conceptos y relaciones. Omitiremos esto aquí. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Elemento 1031 Un nuevo socio comercial mi carnicero mi jefe mi socio mis hijos Legitimidad equidad equidad equidad igualdad necesidad Opciones equidad equidad equidad mixta necesidad Metas equidad necesidad equidad necesidad necesidad Independencia equidad equidad igualdad necesidad necesidad Compromiso equidad equidad equidad mixta necesidad una equidad en la carga, igualdad en lo bueno Tabla 1: Algunos ejemplos de equilibrios deseados (sentido de justicia) dependiendo de la relación, donde ≤ es la jerarquía tradicional es-un. Para simplificar los cálculos en el cálculo de distribuciones de probabilidad, asumimos que hay un número de árboles disjuntos que cubren diferentes espacios ontológicos (por ejemplo, un árbol para tipos de tela, un árbol para formas de ropa, y así sucesivamente). R contiene relaciones entre los conceptos en la jerarquía, esto es necesario para definir objetos (por ejemplo, acuerdos) que se definen como una tupla de problemas. La distancia semántica entre conceptos dentro de una ontología depende de qué tan lejos estén en la estructura definida por la relación ≤. La distancia semántica juega un papel fundamental en las estrategias para la agencia basada en la información. Cómo los contratos firmados, Commit(·), sobre objetos en una región semántica particular, y su ejecución, Done(·), afectan nuestro proceso de toma de decisiones sobre la firma de contratos futuros en regiones semánticas cercanas es crucial para modelar el sentido común que los seres humanos aplican en la gestión de relaciones comerciales. Una medida [10] basa la similitud semántica entre dos conceptos en la longitud del camino inducido por ≤ (mayor distancia en el grafo de ≤ significa menos similitud semántica), y en la profundidad del concepto subsumidor (ancestro común) en el camino más corto entre los dos conceptos (cuanto más profundo en la jerarquía, más cercano es el significado de los conceptos). La similitud semántica se define entonces como: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h donde l es la longitud (es decir, el número de saltos) del camino más corto entre los conceptos, h es la profundidad del concepto más profundo que subsume ambos conceptos, y κ1 y κ2 son parámetros que escalan las contribuciones de la longitud del camino más corto y la profundidad respectivamente. 3.2 Lenguaje La forma del lenguaje que α utiliza para representar la información recibida y el contenido de sus diálogos depende de dos nociones fundamentales. Primero, cuando los agentes interactúan dentro de una institución general aceptan explícita o implícitamente las normas que limitarán su comportamiento, y aceptan las sanciones y penalizaciones establecidas cuando se violan las normas. En segundo lugar, los diálogos en los que α participa se construyen en torno a dos acciones fundamentales: (i) transmitir información y (ii) intercambiar propuestas y contratos. Un contrato δ = (a, b) entre los agentes α y β es un par donde a y b representan las acciones de las que los agentes α y β son responsables respectivamente. Los contratos firmados por agentes y la información transmitida por agentes son similares a las normas en el sentido de que obligan a los agentes a comportarse de una manera particular, ya sea para cumplir con las condiciones del contrato o para hacer que el mundo sea coherente con la información transmitida. Los contratos y la información pueden ser considerados como declaraciones normativas que restringen el comportamiento de un agente. Normas, contratos e información tienen una dimensión temporal obvia. Por lo tanto, un agente debe cumplir con una norma mientras se encuentre dentro de una institución, un contrato tiene un período de validez y una pieza de información es verdadera solo durante un intervalo de tiempo. El conjunto de normas que afectan el comportamiento de un agente define el contexto que el agente debe tener en cuenta. El lenguaje de comunicación α tiene dos primitivas fundamentales: Commit(α, β, ϕ) para representar, en ϕ, el mundo que α pretende lograr y que β tiene derecho a verificar, quejarse o reclamar compensación por cualquier desviación, y Done(μ) para representar el evento de que cierta acción μ ha tenido lugar. De esta manera, las normas, contratos y fragmentos de información se representarán como instancias de Commit(·) donde α y β pueden ser agentes individuales o instituciones. C es: μ ::= illoc(α, β, ϕ, t) | μ; μ | Dejar contexto En μ Fin ϕ ::= término | Hecho(μ) | Comprometer(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv contexto ::= ϕ | id = ϕ | cláusula de prolog | contexto; contexto donde ϕv es una fórmula con la variable libre v, illoc es cualquier conjunto apropiado de partículas ilocucionarias, ; significa secuenciación, y contexto representa acuerdos previos, ilocuciones previas, el contexto de trabajo ontológico, que es una proyección de los árboles ontológicos que representan el foco de la conversación, o código que alinea las diferencias ontológicas entre los hablantes necesarias para interpretar una acción a. Representar una ontología como un conjunto de predicados en Prolog es sencillo. El término establecido contiene instancias de los conceptos y relaciones de la ontología. Por ejemplo, podemos representar la siguiente oferta: Si gastas un total de más de 100 euros en mi tienda durante octubre, entonces te daré un descuento del 10% en todos los productos en noviembre, como: Oferta( α, β, gastado(β, α, octubre, X) ∧ X ≥ 100€ → ∀ y. Hecho(Inform(ξ, α, pagar(β, α, y), noviembre)) → Compromiso(α, β, descuento(y,10%))) ξ es un agente institucional que informa el pago. Sin pérdida de generalidad, asumiremos que todas las acciones son dialógicas. Asumimos la convención de que C(c) significa que c es una instancia del concepto C y r(c1, . . . , cn) determina implícitamente que ci es una instancia del concepto en la i-ésima posición de la relación r. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: La arquitectura del agente LOGIC 4. La arquitectura de agentes de un sistema multiagente {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contiene un agente α que interactúa con otros agentes de argumentación, βi, agentes proveedores de información, θj, y un agente institucional, ξ, que representa la institución donde asumimos que ocurren las interacciones [2]. El agente institucional informa de manera pronta y honesta sobre lo que realmente ocurre después de que un agente firma un contrato o realiza algún otro tipo de compromiso. En la Sección 4.1 esto nos permite medir la diferencia entre una expresión y una observación posterior. El lenguaje de comunicación C introducido en la Sección 3.2 nos permite tanto estructurar los diálogos como estructurar el procesamiento de la información recopilada por los agentes. Los agentes tienen un lenguaje interno de primer orden probabilístico L utilizado para representar un modelo del mundo, Mt. Una arquitectura genérica basada en la información se describe en detalle en [15]. La arquitectura del agente LOGIC se muestra en la Figura 1. El agente α actúa en respuesta a una necesidad que se expresa en términos de la ontología. Una necesidad puede ser exógena, como la necesidad de comerciar de manera rentable y puede ser desencadenada por otro agente que ofrece comerciar, o endógena, como α decidiendo que posee más vino del que necesita. Las necesidades activan el razonamiento proactivo como objetivo/plan, mientras que otros mensajes son manejados por el razonamiento reactivo. Cada plan se prepara para la negociación reuniendo el contenido de una maleta lógica que el agente lleva a la negociación. La estrategia de relaciones determina con qué agente negociar para una necesidad específica; utiliza un análisis de gestión de riesgos para preservar un conjunto estratégico de relaciones comerciales para cada necesidad crítica de la misión, lo cual no se detalla aquí. Para cada relación comercial, esta estrategia genera un objetivo de relación que se expresa en el marco de LOGIC como un nivel deseado de intimidad que se debe lograr a largo plazo. Cada negociación consiste en un diálogo, Ψt, entre dos agentes, con el agente α contribuyendo con el enunciado μ y la parte. Cada uno de los planes y reacciones de α contiene constructores para un modelo de mundo inicial Mt. A continuación, Mt se mantiene a partir de percepciones recibidas utilizando funciones de actualización que transforman percepciones en restricciones en Mt; para más detalles, ver [14, 15]. La evidencia empírica muestra que en la negociación humana, se logran mejores resultados al sesgar las opciones iniciales a favor del proponente. No tenemos conocimiento de ninguna investigación empírica de esta hipótesis para agentes autónomos en escenarios reales de trading. Cada diálogo, Ψt, se evalúa utilizando el marco LOGIC en términos del valor de Ψt tanto para α como para β - ver Sección 5.2. La estrategia de negociación determina el conjunto actual de opciones {δi}, y luego las tácticas, guiadas por el objetivo de la negociación, deciden cuáles, si alguna, de estas opciones presentar y las envuelven en un diálogo argumentativo - ver Sección 6. Ahora describimos dos de las distribuciones en Mt que respaldan el intercambio de ofertas. Pt (acc(α, β, χ, δ)) estima la probabilidad de que α acepte la propuesta δ para satisfacer su necesidad χ, donde δ = (a, b) es un par de compromisos, a para α y b para β. α aceptará δ si: Pt (acc(α, β, χ, δ)) > c, para un nivel de certeza c. Esta estimación se compone de puntos de vista subjetivos y objetivos de aceptabilidad. La estimación subjetiva tiene en cuenta: en qué medida la promulgación de δ satisfará la necesidad α de χ, cuánto vale δ para α y en qué medida α cree que estará en condiciones de cumplir su compromiso a [14, 15]. Sα(β, a) es una variable aleatoria que denota la estimación de α de la valoración subjetiva de β de a sobre algún espacio de evaluación finito y numérico. La estimación objetiva determina si δ es aceptable en el mercado abierto, y la variable Uα(b) denota la valoración en el mercado abierto de αs de la implementación del compromiso b, nuevamente tomada sobre algún espacio de valoración numérica finito. También consideramos las necesidades, la variable Tα(β, a) denota la estimación de α de la fuerza de la necesidad motivadora de β para la promulgación del compromiso a sobre un espacio de valoración. Entonces, para δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) donde g ∈ [0, 1] es la avaricia de α, h ∈ [0, 1] es el grado de altruismo de α, y s ≈ 1 se deriva de la postura8 descrita en la Sección 6. Los parámetros g y h son independientes. Podemos imaginar una relación que comienza con g = 1 y h = 0. Entonces, a medida que los agentes comparten cantidades crecientes de información sobre sus valoraciones de mercado abiertas, g disminuye gradualmente a 0, y luego, a medida que comparten cantidades crecientes de información sobre sus necesidades, h aumenta a 1. La base para el criterio de aceptación ha evolucionado de la equidad a la igualdad, y luego a la necesidad. Pt (acc(β, α, δ)) estima la probabilidad de que β acepte δ, observando las respuestas de β. Por ejemplo, si β envía el mensaje Oferta(δ1) entonces α deriva la restricción: {Pt (acc(β, α, δ1)) = 1} en la distribución Pt (β, α, δ), y si esta es una contraoferta a una oferta anterior de α, δ0, entonces: {Pt (acc(β, α, δ0)) = 0}. En el caso especial no atípico de la negociación de múltiples problemas donde las preferencias de los agentes sobre los problemas individuales son conocidas y son complementarias entre sí, el razonamiento de entropía máxima se puede aplicar para estimar la probabilidad de que cualquier δ de múltiples problemas sea aceptable para β enumerando los posibles mundos que representan el límite de aceptabilidad de β [6]. 4.1 Actualización del modelo de mundo de Mt El modelo de mundo de α consiste en distribuciones de probabilidad que representan su incertidumbre en el estado del mundo. α está interesada 8 Si α decide inflar sus opciones iniciales, esto se logra en la Sección 6 aumentando el valor de s. Si s es 1, entonces un acuerdo puede no ser posible. Esto ilustra la conocida ineficiencia de la negociación bilateral establecida analíticamente por Myerson y Satterthwaite en 1983. La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1033 en el grado en que una enunciación describe con precisión lo que se observará posteriormente. Todas las observaciones sobre el mundo son recibidas como enunciados de un agente de una institución totalmente veraz ξ. Por ejemplo, si β comunica el objetivo de que tengo hambre y la negociación posterior termina con β comprando un libro a α (por ξ informando a α que cierta cantidad de dinero ha sido acreditada en la cuenta de α), entonces α puede concluir que el objetivo que β eligió satisfacer era algo distinto al hambre. Por lo tanto, el modelo del mundo de α contiene distribuciones de probabilidad que representan sus expectativas inciertas de lo que se observará en función de las expresiones recibidas. Representamos la relación entre la emisión, ϕ, y la observación subsiguiente, ϕ, mediante Pt (ϕ |ϕ) ∈ Mt, donde ϕ y ϕ pueden ser categorías ontológicas en interés de la viabilidad computacional. Por ejemplo, si ϕ es \"Entregaré un balde de pescado a ti mañana\", entonces la distribución P(ϕ |ϕ) no necesariamente debe abarcar todas las posibles acciones que β podría realizar, sino que podría ser sobre categorías ontológicas que resuman las acciones posibles de β. En ausencia de enunciados entrantes, las probabilidades condicionales, Pt (ϕ |ϕ), deberían tender hacia la ignorancia representada por una distribución límite de decaimiento D(ϕ |ϕ). α puede tener conocimiento previo sobre D(ϕ |ϕ) a medida que t → ∞, de lo contrario α puede asumir que tiene entropía máxima mientras sea consistente con los datos. En general, dada una distribución, Pt (Xi), y una distribución límite de decaimiento D(Xi), Pt (Xi) decae de la siguiente manera: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) donde Δi es la función de decaimiento para el Xi que satisface la propiedad de que limt→∞ Pt (Xi) = D(Xi). Por ejemplo, Δi podría ser lineal: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), donde νi < 1 es la tasa de decaimiento para la i-ésima distribución. Tanto la función de decaimiento como la distribución límite de decaimiento podrían ser también una función del tiempo: Δt i y Dt (Xi). Supongamos que α recibe una expresión μ = illoc(α, β, ϕ, t) del agente β en el tiempo t. Supongamos que α asigna una creencia epistémica Rt (α, β, μ) a μ - esta probabilidad tiene en cuenta el nivel de precaución personal de α. Modelamos la actualización de Pt (ϕ |ϕ) en dos casos, uno para observaciones dadas ϕ, y otro para observaciones dadas φ en el vecindario semántico de ϕ. 4.2 Actualización de Pt (ϕ |ϕ) dada ϕ. Primero, si ϕk es observado, entonces α puede establecer Pt+1 (ϕk|ϕ) en algún valor d donde {ϕ1, ϕ2, . . . , ϕm} es el conjunto de todas las posibles observaciones. Estimamos la distribución posterior completa Pt+1 (ϕ |ϕ) aplicando el principio de mínima entropía relativa9 de la siguiente manera. Sea p(μ) la distribución: 9 Dada una distribución de probabilidad q, la distribución de entropía relativa mínima p = (p1, . . . , pI ) sujeta a un conjunto de J restricciones lineales g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (que debe incluir la restricción P i pi − 1 = 0) es: p = arg minr P j rj log rj qj. Esto puede calcularse introduciendo multiplicadores de Lagrange λ: L(p, λ) = P j pj log pj qj + λ · g. Minimizando L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J es el conjunto de restricciones dadas g, y una solución a ∂L ∂pi = 0, i = 1, . . . , I conduce eventualmente a p. La inferencia basada en entropía es una forma de inferencia bayesiana que es conveniente cuando los datos son escasos [5] y encapsula el razonamiento de sentido común [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j que satisface la restricción p(μ)k = d. Luego, dejemos que q(μ) sea la distribución: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) y luego, dejemos que: r(μ) = ( q(μ) si q(μ) es más interesante que Pt (ϕ |ϕ) Pt (ϕ |ϕ) en caso contrario Una medida general de si q(μ) es más interesante que Pt (ϕ |ϕ) es: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), donde K(x y) = P j xj ln xj yj es la distancia de Kullback-Leibler entre dos distribuciones de probabilidad x e y [11]. Finalmente, incorporando la Ecuación 2 obtenemos el método para actualizar una distribución Pt (ϕ |ϕ) al recibir un mensaje μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3). Este procedimiento trata con la degradación de la integridad, y con dos probabilidades: primero, la probabilidad z en la enunciación μ, y segundo la creencia Rt (α, β, μ) que α adjunta a μ. 4.3 Actualización de Pt (φ |φ) dada ϕ El método sim: Dado como arriba μ = illoc(α, β, ϕ, t) y la observación ϕk definimos el vector t por ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) con {φ1, φ2, . . . , φp} el conjunto de todas las posibles observaciones en el contexto de φ e i = 1, . . . , p. t no es una distribución de probabilidad. El factor de multiplicación Sim(ϕ , φ) limita la variación de la probabilidad a aquellas fórmulas cuyo contexto ontológico no está demasiado alejado de la observación. El posterior Pt+1 (φ |φ) se obtiene con la Ecuación 3 con r(μ) definido como la normalización de t. El método de valoración: Para un φk dado, wexp (φk) = Pm j=1 Pt (φj|φk) · w(φj) es la expectativa αs del valor de lo que se observará dado que β ha declarado que se observará φk, para alguna medida w. Ahora supongamos que, como antes, α observa ϕk después de que el agente β ha declarado ϕ. α revisa la estimación previa de la valoración esperada wexp (φk) a la luz de la observación ϕk a: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) para alguna función g - la idea es, por ejemplo, que si la ejecución, ϕk, del compromiso, ϕ, de suministrar queso fue devaluada, entonces la expectativa de α del valor de un compromiso, φ, de suministrar vino debería disminuir. Estimamos el posterior aplicando el principio de entropía relativa mínima como en la Ecuación 3, donde la distribución p(μ) = p(φ |φ) satisface la restricción: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5. Las medidas de resumen Un diálogo, Ψt, entre los agentes α y β es una secuencia de enunciados interrelacionados en un contexto. Una relación, Ψ∗t, es una secuencia de diálogos. Primero medimos la confianza que un agente tiene en otro observando, para cada enunciado, la diferencia entre lo que se dice (el enunciado) y lo que 1034 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ocurre posteriormente (la observación). Segundo evaluamos cada diálogo a medida que avanza en términos del marco lógico; esta evaluación emplea las <br>medidas de confianza</br>. Finalmente definimos la intimidad de una relación como una agregación del valor de sus diálogos componentes. 5.1 Confianza. Las medidas de confianza generalizan lo que comúnmente se llaman medidas de confianza, fiabilidad y reputación en un marco computacional único que abarca las categorías de LÓGICA. En la Sección 5.2 se aplican <br>medidas de confianza</br> para valorar el cumplimiento de promesas en la categoría de Legitimidad, que anteriormente llamábamos honor [14], para la ejecución de compromisos, que anteriormente llamábamos confianza [13], y para valorar diálogos en la categoría de Objetivos, que anteriormente llamábamos fiabilidad [14]. Observaciones ideales. Considera una distribución de observaciones que represente α de manera ideal en el sentido de que es lo mejor que α podría esperar razonablemente observar. Esta distribución será una función de αs en el contexto con β denotado por e, y es Pt I (ϕ |ϕ, e). Aquí medimos la entropía relativa entre esta distribución ideal, Pt I (ϕ |ϕ, e), y la distribución de observaciones esperadas, Pt (ϕ |ϕ). Eso es: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) donde el 1 es una constante elegida arbitrariamente siendo el valor máximo que esta medida puede tener. Esta ecuación mide la confianza para una única declaración ϕ. Tiene sentido agregar estos valores sobre una clase de enunciados, digamos sobre aquellos ϕ que están en el contexto ontológico o, es decir, ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) donde Pt β(ϕ) es una distribución de probabilidad sobre el espacio de enunciados que el próximo enunciado β hará a α es ϕ. De manera similar, para una estimación general de la confianza de β en α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Observaciones preferidas. La medida anterior requiere que se especifique una distribución ideal, Pt I (ϕ |ϕ, e), para cada ϕ. Aquí medimos en qué medida la observación ϕ es preferible a la declaración original ϕ. Dado un predicado Preferir(c1, c2, e) que significa que α prefiere c1 a c2 en el entorno e. Entonces si ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Preferir(ϕ , ϕ, o))Pt (ϕ |ϕ) y: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certeza en la observación. Aquí medimos la consistencia en las observaciones aceptables esperadas, o la falta de incertidumbre esperada en esas posibles observaciones que son mejores que la declaración original. Si ϕ ≤ o, entonces: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ para alguna constante κ, y: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) donde Pt +(ϕ |ϕ) es la normalización de Pt (ϕ |ϕ) para ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 si |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| en otro caso. Como se mencionó anteriormente, agregamos esta medida para observaciones en un contexto particular o, y medimos la confianza como antes. Nota computacional. Las diversas medidas mencionadas anteriormente implican cálculos extensos. Por ejemplo, la Ecuación 4 contiene P ϕ que suma sobre todas las posibles observaciones ϕ. Obtenemos una medida más amigable computacionalmente apelando a la estructura de la ontología descrita en la Sección 3.2, y el lado derecho de la Ecuación 4 puede aproximarse a: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) donde Pt η,I (ϕ |ϕ, e) es la normalización de Pt I (ϕ |ϕ, e) para Sim(ϕ , ϕ) ≥ η, y de manera similar para Pt η(ϕ |ϕ). La magnitud de este cálculo está controlada por el parámetro η. Una restricción aún más estricta se puede obtener con: Sim(ϕ, ϕ) ≥ η y ϕ ≤ ψ para algún ψ. 5.2 Valorando diálogos de negociación Supongamos que una negociación comienza en el tiempo s, y para el tiempo t se ha intercambiado una serie de enunciados, Φt = μ1, . . . , μn entre el agente α y el agente β. Este diálogo de negociación es evaluado por α en el contexto del modelo del mundo de α en el tiempo s, Ms, y el entorno e que incluye enunciados que pueden haber sido recibidos de otros agentes en el sistema, incluidas las fuentes de información {θi}. Si Ψt = (Φt , Ms , e), entonces α estima el valor de este diálogo para sí mismo en el contexto de Ms y e como una matriz 2 × 5 Vα(Ψt ) donde: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « donde las funciones I(·) y U(·) son medidas basadas en información y utilidad respectivamente, como describimos a continuación. α estima el valor de este diálogo para β como Vβ(Ψt ) asumiendo que el aparato de razonamiento de β refleja el suyo propio. En términos generales, las valoraciones basadas en la información miden la reducción de incertidumbre, o ganancia de información, que el diálogo proporciona a cada agente, se expresan en términos de disminución de entropía que siempre se puede calcular. Las valoraciones basadas en utilidad que miden la ganancia de utilidad se expresan en términos de alguna función de evaluación de utilidad adecuada U(·) que puede ser difícil de definir. Esta es una razón por la cual el enfoque utilitario no tiene una extensión natural a la gestión de la argumentación que se logra aquí mediante nuestro enfoque basado en la información. Por ejemplo, si α recibe la expresión \"Hoy es martes\", esto puede traducirse en una restricción sobre una única distribución, y la disminución resultante en la entropía es la ganancia de información. Adjuntar una medida utilitaria a esta afirmación puede no ser tan simple. Utilizamos el término matriz 2 × 5 de manera flexible para describir Vα en el sentido de que los elementos de la matriz son listas de medidas que serán determinadas por los requisitos de los agentes. La Tabla 2 muestra una medida de muestra para cada una de las diez categorías, en ella el diálogo comienza en el tiempo s y termina en el tiempo t. En esa Tabla, U(·) es una función de evaluación de utilidad adecuada, needs(β, χ) significa que el agente β necesita la necesidad χ, cho(β, χ, γ) significa que el agente β satisface la necesidad χ eligiendo negociar The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1035 con el agente γ, N es el conjunto de necesidades elegidas de la ontología a algún nivel adecuado de abstracción, Tt es el conjunto de ofertas sobre la mesa en el tiempo t, com(β, γ, b) significa que el agente β tiene un compromiso pendiente con el agente γ para ejecutar el compromiso b donde b está definido en la ontología a algún nivel adecuado de abstracción, B es el número de tales compromisos, y hay n + 1 agentes en el sistema. 5.3 Intimidad y Equilibrio El equilibrio en un diálogo de negociación, Ψt, se define como: Bαβ(Ψt) = Vα(Ψt) Vβ(Ψt) para un operador de diferencia elemento por elemento que respeta la estructura de V(Ψt). La intimidad entre los agentes α y β, I∗t αβ, es el patrón de las dos matrices 2 × 5 V ∗t α y V ∗t β que son calculadas por una función de actualización a medida que cada ronda de negociación termina, I∗t αβ = ` V ∗t α , V ∗t β ´. Si Ψt termina en el tiempo t: V ∗t+1 x = ν × Vx(Ψt) + (1 − ν) × V ∗t x (5) donde ν es la tasa de aprendizaje, y x = α, β. Además, V ∗t x decae continuamente por: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, donde x = α, β; τ es la tasa de decaimiento, y Dx es una matriz de 2 × 5 que representa la distribución límite de decaimiento para el valor del agente x de la intimidad de la relación en ausencia de cualquier interacción. Dx es la reputación del agente x. El equilibrio de la relación entre los agentes α y β es: B∗t αβ = V ∗t α V ∗t β. En particular, la intimidad determina los valores de los parámetros g y h en la Ecuación 1. Como ejemplo simple, si tanto IO α (Ψ∗t ) como IO β (Ψ∗t ) aumentan, entonces g disminuye, y a medida que los ocho componentes restantes de LÓGICA basada en la información aumentan, h aumenta. La noción de equilibrio puede aplicarse a pares de enunciados tratándolos como diálogos degenerados. En la negociación simple de múltiples problemas, la estrategia de revelación equitativa de información generaliza la estrategia de ojo por ojo en la negociación de un solo problema, y se extiende a una estrategia de argumentación de ojo por ojo al aplicar el mismo principio en el marco de LOGIC. 6. ESTRATEGIAS Y TÁCTICAS Cada negociación tiene que lograr dos objetivos. Primero puede tener la intención de lograr algún resultado contractual. En segundo lugar, se buscará contribuir al crecimiento, o declive, de la intimidad en la relación. Ahora describimos con mayor detalle el contenido de la caja de Negociación en la Figura 1. La literatura sobre negociación siempre aconseja que el comportamiento de un agente no debe ser predecible incluso en relaciones cercanas e íntimas. La variación requerida de comportamiento se describe normalmente como variando la postura de negociación que varía informalmente de persona amigable a persona dura. La postura se muestra en la Figura 1, inyecta ruido aleatorio acotado en el proceso, donde el límite se estrecha a medida que aumenta la intimidad. La postura, St αβ, es una matriz de 2 × 5 de multiplicadores elegidos al azar, cada uno ≈ 1, que perturba las acciones de α. El valor en la posición (x, y) en la matriz, donde x = I, U y y = L, O, G, I, C, se elige al azar de [1 l(I∗t αβ, x, y), l(I∗t αβ, x, y)] donde l(I∗t αβ, x, y) es el límite, e I∗t αβ es la intimidad. La estrategia de negociación se preocupa por mantener un conjunto de opciones de trabajo. Si el conjunto de opciones está vacío, entonces α abandonará la negociación. α perturba el mecanismo de aceptación (ver Sección 4) derivando s de la matriz St αβ, como el valor en la posición (I, O). De acuerdo con el comentario en la Nota al pie 7, en las primeras etapas de la negociación α puede decidir inflar sus Opciones iniciales. Esto se logra aumentando el valor de s en la Ecuación 1. La estrategia siguiente utiliza la maquinaria descrita en la Sección 4. Corrige h, g, s y c, establece las Opciones como el conjunto vacío, deja que Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, luego: • repite lo siguiente tantas veces como se desee: añade δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} a Opciones, elimina {y ∈ Dt s | Sim(y, δ) < k} para algún k de Dt s. Al utilizar Pt (acc(β, α, δ)), esta estrategia reacciona al historial de β de propuestas y rechazos. Las tácticas de negociación se ocupan de seleccionar algunas opciones y envolverlas en argumentación. Las interacciones previas con el agente β habrán producido un patrón de intimidad expresado en la forma de `V ∗t α, V ∗t β´. Supongamos que el objetivo de la relación es (T∗t α , T∗t β ). Siguiendo la Ecuación 5, α querrá alcanzar un objetivo de negociación, Nβ(Ψt), tal que: ν · Nβ(Ψt) + (1 − ν) · V ∗t β esté un poco del lado de T∗t β de V ∗t β: Nβ(Ψt) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) para un κ pequeño ∈ [0, ν] que representa la tasa de desarrollo deseada por α para su relación con β. Nβ(Ψt) es una matriz de 2 × 5 que contiene variaciones en las dimensiones de LÓGICA que α le gustaría revelar a β durante Ψt (por ejemplo, Proporcionaré un poco más de información sobre las opciones de lo habitual, seré más flexible en las concesiones sobre las opciones, etc. Es razonable esperar que β avance hacia su objetivo al mismo ritmo y Nα(Ψt) se calcula reemplazando β por α en la Ecuación 6. Nα(Ψt) es lo que α espera recibir de β durante Ψt. Esto proporciona un objetivo de equilibrio de negociación de: Nα(Ψt ) Nβ(Ψt ) que puede ser utilizado como base para tácticas reactivas al esforzarse por mantener este equilibrio en las dimensiones de LÓGICA. Una táctica cautelosa podría utilizar el equilibrio para limitar la respuesta μ a cada enunciado μ de β por la restricción: Vα(μ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt) Nβ(Ψt)), donde ⊗ es la multiplicación de matrices elemento por elemento, y St αβ es la postura. Una táctica menos neurótica podría intentar lograr el equilibrio de negociación objetivo a lo largo del diálogo completo anticipado. Si un equilibrio vinculado requiere la revelación de información negativa en una categoría LÓGICA, entonces α no contribuirá en nada a ello, y dejará esto a la descomposición natural de la reputación D como se describe arriba. 7. DISCUSIÓN En este artículo hemos introducido un enfoque novedoso para la negociación que utiliza información y medidas teóricas de juegos fundamentadas en estudios de negocios y psicología. Introduce los conceptos de intimidad y equilibrio como elementos clave para comprender qué es una estrategia y táctica de negociación. La negociación se entiende como un diálogo que afecta cinco dimensiones básicas: Legitimidad, Opciones, Objetivos, Independencia y Compromiso. Cada movimiento dialógico produce un cambio en una matriz de 2×5 que evalúa el diálogo a lo largo de cinco medidas basadas en la información y cinco medidas basadas en la utilidad. Los niveles actuales de equilibrio e intimidad y los niveles deseados, o de objetivo, son utilizados por las tácticas para determinar qué decir a continuación. Actualmente estamos explorando el uso de este modelo como una extensión de un software de eProcurement ampliamente utilizado comercializado por iSOCO, una empresa derivada del laboratorio de uno de los autores. 1036 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Tabla 2: Medidas de muestra para cada categoría en Vα(Ψt ). (De manera similar para Vβ(Ψt).) Carles Sierra es parcialmente apoyado por el proyecto europeo OpenKnowledge STREP y por el Proyecto IEA español. 8. REFERENCIAS [1] Adams, J. S. Desigualdad en el intercambio social. En Avances en psicología social experimental, L. Berkowitz, Ed., vol. 2. Nueva York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodríguez, J. Ingeniería ambiental para sistemas multiagentes, por A. y Sierra, C. Revista sobre Aplicaciones de la Inteligencia Artificial en Ingeniería 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F. y White, S. B. Reversión de la preferencia en decisiones de asignación: juzgar una alternativa versus elegir entre alternativas. Administración Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., y Nalebuff, B. Co-Opetition: Una mentalidad revolucionaria que combina competencia y cooperación. Doubleday, Nueva York, 1996. [5] Cheeseman, P., y Stutz, J. Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería. Instituto Americano de Física, Melville, NY, EE. UU., 2004, cap. Sobre la relación entre la inferencia bayesiana y la entropía máxima, pp. 445-461. [6] Debenham, J. Negociando con información. En Actas de la Tercera Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2004 (julio de 2004), N. Jennings, C. Sierra, L. Sonenberg y M. Tambe, Eds., ACM Press, Nueva York, pp. 664 - 671. [7] Fischer, R., Ury, W. y Patton, B. Llegar a Sí: Negociar acuerdos sin ceder. Penguin Books, 1995. [8] Kalfoglou, Y., y Schorlemmer, M. IF-Map: Un método de mapeo de ontologías basado en la teoría del flujo de información. En el Journal on Data Semantics I, S. Spaccapietra, S. March y K. Aberer, Eds., vol. 2800 de Lecture Notes in Computer Science. Springer-Verlag: Heidelberg, Alemania, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., y Minton, J. W. Elementos esenciales de la negociación. McGraw Hill, 2001. [10] Li, Y., Bandar, Z. \n\nMcGraw Hill, 2001. [10] Li, Y., Bandar, Z. A., y McLean, D. Un enfoque para medir la similitud semántica entre palabras utilizando múltiples fuentes de información. IEEE Transactions on Knowledge and Data Engineering 15, 4 (julio / agosto de 2003), 871 - 882. [11] MacKay, D. Teoría de la Información, Inferencia y Algoritmos de Aprendizaje. Cambridge University Press, 2003. [12] París, J. Sentido común y entropía máxima. Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., y Debenham, J. Un modelo basado en la información para la confianza. En Actas de la Cuarta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2005 (Utrecht, Países Bajos, julio de 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh y M. Wooldridge, Eds., ACM Press, Nueva York, pp. 497 - 504. [14] Sierra, C., y Debenham, J. Confianza y honor en una agencia basada en la información. En Actas de la Quinta Conferencia Internacional sobre Agentes Autónomos y Sistemas Multiagente AAMAS-2006 (Hakodate, Japón, mayo de 2006), Stone, P. y Weiss, G., Eds., ACM Press, Nueva York, pp. 1225-1232. [15] Sierra, C. y Debenham, J. Agencia basada en la información. En Actas de la Vigésima Conferencia Internacional Conjunta de Inteligencia Artificial IJCAI-07 (Hyderabad, India, enero de 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A. y Pinkley, R. Las asignaciones negociadas de beneficios y cargas: el impacto de la valencia del resultado, la contribución y la relación. Comportamiento Organizacional y Procesos de Decisión Humana, 3 (diciembre de 1995), 249-260. [17] Valley, K. L., Neale, M. A. y Mannix, E. A. Amigos, amantes, colegas, desconocidos: Los efectos de las relaciones en el proceso y resultado de las negociaciones. En Investigación en Negociación en Organizaciones, R. Bies, R. Lewicki y B. Sheppard, Eds., vol. 5. JAI Press, 1995, pp. 65-94. \n\nJAI Press, 1995, pp. 65-94. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1037 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "negotiation": {
            "translated_key": "negociación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The LOGIC <br>negotiation</br> Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a <br>negotiation</br> model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The intimacy is a pair of matrices that evaluate both an agents contribution to the relationship and its opponents contribution each from an information view and from a utilitarian view across the five LOGIC dimensions.",
                "The balance is the difference between these matrices.",
                "A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future.",
                "The <br>negotiation</br> strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive <br>negotiation</br> balances towards the target intimacy.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Theory 1.",
                "INTRODUCTION In this paper we propose a new <br>negotiation</br> model to deal with long term relationships that are founded on successive <br>negotiation</br> encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that <br>negotiation</br> is an information exchange process as well as a utility exchange process [15, 14].",
                "We believe that if agents are to succeed in real application domains they have to reconcile both views: informational and gametheoretical.",
                "Our aim is to model trading scenarios where agents represent their human principals, and thus we want their behaviour to be comprehensible by humans and to respect usual human <br>negotiation</br> procedures, whilst being consistent with, and somehow extending, game theoretical and information theoretical results.",
                "In this sense, agents are not just utility maximisers, but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them.",
                "These two concepts, intimacy and balance are key in the model, and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships (i.e. at different intimacy levels).",
                "These two theories are too specific and distinct to describe how a (business) relationship might grow because interactions have some aspects of these two extremes on a continuum in which, for example, agents reveal increasing amounts of private information as their intimacy grows.",
                "We dont follow the Co-Opetition aproach [4] where co-operation and competition depend on the issue under <br>negotiation</br>, but instead we belief that the willingness to co-operate/compete affect all aspects in the <br>negotiation</br> process.",
                "<br>negotiation</br> strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level.",
                "It is common in human settings to use tactics that compensate for unbalances in one dimension of a <br>negotiation</br> with unbalances in another dimension.",
                "In this sense, humans aim at a general sense of fairness in an interaction.",
                "In Section 2 we outline the aspects of human <br>negotiation</br> modelling that we cover in this work.",
                "Then, in Section 3 we introduce the <br>negotiation</br> language.",
                "Section 4 explains in outline the architecture and the concepts of intimacy and balance, and how they influence the <br>negotiation</br>.",
                "Section 5 contains a description of the different metrics used in the agent model including intimacy.",
                "Finally, Section 6 outlines how strategies and tactics use the LOGIC framework, intimacy and balance. 2.",
                "HUMAN <br>negotiation</br> Before a <br>negotiation</br> starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions [7]: • Legitimacy.",
                "What information is relevant to the <br>negotiation</br> process?",
                "What are the persuasive arguments about the fairness of the options? 1030 978-81-904262-7-5 (RPS) c 2007 IFAAMAS • Options.",
                "What are the possible agreements we can accept? • Goals.",
                "What are the underlying things we need or care about?",
                "What are our goals? • Independence.",
                "What will we do if the <br>negotiation</br> fails?",
                "What alternatives have we got? • Commitment.",
                "What outstanding commitments do we have?",
                "<br>negotiation</br> dialogues, in this context, exchange dialogical moves, i.e. messages, with the intention of getting information about the opponent or giving away information about us along these five dimensions: request for information, propose options, inform about interests, issue promises, appeal to standards . . .",
                "A key part of any <br>negotiation</br> process is to build a model of our opponent(s) along these dimensions.",
                "All utterances agents make during a <br>negotiation</br> give away information about their current LOGIC model, that is, about their legitimacy, options, goals, independence, and commitments.",
                "Also, several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them.",
                "For instance, an offer may inform our <br>negotiation</br> opponent about our willingness to sign a contract in the terms expressed in the offer, and at the same time the opponent can compute what is its associated expected utilitarian gain.",
                "These two views: informationbased and utility-based, are central in the model proposed in this paper. 2.1 Intimacy and Balance in relationships There is evidence from psychological studies that humans seek a balance in their <br>negotiation</br> relationships.",
                "The classical view [1] is that people perceive resource allocations as being distributively fair (i.e. well balanced) if they are proportional to inputs or contributions (i.e. equitable).",
                "However, more recent studies [16, 17] show that humans follow a richer set of norms of distributive justice depending on their intimacy level: equity, equality, and need.",
                "Equity being the allocation proportional to the effort (e.g. the profit of a company goes to the stock holders proportional to their investment), equality being the allocation in equal amounts (e.g. two friends eat the same amount of a cake cooked by one of them), and need being the allocation proportional to the need for the resource (e.g. in case of food scarcity, a mother gives all food to her baby).",
                "For instance, if we are in a purely economic setting (low intimacy) we might request equity for the Options dimension but could accept equality in the Goals dimension.",
                "The perception of a relation being in balance (i.e. fair) depends strongly on the nature of the social relationships between individuals (i.e. the intimacy level).",
                "In purely economical relationships (e.g., business), equity is perceived as more fair; in relations where joint action or fostering of social relationships are the goal (e.g. friends), equality is perceived as more fair; and in situations where personal development or personal welfare are the goal (e.g. family), allocations are usually based on need.",
                "We believe that the perception of balance in dialogues (in <br>negotiation</br> or otherwise) is grounded on social relationships, and that every dimension of an interaction between humans can be correlated to the social closeness, or intimacy, between the parties involved.",
                "According to the previous studies, the more intimacy across the five LOGIC dimensions the more the need norm is used, and the less intimacy the more the equity norm is used.",
                "This might be part of our social evolution.",
                "There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce.",
                "In this context, we can clearly see that, for instance, families exchange not only goods but also information and knowledge based on need, and that few families would consider their relationships as being unbalanced, and thus unfair, when there is a strong asymmetry in the exchanges (a mother explaining everything to her children, or buying toys, does not expect reciprocity).",
                "In the case of partners there is some evidence [3] that the allocations of goods and burdens (i.e. positive and negative utilities) are perceived as fair, or in balance, based on equity for burdens and equality for goods.",
                "See Table 1 for some examples of desired balances along the LOGIC dimensions.",
                "The perceived balance in a <br>negotiation</br> dialogue allows negotiators to infer information about their opponent, about its LOGIC stance, and to compare their relationships with all negotiators.",
                "For instance, if we perceive that every time we request information it is provided, and that no significant questions are returned, or no complaints about not receiving information are given, then that probably means that our opponent perceives our social relationship to be very close.",
                "Alternatively, we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue. 3.",
                "COMMUNICATION MODEL 3.1 Ontology In order to define a language to structure agent dialogues we need an ontology that includes a (minimum) repertoire of elements: a set of concepts (e.g. quantity, quality, material) organised in a is-a hierarchy (e.g. platypus is a mammal, Australian-dollar is a currency), and a set of relations over these concepts (e.g. price(beer,AUD)).3 We model ontologies following an algebraic approach [8] as: An ontology is a tuple O = (C, R, ≤, σ) where: 1.",
                "C is a finite set of concept symbols (including basic data types); 2.",
                "R is a finite set of relation symbols; 3. ≤ is a reflexive, transitive and anti-symmetric relation on C (a partial order) 4. σ : R → C+ is the function assigning to each relation symbol its arity 1 In its purest form, individuals in these societies collect food and consume it when and where it is found.",
                "This is a pure equity sharing of the resources, the gain is proportional to the effort. 2 In these societies there are family units, around a shelter, that represent the basic food sharing structure.",
                "Usually, food is accumulated at the shelter for future use.",
                "Then the food intake depends more on the need of the members. 3 Usually, a set of axioms defined over the concepts and relations is also required.",
                "We will omit this here.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031 Element A new trading partner my butcher my boss my partner my children Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need a equity on burden, equality on good Table 1: Some desired balances (sense of fairness) examples depending on the relationship. where ≤ is the traditional is-a hierarchy.",
                "To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces (e.g. a tree for types of fabric, a tree for shapes of clothing, and so on).",
                "R contains relations between the concepts in the hierarchy, this is needed to define objects (e.g. deals) that are defined as a tuple of issues.",
                "The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the ≤ relation.",
                "Semantic distance plays a fundamental role in strategies for information-based agency.",
                "How signed contracts, Commit(·), about objects in a particular semantic region, and their execution, Done(·), affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships.",
                "A measure [10] bases the semantic similarity between two concepts on the path length induced by ≤ (more distance in the ≤ graph means less semantic similarity), and the depth of the subsumer concept (common ancestor) in the shortest path between the two concepts (the deeper in the hierarchy, the closer the meaning of the concepts).",
                "Semantic similarity is then defined as: Sim(c, c ) = e−κ1l · eκ2h − e−κ2h eκ2h + e−κ2h where l is the length (i.e. number of hops) of the shortest path between the concepts, h is the depth of the deepest concept subsuming both concepts, and κ1 and κ2 are parameters scaling the contributions of the shortest path length and the depth respectively. 3.2 Language The shape of the language that α uses to represent the information received and the content of its dialogues depends on two fundamental notions.",
                "First, when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour, and accept the established sanctions and penalties whenever norms are violated.",
                "Second, the dialogues in which α engages are built around two fundamental actions: (i) passing information, and (ii) exchanging proposals and contracts.",
                "A contract δ = (a, b) between agents α and β is a pair where a and b represent the actions that agents α and β are responsible for respectively.",
                "Contracts signed by agents and information passed by agents, are similar to norms in the sense that they oblige agents to behave in a particular way, so as to satisfy the conditions of the contract, or to make the world consistent with the information passed.",
                "Contracts and Information can thus be thought of as normative statements that restrict an agents behaviour.",
                "Norms, contracts, and information have an obvious temporal dimension.",
                "Thus, an agent has to abide by a norm while it is inside an institution, a contract has a validity period, and a piece of information is true only during an interval in time.",
                "The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account. αs communication language has two fundamental primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α aims at bringing about and that β has the right to verify, complain about or claim compensation for any deviations from, and Done(μ) to represent the event that a certain action μ4 has taken place.",
                "In this way, norms, contracts, and information chunks will be represented as instances of Commit(·) where α and β can be individual agents or institutions.",
                "C is: μ ::= illoc(α, β, ϕ, t) | μ; μ | Let context In μ End ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv context ::= ϕ | id = ϕ | prolog clause | context; context where ϕv is a formula with free variable v, illoc is any appropriate set of illocutionary particles, ; means sequencing, and context represents either previous agreements, previous illocutions, the ontological working context, that is a projection of the ontological trees that represent the focus of the conversation, or code that aligns the ontological differences between the speakers needed to interpret an action a.",
                "Representing an ontology as a set predicates in Prolog is simple.",
                "The set term contains instances of the ontology concepts and relations.5 For example, we can represent the following offer: If you spend a total of more than e100 in my shop during October then I will give you a 10% discount on all goods in November, as: Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 → ∀ y.",
                "Done(Inform(ξ, α, pay(β, α, y), November)) → Commit(α, β, discount(y,10%))) ξ is an institution agent that reports the payment. 4 Without loss of generality we will assume that all actions are dialogical. 5 We assume the convention that C(c) means that c is an instance of concept C and r(c1, . . . , cn) implicitly determines that ci is an instance of the concept in the i-th position of the relation r. 1032 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: The LOGIC agent architecture 4.",
                "AGENT ARCHITECTURE A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains an agent α that interacts with other argumentation agents, βi, information providing agents, θj, and an institutional agent, ξ, that represents the institution where we assume the interactions happen [2].",
                "The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract, or makes some other form of commitment.",
                "In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation.",
                "The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents.",
                "Agents have a probabilistic first-order internal language L used to represent a world model, Mt .",
                "A generic information-based architecture is described in detail in [15].",
                "The LOGIC agent architecture is shown in Figure 1.",
                "Agent α acts in response to a need that is expressed in terms of the ontology.",
                "A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade, or endogenous such as α deciding that it owns more wine than it requires.",
                "Needs trigger αs goal/plan proactive reasoning, while other messages are dealt with by αs reactive reasoning.6 Each plan prepares for the <br>negotiation</br> by assembling the contents of a LOGIC briefcase that the agent carries into the negotiation7 .",
                "The relationship strategy determines which agent to negotiate with for a given need; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need - this is not detailed here.",
                "For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term.",
                "Each <br>negotiation</br> consists of a dialogue, Ψt , between two agents with agent α contributing utterance μ and the part6 Each of αs plans and reactions contain constructors for an initial world model Mt .",
                "Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt - for details, see [14, 15]. 7 Empirical evidence shows that in human <br>negotiation</br>, better outcomes are achieved by skewing the opening Options in favour of the proposer.",
                "We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios. ner β contributing μ using the language described in Section 3.2.",
                "Each dialogue, Ψt , is evaluated using the LOGIC framework in terms of the value of Ψt to both α and β - see Section 5.2.",
                "The <br>negotiation</br> strategy then determines the current set of Options {δi}, and then the tactics, guided by the <br>negotiation</br> target, decide which, if any, of these Options to put forward and wraps them in argumentation dialogue - see Section 6.",
                "We now describe two of the distributions in Mt that support offer exchange.",
                "Pt (acc(α, β, χ, δ)) estimates the probability that α should accept proposal δ in satisfaction of her need χ, where δ = (a, b) is a pair of commitments, a for α and b for β. α will accept δ if: Pt (acc(α, β, χ, δ)) > c, for level of certainty c. This estimate is compounded from subjective and objective views of acceptability.",
                "The subjective estimate takes account of: the extent to which the enactment of δ will satisfy αs need χ, how much δ is worth to α, and the extent to which α believes that she will be in a position to execute her commitment a [14, 15].",
                "Sα(β, a) is a random variable denoting αs estimate of βs subjective valuation of a over some finite, numerical evaluation space.",
                "The objective estimate captures whether δ is acceptable on the open market, and variable Uα(b) denotes αs open-market valuation of the enactment of commitment b, again taken over some finite numerical valuation space.",
                "We also consider needs, the variable Tα(β, a) denotes αs estimate of the strength of βs motivating need for the enactment of commitment a over a valuation space.",
                "Then for δ = (a, b): Pt (acc(α, β, χ, δ)) = Pt „ Tα(β, a) Tα(α, b) «h × „ Sα(α, b) Sα(β, a) «g × Uα(b) Uα(a) ≥ s ! (1) where g ∈ [0, 1] is αs greed, h ∈ [0, 1] is αs degree of altruism, and s ≈ 1 is derived from the stance8 described in Section 6.",
                "The parameters g and h are independent.",
                "We can imagine a relationship that begins with g = 1 and h = 0.",
                "Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0, and then as they share increasing amounts of information about their needs h increases to 1.",
                "The basis for the acceptance criterion has thus developed from equity to equality, and then to need.",
                "Pt (acc(β, α, δ)) estimates the probability that β would accept δ, by observing βs responses.",
                "For example, if β sends the message Offer(δ1) then α derives the constraint: {Pt (acc(β, α, δ1)) = 1} on the distribution Pt (β, α, δ), and if this is a counter offer to a former offer of αs, δ0, then: {Pt (acc(β, α, δ0)) = 0}.",
                "In the not-atypical special case of multi-issue bargaining where the agents preferences over the individual issues only are known and are complementary to each others, maximum entropy reasoning can be applied to estimate the probability that any multi-issue δ will be acceptable to β by enumerating the possible worlds that represent βs limit of acceptability [6]. 4.1 Updating the World Model Mt αs world model consists of probability distributions that represent its uncertainty in the world state. α is interested 8 If α chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s. If s 1 then a deal may not be possible.",
                "This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033 in the degree to which an utterance accurately describes what will subsequently be observed.",
                "All observations about the world are received as utterances from an all-truthful institution agent ξ.",
                "For example, if β communicates the goal I am hungry and the subsequent <br>negotiation</br> terminates with β purchasing a book from α (by ξ advising α that a certain amount of money has been credited to αs account) then α may conclude that the goal that β chose to satisfy was something other than hunger.",
                "So, αs world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received.",
                "We represent the relationship between utterance, ϕ, and subsequent observation, ϕ , by Pt (ϕ |ϕ) ∈ Mt , where ϕ and ϕ may be ontological categories in the interest of computational feasibility.",
                "For example, if ϕ is I will deliver a bucket of fish to you tomorrow then the distribution P(ϕ |ϕ) need not be over all possible things that β might do, but could be over ontological categories that summarise βs possible actions.",
                "In the absence of in-coming utterances, the conditional probabilities, Pt (ϕ |ϕ), should tend to ignorance as represented by a decay limit distribution D(ϕ |ϕ). α may have background knowledge concerning D(ϕ |ϕ) as t → ∞, otherwise α may assume that it has maximum entropy whilst being consistent with the data.",
                "In general, given a distribution, Pt (Xi), and a decay limit distribution D(Xi), Pt (Xi) decays by: Pt+1 (Xi) = Δi(D(Xi), Pt (Xi)) (2) where Δi is the decay function for the Xi satisfying the property that limt→∞ Pt (Xi) = D(Xi).",
                "For example, Δi could be linear: Pt+1 (Xi) = (1 − νi) × D(Xi) + νi × Pt (Xi), where νi < 1 is the decay rate for the ith distribution.",
                "Either the decay function or the decay limit distribution could also be a function of time: Δt i and Dt (Xi).",
                "Suppose that α receives an utterance μ = illoc(α, β, ϕ, t) from agent β at time t. Suppose that α attaches an epistemic belief Rt (α, β, μ) to μ - this probability takes account of αs level of personal caution.",
                "We model the update of Pt (ϕ |ϕ) in two cases, one for observations given ϕ, second for observations given φ in the semantic neighbourhood of ϕ. 4.2 Update of Pt (ϕ |ϕ) given ϕ First, if ϕk is observed then α may set Pt+1 (ϕk|ϕ) to some value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible observations.",
                "We estimate the complete posterior distribution Pt+1 (ϕ |ϕ) by applying the principle of minimum relative entropy9 as follows.",
                "Let p(μ) be the distribution: 9 Given a probability distribution q, the minimum relative entropy distribution p = (p1, . . . , pI ) subject to a set of J linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J (that must include the constraint P i pi − 1 = 0) is: p = arg minr P j rj log rj qj .",
                "This may be calculated by introducing Lagrange multipliers λ: L(p, λ) = P j pj log pj qj + λ · g. Minimising L, { ∂L ∂λj = gj(p) = 0}, j = 1, . . . , J is the set of given constraints g, and a solution to ∂L ∂pi = 0, i = 1, . . . , I leads eventually to p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse [5] and encapsulates common-sense reasoning [12]. arg minx P j xj log xj Pt(ϕ |ϕ)j that satisfies the constraint p(μ)k = d. Then let q(μ) be the distribution: q(μ) = Rt (α, β, μ) × p(μ) + (1 − Rt (α, β, μ)) × Pt (ϕ |ϕ) and then let: r(μ) = ( q(μ) if q(μ) is more interesting than Pt (ϕ |ϕ) Pt (ϕ |ϕ) otherwise A general measure of whether q(μ) is more interesting than Pt (ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt (ϕ |ϕ) D(ϕ |ϕ)), where K(x y) = P j xj ln xj yj is the Kullback-Leibler distance between two probability distributions x and y [11].",
                "Finally incorporating Eqn. 2 we obtain the method for updating a distribution Pt (ϕ |ϕ) on receipt of a message μ: Pt+1 (ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3) This procedure deals with integrity decay, and with two probabilities: first, the probability z in the utterance μ, and second the belief Rt (α, β, μ) that α attached to μ. 4.3 Update of Pt (φ |φ) given ϕ The sim method: Given as above μ = illoc(α, β, ϕ, t) and the observation ϕk we define the vector t by ti = Pt (φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ) with {φ1, φ2, . . . , φp} the set of all possible observations in the context of φ and i = 1, . . . , p. t is not a probability distribution.",
                "The multiplying factor Sim(ϕ , φ) limits the variation of probability to those formulae whose ontological context is not too far away from the observation.",
                "The posterior Pt+1 (φ |φ) is obtained with Equation 3 with r(μ) defined to be the normalisation of t. The valuation method: For a given φk, wexp (φk) =Pm j=1 Pt (φj|φk) · w(φj) is αs expectation of the value of what will be observed given that β has stated that φk will be observed, for some measure w. Now suppose that, as before, α observes ϕk after agent β has stated ϕ. α revises the prior estimate of the expected valuation wexp (φk) in the light of the observation ϕk to: (wrev (φk) | (ϕk|ϕ)) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) for some function g - the idea being, for example, that if the execution, ϕk, of the commitment, ϕ, to supply cheese was devalued then αs expectation of the value of a commitment, φ, to supply wine should decrease.",
                "We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the constraint: p X j=1 p(ϕ ,ϕ)j · wi(φj) = g(wexp (φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk)) 5.",
                "SUMMARY MEASURES A dialogue, Ψt , between agents α and β is a sequence of inter-related utterances in context.",
                "A relationship, Ψ∗t , is a sequence of dialogues.",
                "We first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) subsequently occurs (the observation).",
                "Second we evaluate each dialogue as it progresses in terms of the LOGIC framework - this evaluation employs the confidence measures.",
                "Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues. 5.1 Confidence Confidence measures generalise what are commonly called trust, reliability and reputation measures into a single computational framework that spans the LOGIC categories.",
                "In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing dialogues in the Goals category - we formerly called this reliability [14].",
                "Ideal observations.",
                "Consider a distribution of observations that represent αs ideal in the sense that it is the best that α could reasonably expect to observe.",
                "This distribution will be a function of αs context with β denoted by e, and is Pt I (ϕ |ϕ, e).",
                "Here we measure the relative entropy between this ideal distribution, Pt I (ϕ |ϕ, e), and the distribution of expected observations, Pt (ϕ |ϕ).",
                "That is: C(α, β, ϕ) = 1 − X ϕ Pt I (ϕ |ϕ, e) log Pt I (ϕ |ϕ, e) Pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.",
                "This equation measures confidence for a single statement ϕ.",
                "It makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: C(α, β, o) = 1 − P ϕ:ϕ≤o Pt β(ϕ) [1 − C(α, β, ϕ)] P ϕ:ϕ≤o Pt β(ϕ) where Pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ.",
                "Similarly, for an overall estimate of βs confidence in α: C(α, β) = 1 − X ϕ Pt β(ϕ) [1 − C(α, β, ϕ)] Preferred observations.",
                "The previous measure requires that an ideal distribution, Pt I (ϕ |ϕ, e), has to be specified for each ϕ.",
                "Here we measure the extent to which the observation ϕ is preferable to the original statement ϕ.",
                "Given a predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. Then if ϕ ≤ o: C(α, β, ϕ) = X ϕ Pt (Prefer(ϕ , ϕ, o))Pt (ϕ |ϕ) and: C(α, β, o) = P ϕ:ϕ≤o Pt β(ϕ)C(α, β, ϕ) P ϕ:ϕ≤o Pt β(ϕ) Certainty in observation.",
                "Here we measure the consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.",
                "If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘ ϕ | Pt (Prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: C(α, β, ϕ) = 1 + 1 B∗ · X ϕ ∈Φ+(ϕ,o,κ) Pt +(ϕ |ϕ) log Pt +(ϕ |ϕ) where Pt +(ϕ |ϕ) is the normalisation of Pt (ϕ |ϕ) for ϕ ∈ Φ+(ϕ, o, κ), B∗ = ( 1 if |Φ+(ϕ, o, κ)| = 1 log |Φ+(ϕ, o, κ)| otherwise As above we aggregate this measure for observations in a particular context o, and measure confidence as before.",
                "Computational Note.",
                "The various measures given above involve extensive calculations.",
                "For example, Eqn. 4 containsP ϕ that sums over all possible observations ϕ .",
                "We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2, and the right-hand side of Eqn. 4 may be approximated to: 1 − X ϕ :Sim(ϕ ,ϕ)≥η Pt η,I (ϕ |ϕ, e) log Pt η,I (ϕ |ϕ, e) Pt η(ϕ |ϕ) where Pt η,I (ϕ |ϕ, e) is the normalisation of Pt I (ϕ |ϕ, e) for Sim(ϕ , ϕ) ≥ η, and similarly for Pt η(ϕ |ϕ).",
                "The extent of this calculation is controlled by the parameter η.",
                "An even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ. 5.2 Valuing <br>negotiation</br> dialogues Suppose that a <br>negotiation</br> commences at time s, and by time t a string of utterances, Φt = μ1, . . . , μn has been exchanged between agent α and agent β.",
                "This <br>negotiation</br> dialogue is evaluated by α in the context of αs world model at time s, Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.",
                "Let Ψt = (Φt , Ms , e), then α estimates the value of this dialogue to itself in the context of Ms and e as a 2 × 5 array Vα(Ψt ) where: Vx(Ψt ) = „ IL x (Ψt ) IO x (Ψt ) IG x (Ψt ) II x(Ψt ) IC x (Ψt ) UL x (Ψt ) UO x (Ψt ) UG x (Ψt ) UI x(Ψt ) UC x (Ψt ) « where the I(·) and U(·) functions are information-based and utility-based measures respectively as we now describe. α estimates the value of this dialogue to β as Vβ(Ψt ) by assuming that βs reasoning apparatus mirrors its own.",
                "In general terms, the information-based valuations measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.",
                "The utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function U(·) that can be difficult to define.",
                "This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach.",
                "For example, if α receives the utterance Today is Tuesday then this may be translated into a constraint on a single distribution, and the resulting decrease in entropy is the information gain.",
                "Attaching a utilitarian measure to this utterance may not be so simple.",
                "We use the term 2 × 5 array loosely to describe Vα in that the elements of the array are lists of measures that will be determined by the agents requirements.",
                "Table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. In that Table, U(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035 with agent γ, N is the set of needs chosen from the ontology at some suitable level of abstraction, Tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, B is the number of such commitments, and there are n + 1 agents in the system. 5.3 Intimacy and Balance The balance in a <br>negotiation</br> dialogue, Ψt , is defined as: Bαβ(Ψt ) = Vα(Ψt ) Vβ(Ψt ) for an element-by-element difference operator that respects the structure of V (Ψt ).",
                "The intimacy between agents α and β, I∗t αβ, is the pattern of the two 2 × 5 arrays V ∗t α and V ∗t β that are computed by an update function as each <br>negotiation</br> round terminates, I∗t αβ = ` V ∗t α , V ∗t β ´ .",
                "If Ψt terminates at time t: V ∗t+1 x = ν × Vx(Ψt ) + (1 − ν) × V ∗t x (5) where ν is the learning rate, and x = α, β. Additionally, V ∗t x continually decays by: V ∗t+1 x = τ × V ∗t x + (1 − τ) × Dx, where x = α, β; τ is the decay rate, and Dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.",
                "Dx is the reputation of agent x.",
                "The relationship balance between agents α and β is: B∗t αβ = V ∗t α V ∗t β .",
                "In particular, the intimacy determines values for the parameters g and h in Equation 1.",
                "As a simple example, if both IO α (Ψ∗t ) and IO β (Ψ∗t ) increase then g decreases, and as the remaining eight information-based LOGIC components increase, h increases.",
                "The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues.",
                "In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework. 6.",
                "STRATEGIES AND TACTICS Each <br>negotiation</br> has to achieve two goals.",
                "First it may be intended to achieve some contractual outcome.",
                "Second it will aim to contribute to the growth, or decline, of the relationship intimacy.",
                "We now describe in greater detail the contents of the <br>negotiation</br> box in Figure 1.",
                "The <br>negotiation</br> literature consistently advises that an agents behaviour should not be predictable even in close, intimate relationships.",
                "The required variation of behaviour is normally described as varying the <br>negotiation</br> stance that informally varies from friendly guy to tough guy.",
                "The stance is shown in Figure 1, it injects bounded random noise into the process, where the bound tightens as intimacy increases.",
                "The stance, St αβ, is a 2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that perturbs αs actions.",
                "The value in the (x, y) position in the matrix, where x = I, U and y = L, O, G, I, C, is chosen at random from [ 1 l(I∗t αβ ,x,y) , l(I∗t αβ, x, y)] where l(I∗t αβ, x, y) is the bound, and I∗t αβ is the intimacy.",
                "The <br>negotiation</br> strategy is concerned with maintaining a working set of Options.",
                "If the set of options is empty then α will quit the <br>negotiation</br>. α perturbs the acceptance machinery (see Section 4) by deriving s from the St αβ matrix such as the value at the (I, O) position.",
                "In line with the comment in Footnote 7, in the early stages of the <br>negotiation</br> α may decide to inflate her opening Options.",
                "This is achieved by increasing the value of s in Equation 1.",
                "The following strategy uses the machinery described in Section 4.",
                "Fix h, g, s and c, set the Options to the empty set, let Dt s = {δ | Pt (acc(α, β, χ, δ) > c}, then: • repeat the following as many times as desired: add δ = arg maxx{Pt (acc(β, α, x)) | x ∈ Dt s} to Options, remove {y ∈ Dt s | Sim(y, δ) < k} for some k from Dt s By using Pt (acc(β, α, δ)) this strategy reacts to βs history of Propose and Reject utterances.",
                "<br>negotiation</br> tactics are concerned with selecting some Options and wrapping them in argumentation.",
                "Prior interactions with agent β will have produced an intimacy pattern expressed in the form of ` V ∗t α , V ∗t β ´ .",
                "Suppose that the relationship target is (T∗t α , T∗t β ).",
                "Following from Equation 5, α will want to achieve a <br>negotiation</br> target, Nβ(Ψt ) such that: ν · Nβ(Ψt ) + (1 − ν) · V ∗t β is a bit on the T∗t β side of V ∗t β : Nβ(Ψt ) = ν − κ ν V ∗t β ⊕ κ ν T∗t β (6) for small κ ∈ [0, ν] that represents αs desired rate of development for her relationship with β. Nβ(Ψt ) is a 2 × 5 matrix containing variations in the LOGIC dimensions that α would like to reveal to β during Ψt (e.g.",
                "Ill pass a bit more information on options than usual, Ill be stronger in concessions on options, etc.).",
                "It is reasonable to expect β to progress towards her target at the same rate and Nα(Ψt ) is calculated by replacing β by α in Equation 6.",
                "Nα(Ψt ) is what α hopes to receive from β during Ψt .",
                "This gives a <br>negotiation</br> balance target of: Nα(Ψt ) Nβ(Ψt ) that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions.",
                "A cautious tactic could use the balance to bound the response μ to each utterance μ from β by the constraint: Vα(μ ) Vβ(μ) ≈ St αβ ⊗ (Nα(Ψt ) Nβ(Ψt )), where ⊗ is element-by-element matrix multiplication, and St αβ is the stance.",
                "A less neurotic tactic could attempt to achieve the target <br>negotiation</br> balance over the anticipated complete dialogue.",
                "If a balance bound requires negative information revelation in one LOGIC category then α will contribute nothing to it, and will leave this to the natural decay to the reputation D as described above. 7.",
                "DISCUSSION In this paper we have introduced a novel approach to <br>negotiation</br> that uses information and game-theoretical measures grounded on business and psychological studies.",
                "It introduces the concepts of intimacy and balance as key elements in understanding what is a <br>negotiation</br> strategy and tactic.",
                "<br>negotiation</br> is understood as a dialogue that affect five basic dimensions: Legitimacy, Options, Goals, Independence, and Commitment.",
                "Each dialogical move produces a change in a 2×5 matrix that evaluates the dialogue along five information-based measures and five utility-based measures.",
                "The current Balance and intimacy levels and the desired, or target, levels are used by the tactics to determine what to say next.",
                "We are currently exploring the use of this model as an extension of a currently widespread eProcurement software commercialised by iSOCO, a spin-off company of the laboratory of one of the authors. 1036 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) IL α(Ψt ) = X ϕ∈Ψt Ct (α, β, ϕ) − Cs (α, β, ϕ) UL α (Ψt ) = X ϕ∈Ψt X ϕ Pt β(ϕ |ϕ) × Uα(ϕ ) IO α (Ψt ) = P δ∈T t Hs (acc(β, α, δ)) − P δ∈T t Ht (acc(β, α, δ)) |Tt| UO α (Ψt ) = X δ∈T t Pt (acc(β, α, δ)) × X δ Pt (δ |δ)Uα(δ ) IG α (Ψt ) = P χ∈N Hs (needs(β, χ)) − Ht (needs(β, χ)) |N| UG α (Ψt ) = X χ∈N Pt (needs(β, χ)) × Et (Uα(needs(β, χ))) II α(Ψt ) = Po i=1 P χ∈N Hs (cho(β, χ, βi)) − Ht (cho(β, χ, βi)) n × |N| UI α(Ψt ) = oX i=1 X χ∈N Ut (cho(β, χ, βi)) − Us (cho(β, χ, βi)) IC α (Ψt ) = Po i=1 P δ∈B Hs (com(β, βi, b)) − Ht (com(β, βi, b)) n × |B| UC α (Ψt ) = oX i=1 X δ∈B Ut (com(β, βi, b)) − Us (com(β, βi, b)) Table 2: Sample measures for each category in Vα(Ψt ). (Similarly for Vβ(Ψt ).)",
                "Acknowledgements Carles Sierra is partially supported by the OpenKnowledge European STREP project and by the Spanish IEA Project. 8.",
                "REFERENCES [1] Adams, J. S. Inequity in social exchange.",
                "In Advances in experimental social psychology, L. Berkowitz, Ed., vol. 2.",
                "New York: Academic Press, 1965. [2] Arcos, J. L., Esteva, M., Noriega, P., Rodr´ıguez, J.",
                "A., and Sierra, C. Environment engineering for multiagent systems.",
                "Journal on Engineering Applications of Artificial Intelligence 18 (2005). [3] Bazerman, M. H., Loewenstein, G. F., and White, S. B.",
                "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives.",
                "Administration Science Quarterly, 37 (1992), 220-240. [4] Brandenburger, A., and Nalebuff, B. Co-Opetition : A Revolution Mindset That Combines Competition and Cooperation.",
                "Doubleday, New York, 1996. [5] Cheeseman, P., and Stutz, J. Bayesian Inference and Maximum Entropy Methods in Science and Engineering.",
                "American Institute of Physics, Melville, NY, USA, 2004, ch.",
                "On The Relationship between Bayesian and Maximum Entropy Inference, pp. 445461. [6] Debenham, J.",
                "Bargaining with information.",
                "In Proceedings Third International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2004 (July 2004), N. Jennings, C. Sierra, L. Sonenberg, and M. Tambe, Eds., ACM Press, New York, pp. 664 - 671. [7] Fischer, R., Ury, W., and Patton, B.",
                "Getting to Yes: Negotiating agreements without giving in.",
                "Penguin Books, 1995. [8] Kalfoglou, Y., and Schorlemmer, M. IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, S. Spaccapietra, S. March, and K. Aberer, Eds., vol. 2800 of Lecture Notes in Computer Science.",
                "Springer-Verlag: Heidelberg, Germany, 2003, pp. 98-127. [9] Lewicki, R. J., Saunders, D. M., and Minton, J. W. Essentials of <br>negotiation</br>.",
                "McGraw Hill, 2001. [10] Li, Y., Bandar, Z.",
                "A., and McLean, D. An approach for measuring semantic similarity between words using multiple information sources.",
                "IEEE Transactions on Knowledge and Data Engineering 15, 4 (July / August 2003), 871 - 882. [11] MacKay, D. Information Theory, Inference and Learning Algorithms.",
                "Cambridge University Press, 2003. [12] Paris, J.",
                "Common sense and maximum entropy.",
                "Synthese 117, 1 (1999), 75 - 93. [13] Sierra, C., and Debenham, J.",
                "An information-based model for trust.",
                "In Proceedings Fourth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2005 (Utrecht, The Netherlands, July 2005), F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. Singh, and M. Wooldridge, Eds., ACM Press, New York, pp. 497 - 504. [14] Sierra, C., and Debenham, J.",
                "Trust and honour in information-based agency.",
                "In Proceedings Fifth International Conference on Autonomous Agents and Multi Agent Systems AAMAS-2006 (Hakodate, Japan, May 2006), P. Stone and G. Weiss, Eds., ACM Press, New York, pp. 1225 - 1232. [15] Sierra, C., and Debenham, J. Information-based agency.",
                "In Proceedings of Twentieth International Joint Conference on Artificial Intelligence IJCAI-07 (Hyderabad, India, January 2007), pp. 1513-1518. [16] Sondak, H., Neale, M. A., and Pinkley, R. The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship.",
                "Organizational Behaviour and Human Decision Processes, 3 (December 1995), 249-260. [17] Valley, K. L., Neale, M. A., and Mannix, E. A.",
                "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations.",
                "In Research in <br>negotiation</br> in Organizations, R. Bies, R. Lewicki, and B. Sheppard, Eds., vol. 5.",
                "JAI Press, 1995, pp. 65-94.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037"
            ],
            "original_annotated_samples": [
                "The LOGIC <br>negotiation</br> Model Carles Sierra Institut dInvestigacio en Intel.ligencia Artificial Spanish Scientific Research Council, UAB 08193 Bellaterra, Catalonia, Spain sierra@iiia.csic.es John Debenham Faculty of Information Technology University of Technology, Sydney NSW, Australia debenham@it.uts.edu.au ABSTRACT Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC).",
                "We introduce a <br>negotiation</br> model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness).",
                "The <br>negotiation</br> strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive <br>negotiation</br> balances towards the target intimacy.",
                "INTRODUCTION In this paper we propose a new <br>negotiation</br> model to deal with long term relationships that are founded on successive <br>negotiation</br> encounters.",
                "The model is grounded on results from business and psychological studies [1, 16, 9], and acknowledges that <br>negotiation</br> is an information exchange process as well as a utility exchange process [15, 14]."
            ],
            "translated_annotated_samples": [
                "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC).",
                "Introducimos un modelo de <br>negociación</br> basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad).",
                "La estrategia de <br>negociación</br> mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la <br>negociación</br> hacia la intimidad deseada.",
                "INTRODUCCIÓN En este artículo proponemos un nuevo modelo de <br>negociación</br> para tratar relaciones a largo plazo que se basan en encuentros de <br>negociación</br> sucesivos.",
                "El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la <br>negociación</br> es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]."
            ],
            "translated_text": "El Modelo de Negociación LOGIC Carles Sierra Institut d'Investigació en Intel·ligència Artificial Consejo Superior de Investigaciones Científicas, UAB 08193 Bellaterra, Cataluña, España sierra@iiia.csic.es John Debenham Facultad de Tecnologías de la Información Universidad de Tecnología, Sídney NSW, Australia debenham@it.uts.edu.au RESUMEN Los negociadores exitosos se preparan determinando su posición a lo largo de cinco dimensiones: Legitimidad, Opciones, Metas, Independencia y Compromiso (LOGIC). Introducimos un modelo de <br>negociación</br> basado en estas dimensiones y en dos conceptos primitivos: intimidad (grado de cercanía) y equilibrio (grado de equidad). La intimidad es un par de matrices que evalúan tanto la contribución de un agente a la relación como la contribución de su oponente, cada una desde una perspectiva de información y desde una perspectiva utilitaria a lo largo de las cinco dimensiones de la LÓGICA. El equilibrio es la diferencia entre estas matrices. Una estrategia de relaciones mantiene una intimidad objetivo para cada relación hacia la cual un agente quisiera que la relación se moviera en el futuro. La estrategia de <br>negociación</br> mantiene un conjunto de opciones que están alineadas con el nivel actual de intimidad, y luego las tácticas envuelven las opciones en argumentación con el objetivo de lograr un acuerdo exitoso y manipular los equilibrios sucesivos de la <br>negociación</br> hacia la intimidad deseada. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Teoría 1. INTRODUCCIÓN En este artículo proponemos un nuevo modelo de <br>negociación</br> para tratar relaciones a largo plazo que se basan en encuentros de <br>negociación</br> sucesivos. El modelo se basa en resultados de estudios de negocios y psicología [1, 16, 9], y reconoce que la <br>negociación</br> es un proceso de intercambio de información, así como un proceso de intercambio de utilidad [15, 14]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}