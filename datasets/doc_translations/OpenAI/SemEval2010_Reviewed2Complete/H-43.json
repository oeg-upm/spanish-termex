{
    "id": "H-43",
    "original_text": "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks. This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample. It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure. The research in this direction has recently received considerable attention but are still in an early stage. Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features. Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors. Further analysis can be performed based on the compact representation of web pages. In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks. Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1. INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web. Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs). Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on. Performing information management tasks on such structured data raises many new research challenges. In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining. For the classification problem of web pages, a simple approach is to treat web pages as independent documents. The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem. However, this approach relies only on the content of web pages and ignores the structure of links among them. Link structures provide invaluable information about properties of the documents as well as relationships among them. For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects). Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more. From this point of view, the traditional classification methods that ignore the link structure may not be suitable. On the other hand, a few studies, for example [25], rely solely on link structures. It is however a very rare case that content information can be ignorable. For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article. To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration. To achieve this goal, a simple approach is to convert one type of information to the other. For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog. In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links. However, link and content information have different properties. For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way. Therefore, directly converting one type of information to the other usually degrades the quality of information. On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them. We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly. Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure. In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other. In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively. But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18]. In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors. Our model contains two components. The first component captures the content information. This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval. That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors. The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members. A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students). It is worth noting that we do not explicitly define the semantic of a factor a priori. Instead, similar to LSI, the factors are learned from the data. Traditional factor analysis models the variables associated with entities through the factors. However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs. Therefore, the model should involve factors of both vertices of the edge. This is a key difference between traditional factor analysis and our model. In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links). By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly. In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links. Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification. This paper is organized as follows. Section 2 reviews related work. Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content. Section 4 extends the basic framework and a few variants for fine tune. Section 5 shows the experiment results. Section 6 discusses the details of this approach and Section 7 concludes. 2. RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8]. LSI maps documents into a lower dimensional latent space. The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space. The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space. Analysis tasks, such as classification, could be performed on the latent space. The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents. Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages. In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities. Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs. Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities. He et al. [9] propose a clustering algorithm for web document clustering. The algorithm incorporates link structure and the co-citation patterns. In the algorithm, all links are treated as undirected edge of the link graph. The content information is only used for weighing the links by the textual similarity of both ends of the links. Zhang et al. [23] uses the undirected graph regularization framework for document classification. Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content. Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning. The framework combines the hub and authority information of web pages. But it is difficult to combine the content information into that framework. Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors. Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5]. The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis. In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page. In the model, the outgoing links of the destination web page have no effect on the source web page. In other words, the overall link structure is not utilized in PHITS. In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself. The factor of the destination web page contains information of its outgoing links. In turn, such information is passed to the factor of the source web page. As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph. Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships. The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships. RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques. However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions. On the other hand, the inference for RMNs is intractable and requires belief propagation. The following are some work on combining documents and links, but the methods are loosely related to our approach. The experiments of [21] show that using terms from the linked document improves the classification accuracy. Chakrabarti et al.[3] use co-citation information in their classification model. Joachims et al.[11] combine text kernels and co-citation kernels for classification. Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3. OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis. Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages. Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper. For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise. Note that A is an asymmetric matrix, because hyperlinks are directed. Most machine learning algorithms assume a feature-vector representation of instances. For web page classification, however, the link graph does not readily give such a vector representation for web pages. If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages. On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 . The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page. The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient. One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A. The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm. The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U. In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features. A link generation model proposed in [2] is similar to the PCA approach. Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar. However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I. The solution Z is identical subject to a scaling factor. However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization. The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages). The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects. To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j. Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj. This is obviously a miss interpretation to the original link path. To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix. Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A. Again, each row vector of Z corresponds to a feature vector of a web pages. The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z. The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space. We call the Rl space the factor space. Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}. The factor loadings, U, explain how these observed connections happened based on {zi}. Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis. Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1. Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2. In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix. A good low-rank representation should reveal the structure of the factor graph. First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes. The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes. It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different. Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same. Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U. When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints. Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices. To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links. Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords. Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V . Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification. Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation. To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization. The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z. The relationship among these matrices can be depicted as Figure 3. A Y C U Z V Figure 3: Relationship among the matrices. Node Y is the target of classification. Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods. Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV . Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A. Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ). Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ). The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n). Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4. SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem. We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification. However, this approach does not take data labels into account in the first step. Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22]. Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning. Let C be the set of classes. For simplicity, we first consider binary class problem, i.e. C = {−1, 1}. Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise. We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate. Here, w is the norm of the decision boundary. Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0. However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem. To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2. We reduce a multiclass problem into multiple binary ones. One simple scheme of reduction is the one-against-rest coding scheme. In the one-against-rest scheme, we assign a label vector for each class label. The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled. Let Y be the label matrix, each column of which is a label vector. Therefore, Y is a matrix of n × c, where c is the number of classes, |C|. Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term. To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term. Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). We can also use gradient methods to solve the problem of Eq. (8). The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2. Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5. EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach. The two datasets are the WebKB data set[1] and the Cora data set [15]. The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin). The web pages are classified into seven categories. The numbers of pages in each category are shown in Table 1. The Cora data set consists of the abstracts and references of about 34,000 computer science research papers. We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL). We remove those articles without reference to other articles in the set. The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure. We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg. PLSI+PHITS link-content MF link-content sup. MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup. MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents. The features are the bag-ofwords and all word are stemmed. This method ignores link structure in the data. Linear SVM is used. The regularization parameter of SVM is selected using the cross-validation method. The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei. We apply SVM on link features. This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods. We use different weights for these two set of features. The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24]. This method is solely based on link structure. • PLSI+PHITS This method is described in [6]. This method combines text content information and link structure for analysis. The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint. It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3. We use 50 latent factors for Z. After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup. MF This method is our approach of the supervised matrix factorization in Section 4. We use 50 latent factors for Z. After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training. During the training process, we use the crossvalidation to select all model parameters. We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set. The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3. For this task, the accuracies of SVM on links are worse than that of SVM on content. But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy. This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information. The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information. Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches. The difference between the results of our two methods is not significant. However in the experiments below, we show the difference between them. The classification accuracies for the Cora data set are shown in Table 4. In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links. This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg. PLSI+PHITS link-content MF link-content sup. MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields. The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task. Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors. We perform experiments to study how the number of factors affects the accuracy of predication. We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set. The result shown in Figure 4(a) and 4(b). The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup. MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup. MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases. It is a different concept from choosing the optimal number of clusters in clustering application. It is how much information to represent in the latent variables. We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors. To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors. The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases. This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6. DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience. Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers. Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g. LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ . In our paper, we mainly discuss the application of classification. A entry of matrix Z means the relationship of a web page and a factor. The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics. Therefore, we allow the components take any possible real values. When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters. Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages. As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want. Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7. CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application. We propose a simple approach using factors to model the text content and link structure of web pages/documents. The directed links are generated from the linear combination of linkage of between source and destination factors. By sharing factors between text content and link structure, it is easy to combine both the content information and link structure. Our experiments show our approach is effective for classification. We also discuss an extension for clustering application. Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm. Also, thanks to the reviewers for constructive comments. 8. REFERENCES [1] CMU world wide knowledge base (WebKB) project. Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry. Web search via hub synthesis. In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk. Enhanced hypertext categorization using hyperlinks. In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998. ACM Press, New York, US. [4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang. Learning to probabilistically identify authoritative documents. Proc. ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann. The missing link - a probabilistic model of document content and hypertext connectivity. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436. MIT Press, 2001. [7] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X. He, H. Zha, C. Ding, and H. Simon. Web document clustering using hyperlink structures. Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite kernels for hypertext categorisation. In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001. Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi. SVMs for the Blogosphere: Blog Identification and Splog Detection. In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee. Pagerank without hyperlinks: structural re-ranking using links induced by language models. In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the contruction of internet portals with machine learning. Information Retrieval Journal, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng, and M.-H. Lee. A practical hypertext catergorization method using links and incrementally available class information. In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd. PageRank citation ranking: bring order to the web. Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman. General Intelligence, objectively determined and measured. The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong. Document clustering based on non-negative matrix factorization. In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani. A study of approaches to hypertext categorization. Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp. Multi-label informed latent semantic indexing. In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005. ACM Press. [23] T. Zhang, A. Popescul, and B. Dom. Linear prediction models with graph regularization for web-page categorization. In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006. ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf. Learning from labeled and unlabeled data on a directed graph. In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann. Semi-supervised learning on directed graphs. Proc. Neural Info. Processing Systems, 2004.",
    "original_translation": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004.",
    "original_sentences": [
        "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
        "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
        "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
        "The research in this direction has recently received considerable attention but are still in an early stage.",
        "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
        "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
        "Further analysis can be performed based on the compact representation of web pages.",
        "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
        "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
        "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
        "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
        "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
        "Performing information management tasks on such structured data raises many new research challenges.",
        "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
        "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
        "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
        "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
        "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
        "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
        "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
        "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
        "On the other hand, a few studies, for example [25], rely solely on link structures.",
        "It is however a very rare case that content information can be ignorable.",
        "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
        "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
        "To achieve this goal, a simple approach is to convert one type of information to the other.",
        "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
        "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
        "However, link and content information have different properties.",
        "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
        "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
        "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
        "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
        "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
        "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
        "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
        "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
        "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
        "Our model contains two components.",
        "The first component captures the content information.",
        "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
        "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
        "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
        "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
        "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
        "Instead, similar to LSI, the factors are learned from the data.",
        "Traditional factor analysis models the variables associated with entities through the factors.",
        "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
        "Therefore, the model should involve factors of both vertices of the edge.",
        "This is a key difference between traditional factor analysis and our model.",
        "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
        "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
        "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
        "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
        "This paper is organized as follows.",
        "Section 2 reviews related work.",
        "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
        "Section 4 extends the basic framework and a few variants for fine tune.",
        "Section 5 shows the experiment results.",
        "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
        "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
        "LSI maps documents into a lower dimensional latent space.",
        "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
        "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
        "Analysis tasks, such as classification, could be performed on the latent space.",
        "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
        "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
        "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
        "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
        "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
        "He et al. [9] propose a clustering algorithm for web document clustering.",
        "The algorithm incorporates link structure and the co-citation patterns.",
        "In the algorithm, all links are treated as undirected edge of the link graph.",
        "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
        "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
        "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
        "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
        "The framework combines the hub and authority information of web pages.",
        "But it is difficult to combine the content information into that framework.",
        "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
        "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
        "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
        "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
        "In the model, the outgoing links of the destination web page have no effect on the source web page.",
        "In other words, the overall link structure is not utilized in PHITS.",
        "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
        "The factor of the destination web page contains information of its outgoing links.",
        "In turn, such information is passed to the factor of the source web page.",
        "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
        "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
        "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
        "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
        "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
        "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
        "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
        "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
        "Chakrabarti et al. [3] use co-citation information in their classification model.",
        "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
        "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
        "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
        "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
        "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
        "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
        "Note that A is an asymmetric matrix, because hyperlinks are directed.",
        "Most machine learning algorithms assume a feature-vector representation of instances.",
        "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
        "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
        "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
        "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
        "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
        "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
        "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
        "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
        "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
        "A link generation model proposed in [2] is similar to the PCA approach.",
        "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
        "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
        "The solution Z is identical subject to a scaling factor.",
        "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
        "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
        "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
        "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
        "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
        "This is obviously a miss interpretation to the original link path.",
        "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
        "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
        "Again, each row vector of Z corresponds to a feature vector of a web pages.",
        "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
        "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
        "We call the Rl space the factor space.",
        "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
        "The factor loadings, U, explain how these observed connections happened based on {zi}.",
        "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
        "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
        "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
        "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
        "A good low-rank representation should reveal the structure of the factor graph.",
        "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
        "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
        "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
        "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
        "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
        "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
        "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
        "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
        "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
        "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
        "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
        "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
        "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
        "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
        "The relationship among these matrices can be depicted as Figure 3.",
        "A Y C U Z V Figure 3: Relationship among the matrices.",
        "Node Y is the target of classification.",
        "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
        "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
        "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
        "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
        "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
        "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
        "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
        "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
        "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
        "However, this approach does not take data labels into account in the first step.",
        "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
        "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
        "Let C be the set of classes.",
        "For simplicity, we first consider binary class problem, i.e.",
        "C = {−1, 1}.",
        "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
        "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
        "Here, w is the norm of the decision boundary.",
        "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
        "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
        "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
        "We reduce a multiclass problem into multiple binary ones.",
        "One simple scheme of reduction is the one-against-rest coding scheme.",
        "In the one-against-rest scheme, we assign a label vector for each class label.",
        "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
        "Let Y be the label matrix, each column of which is a label vector.",
        "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
        "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
        "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
        "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
        "We can also use gradient methods to solve the problem of Eq. (8).",
        "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
        "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
        "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
        "The two datasets are the WebKB data set[1] and the Cora data set [15].",
        "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
        "The web pages are classified into seven categories.",
        "The numbers of pages in each category are shown in Table 1.",
        "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
        "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
        "We remove those articles without reference to other articles in the set.",
        "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
        "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
        "PLSI+PHITS link-content MF link-content sup.",
        "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
        "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
        "The features are the bag-ofwords and all word are stemmed.",
        "This method ignores link structure in the data.",
        "Linear SVM is used.",
        "The regularization parameter of SVM is selected using the cross-validation method.",
        "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
        "We apply SVM on link features.",
        "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
        "We use different weights for these two set of features.",
        "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
        "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
        "This method combines text content information and link structure for analysis.",
        "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
        "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
        "We use 50 latent factors for Z.",
        "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
        "MF This method is our approach of the supervised matrix factorization in Section 4.",
        "We use 50 latent factors for Z.",
        "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
        "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
        "During the training process, we use the crossvalidation to select all model parameters.",
        "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
        "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
        "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
        "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
        "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
        "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
        "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
        "The difference between the results of our two methods is not significant.",
        "However in the experiments below, we show the difference between them.",
        "The classification accuracies for the Cora data set are shown in Table 4.",
        "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
        "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
        "PLSI+PHITS link-content MF link-content sup.",
        "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
        "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
        "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
        "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
        "We perform experiments to study how the number of factors affects the accuracy of predication.",
        "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
        "The result shown in Figure 4(a) and 4(b).",
        "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
        "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
        "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
        "It is a different concept from choosing the optimal number of clusters in clustering application.",
        "It is how much information to represent in the latent variables.",
        "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
        "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
        "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
        "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
        "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
        "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
        "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
        "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
        "In our paper, we mainly discuss the application of classification.",
        "A entry of matrix Z means the relationship of a web page and a factor.",
        "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
        "Therefore, we allow the components take any possible real values.",
        "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
        "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
        "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
        "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
        "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
        "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
        "The directed links are generated from the linear combination of linkage of between source and destination factors.",
        "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
        "Our experiments show our approach is effective for classification.",
        "We also discuss an extension for clustering application.",
        "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
        "Also, thanks to the reviewers for constructive comments. 8.",
        "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
        "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
        "Web search via hub synthesis.",
        "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
        "Enhanced hypertext categorization using hyperlinks.",
        "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
        "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
        "Lin.",
        "LIBSVM: a library for support vector machines, 2001.",
        "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
        "Learning to probabilistically identify authoritative documents.",
        "Proc.",
        "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
        "The missing link - a probabilistic model of document content and hypertext connectivity.",
        "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
        "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
        "Support-vector networks.",
        "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
        "Indexing by latent semantic analysis.",
        "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
        "He, H. Zha, C. Ding, and H. Simon.",
        "Web document clustering using hyperlink structures.",
        "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
        "Probabilistic latent semantic indexing.",
        "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
        "Composite kernels for hypertext categorisation.",
        "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
        "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
        "Authoritative sources in a hyperlinked environment.",
        "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
        "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
        "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
        "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
        "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
        "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
        "Automating the contruction of internet portals with machine learning.",
        "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
        "Oh, S. H. Myaeng, and M.-H. Lee.",
        "A practical hypertext catergorization method using links and incrementally available class information.",
        "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
        "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
        "PageRank citation ranking: bring order to the web.",
        "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
        "General Intelligence, objectively determined and measured.",
        "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
        "Discriminative probabilistic models for relational data.",
        "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
        "Document clustering based on non-negative matrix factorization.",
        "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
        "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
        "A study of approaches to hypertext categorization.",
        "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
        "Multi-label informed latent semantic indexing.",
        "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
        "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
        "Linear prediction models with graph regularization for web-page categorization.",
        "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
        "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
        "Learning from labeled and unlabeled data on a directed graph.",
        "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
        "Semi-supervised learning on directed graphs.",
        "Proc.",
        "Neural Info.",
        "Processing Systems, 2004."
    ],
    "translated_text_sentences": [
        "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos.",
        "Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica.",
        "Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces.",
        "La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana.",
        "Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento.",
        "Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad.",
        "Se puede realizar un análisis adicional basado en la representación compacta de las páginas web.",
        "En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora.",
        "Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1.",
        "INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red.",
        "Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs).",
        "En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente.",
        "Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación.",
        "En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos.",
        "Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes.",
        "La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema.",
        "Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas.",
        "Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos.",
        "Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos).",
        "Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple.",
        "Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados.",
        "Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces.",
        "Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada.",
        "Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo.",
        "Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración.",
        "Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro.",
        "Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog.",
        "En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces.",
        "Sin embargo, el enlace y la información del contenido tienen propiedades diferentes.",
        "Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica.",
        "Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información.",
        "Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas.",
        "Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida.",
        "Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces.",
        "En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí.",
        "A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente.",
        "Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18].",
        "En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes.",
        "Nuestro modelo contiene dos componentes.",
        "El primer componente captura la información del contenido.",
        "Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional.",
        "Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos.",
        "El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado.",
        "Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes).",
        "Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori.",
        "En cambio, al igual que en LSI, los factores se aprenden a partir de los datos.",
        "El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores.",
        "Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices.",
        "Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde.",
        "Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo.",
        "En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces).",
        "Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente.",
        "En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces.",
        "Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación.",
        "Este documento está organizado de la siguiente manera.",
        "La sección 2 revisa el trabajo relacionado.",
        "La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido.",
        "La sección 4 amplía el marco básico y algunas variantes para ajustes finos.",
        "La sección 5 muestra los resultados del experimento.",
        "La sección 6 discute los detalles de este enfoque y la sección 7 concluye.",
        "TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8].",
        "LSI mapea documentos en un espacio latente de dimensiones inferiores.",
        "El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente.",
        "La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente.",
        "Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente.",
        "El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales.",
        "Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web.",
        "En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades.",
        "Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros.",
        "En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades.",
        "Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web.",
        "El algoritmo incorpora la estructura de enlaces y los patrones de co-citación.",
        "En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces.",
        "La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces.",
        "Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos.",
        "Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido.",
        "Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado.",
        "El marco combina la información de hub y autoridad de las páginas web.",
        "Pero es difícil combinar la información del contenido en ese marco.",
        "Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores.",
        "Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5].",
        "La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces.",
        "En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino.",
        "En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen.",
        "En otras palabras, la estructura general de enlaces no se utiliza en PHITS.",
        "En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma.",
        "El factor de la página web de destino contiene información sobre sus enlaces salientes.",
        "A su vez, esta información se transmite al factor de la página web de origen.",
        "Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original.",
        "Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades.",
        "El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones.",
        "Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques.",
        "Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales.",
        "Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias.",
        "Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque.",
        "Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación.",
        "Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación.",
        "Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación.",
        "Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3.",
        "NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces.",
        "Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web.",
        "Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo.",
        "Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario.",
        "Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos.",
        "La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características.",
        "Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web.",
        "Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web.",
        "Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso.",
        "La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web.",
        "La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente.",
        "Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A.",
        "El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius.",
        "La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U.",
        "Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad.",
        "Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA.",
        "Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes.",
        "Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I.",
        "La solución Z es idéntica sujeta a un factor de escala.",
        "Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace.",
        "El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web).",
        "La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos.",
        "Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j.",
        "Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj.",
        "Esto es claramente una mala interpretación del enlace original.",
        "Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l.",
        "Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A.",
        "Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web.",
        "La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z.",
        "El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl.",
        "Llamamos al espacio Rl el espacio factor.",
        "Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}.",
        "Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}.",
        "Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis.",
        "Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1.",
        "Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2.",
        "En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces.",
        "Una buena representación de bajo rango debería revelar la estructura del grafo de factores.",
        "Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro.",
        "Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad.",
        "No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes.",
        "Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales.",
        "Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U.",
        "Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas.",
        "Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices.",
        "Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces.",
        "Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave.",
        "Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l.",
        "Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web.",
        "Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta.",
        "Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización.",
        "La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z.",
        "La relación entre estas matrices puede ser representada como se muestra en la Figura 3.",
        "Figura 3: Relación entre las matrices.",
        "El nodo Y es el objetivo de clasificación.",
        "La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton.",
        "La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV.",
        "Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A.",
        "De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2).",
        "Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2).",
        "El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n).",
        "Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4.",
        "CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web.",
        "Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación.",
        "Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso.",
        "Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22].",
        "Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado.",
        "Sea C el conjunto de clases.",
        "Para simplificar, primero consideramos un problema de clase binaria, es decir,",
        "C = {−1, 1}. \n\nC = {−1, 1}.",
        "Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario.",
        "Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar.",
        "Aquí, w es la norma del límite de decisión.",
        "Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0.",
        "Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema.",
        "Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2.",
        "Reducimos un problema de múltiples clases en varios problemas binarios.",
        "Un esquema simple de reducción es el esquema de codificación uno contra el resto.",
        "En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase.",
        "El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado.",
        "Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas.",
        "Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|.",
        "Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término.",
        "Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término.",
        "Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
        "También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8).",
        "Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2.",
        "Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5.",
        "EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque.",
        "Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15].",
        "El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin).",
        "Las páginas web están clasificadas en siete categorías.",
        "Los números de páginas en cada categoría se muestran en la Tabla 1.",
        "El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática.",
        "Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL).",
        "Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto.",
        "El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces.",
        "Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg.",
        "PLSI+PHITS enlace-contenido MF enlace-contenido sup.",
        "Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup.",
        "Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos.",
        "Las características son el modelo de bolsa de palabras y todas las palabras están truncadas.",
        "Este método ignora la estructura de enlaces en los datos.",
        "Se utiliza SVM lineal.",
        "El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada.",
        "La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai.",
        "Aplicamos SVM en las características de enlaces.",
        "Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores.",
        "Utilizamos diferentes pesos para estos dos conjuntos de características.",
        "Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24].",
        "Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6].",
        "Este método combina información del contenido de texto y estructura de enlaces para su análisis.",
        "El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa.",
        "Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3.",
        "Utilizamos 50 factores latentes para Z.",
        "Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup.",
        "Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4.",
        "Utilizamos 50 factores latentes para Z.",
        "Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase.",
        "Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento.",
        "Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo.",
        "Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos.",
        "Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3.",
        "Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido.",
        "Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor.",
        "Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información.",
        "La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información.",
        "Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques.",
        "La diferencia entre los resultados de nuestros dos métodos no es significativa.",
        "Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos.",
        "Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4.",
        "En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces.",
        "Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg.",
        "PLSI+PHITS enlace-contenido MF enlace-contenido sup.",
        "Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup.",
        "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos.",
        "El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea.",
        "Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores.",
        "Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción.",
        "Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora.",
        "El resultado mostrado en la Figura 4(a) y 4(b).",
        "Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible.",
        "Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup.",
        "La precisión aumenta a medida que aumenta el número de factores.",
        "Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento.",
        "Es cuánta información representar en las variables latentes.",
        "Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores.",
        "Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores.",
        "La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores.",
        "Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6.",
        "Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional.",
        "De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos.",
        "Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo.",
        "LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+.",
        "En nuestro artículo, principalmente discutimos la aplicación de la clasificación.",
        "Una entrada de la matriz Z significa la relación entre una página web y un factor.",
        "Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes.",
        "Por lo tanto, permitimos que los componentes tomen cualquier valor real posible.",
        "Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters.",
        "De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web.",
        "Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos.",
        "Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7.",
        "CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación.",
        "Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos.",
        "Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino.",
        "Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces.",
        "Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación.",
        "También discutimos una extensión para la aplicación de agrupamiento.",
        "Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo.",
        "También, gracias a los revisores por los comentarios constructivos. 8.",
        "REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB).",
        "Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry.",
        "Búsqueda web a través de la síntesis de concentradores.",
        "En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk.",
        "Categorización de hipertexto mejorada utilizando hiperenlaces.",
        "En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998.",
        "ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J.",
        "I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish?",
        "LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001.",
        "Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang.",
        "Aprendiendo a identificar de manera probabilística documentos autoritativos.",
        "Procesado.",
        "ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann.",
        "El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto.",
        "En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436.",
        "MIT Press, 2001. [7] C. Cortes y V. Vapnik.",
        "Redes de vectores de soporte.",
        "Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman.",
        "Indexación mediante análisis semántico latente.",
        "Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X.",
        "Él, H. Zha, C. Ding y H. Simon.",
        "Agrupación de documentos web utilizando estructuras de hipervínculos.",
        "Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann.",
        "Indexación semántica latente probabilística.",
        "En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor.",
        "Núcleos compuestos para la categorización de hipertexto.",
        "En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001.",
        "Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg.",
        "Fuentes autorizadas en un entorno hiperenlazado.",
        "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi.",
        "SVM para la blogosfera: Identificación de blogs y detección de splogs.",
        "En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee.",
        "Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje.",
        "En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005.",
        "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore.",
        "Automatizando la construcción de portales de internet con aprendizaje automático.",
        "Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J.",
        "Oh, S. H. Myaeng y M.-H. Lee.",
        "Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental.",
        "En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000.",
        "ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd.",
        "Clasificación de citas PageRank: orden en la web.",
        "Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman.",
        "Inteligencia general, determinada y medida de manera objetiva.",
        "La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller.",
        "Modelos probabilísticos discriminativos para datos relacionales.",
        "En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong.",
        "Agrupación de documentos basada en la factorización de matrices no negativas.",
        "En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273.",
        "ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani.",
        "Un estudio de enfoques para la categorización de hipertexto.",
        "Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp.",
        "Indexación semántica latente informada de múltiples etiquetas.",
        "En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005.",
        "ACM Press. [23] T. Zhang, A. Popescul y B. Dom.",
        "Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web.",
        "En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006.",
        "ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf.",
        "Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido.",
        "En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann.",
        "Aprendizaje semisupervisado en grafos dirigidos.",
        "Procesado.",
        "Información neural.",
        "Sistemas de Procesamiento, 2004."
    ],
    "error_count": 1,
    "keys": {
        "matrix factorization": {
            "translated_key": "factorización de matrices",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using <br>matrix factorization</br> Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on <br>matrix factorization</br>: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of <br>matrix factorization</br>, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel <br>matrix factorization</br> method, which is more suitable than conventional <br>matrix factorization</br> methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link <br>matrix factorization</br> Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link <br>matrix factorization</br> is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link <br>matrix factorization</br>.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link <br>matrix factorization</br> Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content <br>matrix factorization</br> Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content <br>matrix factorization</br> There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined <br>matrix factorization</br> of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED <br>matrix factorization</br> Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the <br>matrix factorization</br>, called supervised <br>matrix factorization</br> [22].",
                "Because some data used in the <br>matrix factorization</br> have no label information, the supervised <br>matrix factorization</br> falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised <br>matrix factorization</br> problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of <br>matrix factorization</br> described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised <br>matrix factorization</br> in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content <br>matrix factorization</br> perform slightly better than other methods, our method of linkcontent supervised <br>matrix factorization</br> outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of <br>matrix factorization</br> and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised <br>matrix factorization</br> at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative <br>matrix factorization</br> for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative <br>matrix factorization</br>.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Combining Content and Link for Classification using <br>matrix factorization</br> Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "In the formulation, we perform factor analysis based on <br>matrix factorization</br>: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "As the result of <br>matrix factorization</br>, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "OUR APPROACH In this section we will first introduce a novel <br>matrix factorization</br> method, which is more suitable than conventional <br>matrix factorization</br> methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link <br>matrix factorization</br> Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages."
            ],
            "translated_annotated_samples": [
                "Combinando contenido y enlaces para clasificación utilizando <br>factorización de matrices</br> Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos.",
                "En la formulación, realizamos un análisis de factores basado en la <br>factorización de matrices</br>: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces.",
                "Como resultado de la <br>factorización de matrices</br>, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original.",
                "NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de <br>factorización de matrices</br>, que es más adecuado que los métodos convencionales de <br>factorización de matrices</br> para el análisis de enlaces.",
                "Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando <br>factorización de matrices</br> Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la <br>factorización de matrices</br>: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la <br>factorización de matrices</br>, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de <br>factorización de matrices</br>, que es más adecuado que los métodos convencionales de <br>factorización de matrices</br> para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "combining content and link": {
            "translated_key": "contenido y enlaces",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>combining content and link</br> for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "<br>combining content and link</br> for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks."
            ],
            "translated_annotated_samples": [
                "Combinando <br>contenido y enlaces</br> para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos."
            ],
            "translated_text": "Combinando <br>contenido y enlaces</br> para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "classification": {
            "translated_key": "clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for <br>classification</br> using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page <br>classification</br>, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext <br>classification</br> on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page <br>classification</br> as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the <br>classification</br> problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf <br>classification</br> tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional <br>classification</br> methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page <br>classification</br>, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog <br>classification</br>, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document <br>classification</br>, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as <br>classification</br>.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as <br>classification</br>, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document <br>classification</br>.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity <br>classification</br>, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page <br>classification</br>, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the <br>classification</br> accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their <br>classification</br> model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for <br>classification</br>.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page <br>classification</br>, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor <br>classification</br> accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional <br>classification</br> methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, <br>classification</br>, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page <br>classification</br>.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional <br>classification</br> or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of <br>classification</br>.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page <br>classification</br> problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform <br>classification</br>.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the <br>classification</br>, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional <br>classification</br> algorithms on Z to get the <br>classification</br> results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform <br>classification</br> on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: <br>classification</br> accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the <br>classification</br> accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average <br>classification</br> accuracies and it standard deviation over the five repeats. 5.3 Results The average <br>classification</br> accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the <br>classification</br> of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The <br>classification</br> accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: <br>classification</br> accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of <br>classification</br>.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on <br>classification</br> application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for <br>classification</br>.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Combining Content and Link for <br>classification</br> using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page <br>classification</br>, by exploiting both the content and the link structure.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext <br>classification</br> on the WebKB and Cora benchmarks.",
                "In the following discussion, we use the task of web page <br>classification</br> as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the <br>classification</br> problem of web pages, a simple approach is to treat web pages as independent documents."
            ],
            "translated_annotated_samples": [
                "Combinando contenido y enlaces para <br>clasificación</br> utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos.",
                "Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la <br>clasificación</br> de páginas web, aprovechando tanto el contenido como la estructura de enlaces.",
                "En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la <br>clasificación</br> de hipertexto en los benchmarks WebKB y Cora.",
                "En la siguiente discusión, utilizamos la tarea de <br>clasificación</br> de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos.",
                "Para el problema de <br>clasificación</br> de páginas web, un enfoque simple es tratar las páginas web como documentos independientes."
            ],
            "translated_text": "Combinando contenido y enlaces para <br>clasificación</br> utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la <br>clasificación</br> de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la <br>clasificación</br> de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de <br>clasificación</br> de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de <br>clasificación</br> de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web mining problem": {
            "translated_key": "problemas de minería web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving <br>web mining problem</br>s, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "It is thus difficult to apply traditional mining or learning methods for solving <br>web mining problem</br>s, e.g., web page classification, by exploiting both the content and the link structure."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver <br>problemas de minería web</br>, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver <br>problemas de minería web</br>, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "link structure": {
            "translated_key": "estructura de enlaces",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the <br>link structure</br>.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the <br>link structure</br> or the content information, some of them combine the only authority information with the content information, and the others first decompose the <br>link structure</br> into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the <br>link structure</br> provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the <br>link structure</br> may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both <br>link structure</br> and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global <br>link structure</br>.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of <br>link structure</br> In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying <br>link structure</br>, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the <br>link structure</br> as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates <br>link structure</br> and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall <br>link structure</br> is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the <br>link structure</br> provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the <br>link structure</br> and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or <br>link structure</br>.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores <br>link structure</br> in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the <br>link structure</br>. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on <br>link structure</br>. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and <br>link structure</br> for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the <br>link structure</br> plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the <br>link structure</br> as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and <br>link structure</br> of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and <br>link structure</br>, it is easy to combine both the content information and <br>link structure</br>.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the <br>link structure</br>.",
                "Though a few methods exploit both the <br>link structure</br> or the content information, some of them combine the only authority information with the content information, and the others first decompose the <br>link structure</br> into hub and authority features, then apply them as additional document features.",
                "For example, in the WebKB dataset, the <br>link structure</br> provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "From this point of view, the traditional classification methods that ignore the <br>link structure</br> may not be suitable.",
                "To improve the performance of web page classification, therefore, both <br>link structure</br> and content information should be taken into consideration."
            ],
            "translated_annotated_samples": [
                "Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la <br>estructura de enlaces</br>.",
                "Aunque algunos métodos explotan tanto la <br>estructura de enlaces</br> como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la <br>estructura de enlaces</br> en características de hub y autoridad, para luego aplicarlas como características adicionales del documento.",
                "Por ejemplo, en el conjunto de datos de WebKB, la <br>estructura de enlaces</br> proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos).",
                "Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la <br>estructura de enlaces</br> pueden no ser adecuados.",
                "Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la <br>estructura de enlaces</br> como la información de contenido deben ser tomadas en consideración."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la <br>estructura de enlaces</br>. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la <br>estructura de enlaces</br> como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la <br>estructura de enlaces</br> en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la <br>estructura de enlaces</br> proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la <br>estructura de enlaces</br> pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la <br>estructura de enlaces</br> como la información de contenido deben ser tomadas en consideración. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "content information": {
            "translated_key": "información de contenido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the <br>content information</br>, some of them combine the only authority information with the <br>content information</br>, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that <br>content information</br> can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and <br>content information</br> should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and <br>content information</br> have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and <br>content information</br> separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and <br>content information</br> and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both <br>content information</br> and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the <br>content information</br>.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both <br>content information</br> and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The <br>content information</br> is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the <br>content information</br> into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the <br>content information</br> on the vertices.",
                "To combine the link information and <br>content information</br>, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their <br>content information</br> and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text <br>content information</br> and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the <br>content information</br>, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the <br>content information</br> and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Though a few methods exploit both the link structure or the <br>content information</br>, some of them combine the only authority information with the <br>content information</br>, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "It is however a very rare case that <br>content information</br> can be ignorable.",
                "To improve the performance of web page classification, therefore, both link structure and <br>content information</br> should be taken into consideration.",
                "However, link and <br>content information</br> have different properties.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and <br>content information</br> separately and then combine them."
            ],
            "translated_annotated_samples": [
                "Aunque algunos métodos explotan tanto la estructura de enlaces como la <br>información de contenido</br>, algunos de ellos combinan únicamente la información de autoridad con la <br>información de contenido</br>, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento.",
                "Sin embargo, es un caso muy raro que la <br>información de contenido</br> pueda ser ignorada.",
                "Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la <br>información de contenido</br> deben ser tomadas en consideración.",
                "Sin embargo, el enlace y la <br>información del contenido</br> tienen propiedades diferentes.",
                "Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la <br>información de contenido</br> por separado para luego combinarlas."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la <br>información de contenido</br>, algunos de ellos combinan únicamente la información de autoridad con la <br>información de contenido</br>, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la <br>información de contenido</br> pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la <br>información de contenido</br> deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la <br>información del contenido</br> tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la <br>información de contenido</br> por separado para luego combinarlas. ",
            "candidates": [],
            "error": [
                [
                    "información de contenido",
                    "información de contenido",
                    "información de contenido",
                    "información de contenido",
                    "información del contenido",
                    "información de contenido"
                ]
            ]
        },
        "authority information": {
            "translated_key": "información de autoridad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only <br>authority information</br> with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and <br>authority information</br> of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Though a few methods exploit both the link structure or the content information, some of them combine the only <br>authority information</br> with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "The framework combines the hub and <br>authority information</br> of web pages."
            ],
            "translated_annotated_samples": [
                "Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la <br>información de autoridad</br> con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento.",
                "El marco combina la <br>información de hub y autoridad</br> de las páginas web."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la <br>información de autoridad</br> con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la <br>información de hub y autoridad</br> de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    "información de autoridad",
                    "información de hub y autoridad"
                ]
            ]
        },
        "joint factorization": {
            "translated_key": "factorización conjunta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a <br>joint factorization</br> on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a <br>joint factorization</br> on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors."
            ],
            "translated_annotated_samples": [
                "Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una <br>factorización conjunta</br> en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una <br>factorización conjunta</br> en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "linkage adjacency matrix": {
            "translated_key": "matriz de adyacencia de enlaces",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the <br>linkage adjacency matrix</br> and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the <br>linkage adjacency matrix</br> and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors."
            ],
            "translated_annotated_samples": [
                "Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la <br>matriz de adyacencia de enlaces</br> y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la <br>matriz de adyacencia de enlaces</br> y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "document-term matrix": {
            "translated_key": "matriz de documentos-términos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the <br>document-term matrix</br>, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the <br>document-term matrix</br> and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the <br>document-term matrix</br>, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Then we will introduce our approach that jointly factorizes the <br>document-term matrix</br> and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages."
            ],
            "translated_annotated_samples": [
                "Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la <br>matriz de documentos-términos</br>, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad.",
                "Luego presentaremos nuestro enfoque que factoriza conjuntamente la <br>matriz documento-término</br> y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la <br>matriz de documentos-términos</br>, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la <br>matriz documento-término</br> y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    "matriz de documentos-términos",
                    "matriz documento-término"
                ]
            ]
        },
        "low-dimensional factor space": {
            "translated_key": "espacio de factores de baja dimensión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a <br>low-dimensional factor space</br>, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a <br>low-dimensional factor space</br>, without explicitly separating them as content, hub or authority factors."
            ],
            "translated_annotated_samples": [
                "Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un <br>espacio de factores de baja dimensión</br>, sin separar explícitamente factores de contenido, hub o autoridad."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un <br>espacio de factores de baja dimensión</br>, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "webkb and cora benchmark": {
            "translated_key": "webkb y cora benchmark",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "relationship": {
            "translated_key": "relación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the <br>relationship</br> among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric <br>relationship</br> whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the <br>relationship</br> of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The <br>relationship</br> among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: <br>relationship</br> among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the <br>relationship</br> of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "For example, in the WebKB dataset, the link structure provides additional insights about the <br>relationship</br> among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "For example, a link is an actual piece of evidence that represents an asymmetric <br>relationship</br> whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "However, in analysis of link structures, we need to model the <br>relationship</br> of two ends of links, i.e., edges between vertex pairs.",
                "The <br>relationship</br> among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: <br>relationship</br> among the matrices."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la <br>relación</br> entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos).",
                "Por ejemplo, un enlace es una pieza de evidencia real que representa una <br>relación</br> asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica.",
                "Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la <br>relación</br> de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices.",
                "La <br>relación</br> entre estas matrices puede ser representada como se muestra en la Figura 3.",
                "Figura 3: Relación entre las matrices."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la <br>relación</br> entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una <br>relación</br> asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la <br>relación</br> de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La <br>relación</br> entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "asymmetric relationship": {
            "translated_key": "relación asimétrica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an <br>asymmetric relationship</br> whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "For example, a link is an actual piece of evidence that represents an <br>asymmetric relationship</br> whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, un enlace es una pieza de evidencia real que representa una <br>relación asimétrica</br>, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una <br>relación asimétrica</br>, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "cocitation similarity": {
            "translated_key": "similitud de cocitación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using <br>cocitation similarity</br>, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using <br>cocitation similarity</br>, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "Some work, such as [3], incorporates link information using <br>cocitation similarity</br>, but this may not fully capture the global link structure.",
                "But using <br>cocitation similarity</br>, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18]."
            ],
            "translated_annotated_samples": [
                "Algunos trabajos, como [3], incorporan información de enlaces utilizando la <br>similitud de cocitación</br>, pero esto puede no capturar completamente la estructura global de enlaces.",
                "Pero utilizando la <br>similitud de cocitación</br>, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la <br>similitud de cocitación</br>, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la <br>similitud de cocitación</br>, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "text content": {
            "translated_key": "contenido de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor analysis[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional factor analysis models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional factor analysis and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines <br>text content</br> information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the <br>text content</br> and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between <br>text content</br> and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "This method combines <br>text content</br> information and link structure for analysis.",
                "We propose a simple approach using factors to model the <br>text content</br> and link structure of web pages/documents.",
                "By sharing factors between <br>text content</br> and link structure, it is easy to combine both the content information and link structure."
            ],
            "translated_annotated_samples": [
                "Este método combina información del <br>contenido de texto</br> y estructura de enlaces para su análisis.",
                "Proponemos un enfoque sencillo que utiliza factores para modelar el <br>contenido de texto</br> y la estructura de enlaces de páginas web/documentos.",
                "Al compartir factores entre el <br>contenido del texto</br> y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando análisis factorial[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El análisis factorial tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un análisis de factores basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del <br>contenido de texto</br> y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el <br>contenido de texto</br> y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el <br>contenido del texto</br> y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    "contenido de texto",
                    "contenido de texto",
                    "contenido del texto"
                ]
            ]
        },
        "factor analysis": {
            "translated_key": "análisis factorial",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combining Content and Link for Classification using Matrix Factorization Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, USA ABSTRACT The world wide web contains rich textual contents that are interconnected via complex hyperlinks.",
                "This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample.",
                "It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure.",
                "The research in this direction has recently received considerable attention but are still in an early stage.",
                "Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features.",
                "Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors.",
                "Further analysis can be performed based on the compact representation of web pages.",
                "In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation 1.",
                "INTRODUCTION With the advance of the World Wide Web, more and more hypertext documents become available on the Web.",
                "Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).",
                "Comparing to data in traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a students homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.",
                "Performing information management tasks on such structured data raises many new research challenges.",
                "In the following discussion, we use the task of web page classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.",
                "For the classification problem of web pages, a simple approach is to treat web pages as independent documents.",
                "The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.",
                "However, this approach relies only on the content of web pages and ignores the structure of links among them.",
                "Link structures provide invaluable information about properties of the documents as well as relationships among them.",
                "For example, in the WebKB dataset, the link structure provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).",
                "Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d. (independent and identical distributed) assumption of documents does not hold any more.",
                "From this point of view, the traditional classification methods that ignore the link structure may not be suitable.",
                "On the other hand, a few studies, for example [25], rely solely on link structures.",
                "It is however a very rare case that content information can be ignorable.",
                "For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.",
                "To improve the performance of web page classification, therefore, both link structure and content information should be taken into consideration.",
                "To achieve this goal, a simple approach is to convert one type of information to the other.",
                "For example, in spam blog classification, Kolari et al. [13] concatenate outlink features with the content features of the blog.",
                "In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.",
                "However, link and content information have different properties.",
                "For example, a link is an actual piece of evidence that represents an asymmetric relationship whereas the content similarity is usually defined conceptually for every pair of documents in a symmetric way.",
                "Therefore, directly converting one type of information to the other usually degrades the quality of information.",
                "On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.",
                "We argue that such an approach ignores the inherent consistency between link and content information and therefore fails to combine the two seamlessly.",
                "Some work, such as [3], incorporates link information using cocitation similarity, but this may not fully capture the global link structure.",
                "In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.",
                "In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.",
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using <br>factor analysis</br>[18].",
                "In the proposed technique, both content information and link structures are seamlessly combined through a single set of latent factors.",
                "Our model contains two components.",
                "The first component captures the content information.",
                "This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.",
                "That is, documents are decomposed into latent topics/factors, which in turn are represented as term vectors.",
                "The second component captures the information contained in the underlying link structure, such as links from homepages of students to those of faculty members.",
                "A factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).",
                "It is worth noting that we do not explicitly define the semantic of a factor a priori.",
                "Instead, similar to LSI, the factors are learned from the data.",
                "Traditional <br>factor analysis</br> models the variables associated with entities through the factors.",
                "However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.",
                "Therefore, the model should involve factors of both vertices of the edge.",
                "This is a key difference between traditional <br>factor analysis</br> and our model.",
                "In our model, we connect two components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).",
                "By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.",
                "In the formulation, we perform <br>factor analysis</br> based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links.",
                "Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.",
                "This paper is organized as follows.",
                "Section 2 reviews related work.",
                "Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.",
                "Section 4 extends the basic framework and a few variants for fine tune.",
                "Section 5 shows the experiment results.",
                "Section 6 discusses the details of this approach and Section 7 concludes. 2.",
                "RELATED WORK In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].",
                "LSI maps documents into a lower dimensional latent space.",
                "The latent space implicitly captures a large portion of information of documents, therefore it is called the latent semantic space.",
                "The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.",
                "Analysis tasks, such as classification, could be performed on the latent space.",
                "The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.",
                "Though our approach also uses latent space to represent web pages (documents), we consider the link structure as well as the content of web pages.",
                "In the link analysis approach, the framework of hubs and authorities (HITS) [12] puts web page into two categories, hubs and authorities.",
                "Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.",
                "Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.",
                "He et al. [9] propose a clustering algorithm for web document clustering.",
                "The algorithm incorporates link structure and the co-citation patterns.",
                "In the algorithm, all links are treated as undirected edge of the link graph.",
                "The content information is only used for weighing the links by the textual similarity of both ends of the links.",
                "Zhang et al. [23] uses the undirected graph regularization framework for document classification.",
                "Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.",
                "Zhou et al. [25] and [24] propose a directed graph regularization framework for semi-supervised learning.",
                "The framework combines the hub and authority information of web pages.",
                "But it is difficult to combine the content information into that framework.",
                "Our approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.",
                "Cohn and Hofmann [6] construct the latent space from both content and link information, using content analysis based on probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].",
                "The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.",
                "In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.",
                "In the model, the outgoing links of the destination web page have no effect on the source web page.",
                "In other words, the overall link structure is not utilized in PHITS.",
                "In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the destination web page, instead of the destination web page itself.",
                "The factor of the destination web page contains information of its outgoing links.",
                "In turn, such information is passed to the factor of the source web page.",
                "As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.",
                "Taskar et al. [19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.",
                "The model was applied to web page classification, where web pages are entities and hyperlinks are treated as relationships.",
                "RMNs apply conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.",
                "However the model does not give an off-the-shelf solution, because the success highly depends on the arts of designing the potential functions.",
                "On the other hand, the inference for RMNs is intractable and requires belief propagation.",
                "The following are some work on combining documents and links, but the methods are loosely related to our approach.",
                "The experiments of [21] show that using terms from the linked document improves the classification accuracy.",
                "Chakrabarti et al. [3] use co-citation information in their classification model.",
                "Joachims et al. [11] combine text kernels and co-citation kernels for classification.",
                "Oh et al [16] use the Naive Bayesian frame to combine link information with content. 3.",
                "OUR APPROACH In this section we will first introduce a novel matrix factorization method, which is more suitable than conventional matrix factorization methods for link analysis.",
                "Then we will introduce our approach that jointly factorizes the document-term matrix and link matrix and obtains compact and highly indicative factors for representing documents or web pages. 3.1 Link Matrix Factorization Suppose we have a directed graph G = (V, E), where the vertex set V = {vi}n i=1 represents the web pages and the edge set E represents the hyperlinks between web pages.",
                "Let A = {asd} denotes the n×n adjacency matrix of G, which is also called the link matrix in this paper.",
                "For a pair of vertices, vs and vd, let asd = 1 when there is an edge from vs to vd, and asd = 0, otherwise.",
                "Note that A is an asymmetric matrix, because hyperlinks are directed.",
                "Most machine learning algorithms assume a feature-vector representation of instances.",
                "For web page classification, however, the link graph does not readily give such a vector representation for web pages.",
                "If one directly uses each row or column of A for the job, she will suffer a very high computational cost because the dimensionality equals to the number of web pages.",
                "On the other hand, it will produces a poor classification accuracy (see our experiments in Section 5), because A is extremely sparse1 .",
                "The idea of link matrix factorization is to derive a high-quality feature representation Z of web pages based on analyzing the link matrix A, where Z is an n × l matrix, with each row being the ldimensional feature vector of a web page.",
                "The new representation of web pages captures the principal factors of the link structure and makes further processing more efficient.",
                "One may use a method similar to LSI, to apply the well-known principal component analysis (PCA) for deriving Z from A.",
                "The corresponding optimization problem 2 is min Z,U A − ZU 2 F + γ U 2 F (1) where γ is a small positive number, U is an l ×n matrix, and · F is the Frobenius norm.",
                "The optimization aims to approximate A by ZU , a product of two low-rank matrices, with a regularization on U.",
                "In the end, the i-th row vector of Z can be thought as the hub feature vector of vertex vi, and the row vector of U can be thought as the authority features.",
                "A link generation model proposed in [2] is similar to the PCA approach.",
                "Since A is a nonnegative matrix here, one can also consider to put nonnegative constraints on U and Z, which produces an algorithm similar to PLSA [10] and NMF [20]. 1 Due to the sparsity of A, links from two similar pages may not share any common target pages, which makes them to appear dissimilar.",
                "However the two pages may be indirectly linked to many common pages via their neighbors. 2 Another equivalent form is minZ,U A − ZU 2 F , s. t. U U = I.",
                "The solution Z is identical subject to a scaling factor.",
                "However, despite its popularity in matrix analysis, PCA (or other similar methods like PLSA) is restrictive for link matrix factorization.",
                "The major problem is that, PCA ignores the fact that the rows and columns of A are indexed by exactly the same set of objects (i.e., web pages).",
                "The approximating matrix ˜A = ZU shows no evidence that links are within the same set of objects.",
                "To see the drawback, lets consider a link transitivity situation vi → vs → vj, where page i is linked to page s which itself is linked to page j.",
                "Since ˜A = ZU treats A as links from web pages {vi} to a different set of objects, let it be denoted by {oi}, ˜A = ZU actually splits an linked object os from vs and breaks down the link path into two parts vi → os and vs → oj.",
                "This is obviously a miss interpretation to the original link path.",
                "To overcome the problem of PCA, in this paper we suggest to use a different factorization: min Z,U A − ZUZ 2 F + γ U 2 F (2) where U is an l × l full matrix.",
                "Note that U is not symmetric, thus ZUZ produces an asymmetric matrix, which is the case of A.",
                "Again, each row vector of Z corresponds to a feature vector of a web pages.",
                "The new approximating form ˜A = ZUZ puts a clear meaning that the links are between the same set of objects, represented by features Z.",
                "The factor model actually maps each vertex, vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl space.",
                "We call the Rl space the factor space.",
                "Then, {zi} encodes the information of incoming and outgoing connectivity of vertices {vi}.",
                "The factor loadings, U, explain how these observed connections happened based on {zi}.",
                "Once we have the vector zi, we can use many traditional classification methods (such as SVMs) or clustering tools (such as K-Means) to perform the analysis.",
                "Illustration Based on a Synthetic Problem To further illustrate the advantages of the proposed link matrix factorization Eq. (2), let us consider the graph in Figure 1.",
                "Given v1 v2 v3 v4 v5 v6 v7 v8 Figure 2: Summarize Figure 1 with a factor graph these observations, we can summarize the graph by grouping as factor graph depicted in Figure 2.",
                "In the next we preform the two factorization methods Eq. (2) and Eq. (1) on this link matrix.",
                "A good low-rank representation should reveal the structure of the factor graph.",
                "First we try PCA-like decomposition, solving Eq. (1) and obtaining Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 We can see that the row vectors of v6 and v7 are the same in Z, indicating that v6 and v7 have the same hub attributes.",
                "The row vectors of v2 and v3 are the same in U, indicating that v2 and v3 have the same authority attributes.",
                "It is not clear to see the similarity between v4 and v5, because their inlinks (and outlinks) are different.",
                "Then, we factorize A by ZUZ via solving Eq. (2), and obtain the results Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 The resultant Z is very consistent with the clustering structure of vertices: the row vectors of v2 and v3 are the same, those of v4 and v5 are the same, those of v6 and v7 are the same.",
                "Even interestingly, if we add constraints to ensure Z and U be nonnegative, we have Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 which clearly tells the assignment of vertices to clusters from Z and the links of factor graph from U.",
                "When the interpretability is not critical in some tasks, for example, classification, we found that it achieves better accuracies without the nonnegative constraints.",
                "Given our above analysis, it is clear that the factorization ZUZ is more expressive than ZU in representing the link matrix A. 3.2 Content Matrix Factorization Now let us consider the content information on the vertices.",
                "To combine the link information and content information, we want to use the same latent space to approximate the content as the latent space for the links.",
                "Using the bag-of-words approach, we denote the content of web pages by an n×m matrix C, each of whose rows represents a document, each column represents a keyword, where m is the number of keywords.",
                "Like the latent semantic indexing (LSI) [8], the l-dimensional latent space for words is denoted by an m × l matrix V .",
                "Therefore, we use ZV to approximate matrix C, min V,Z C − ZV 2 F + β V 2 F , (3) where β is a small positive number, β V 2 F serves as a regularization term to improve the robustness. 3.3 Joint Link-Content Matrix Factorization There are many ways to employ both the content and link information for web page classification.",
                "Our idea in this paper is not to simply combine them, but rather to fuse them into a single, consistent, and compact feature representation.",
                "To achieve this goal, we solve the following problem, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) Eq. (4) is the joined matrix factorization of A and C with regularization.",
                "The new representation Z is ensured to capture both the structures of the link matrix A and the content matrix C. Once we find the optimal Z, we can apply the traditional classification or clustering methods on vectorial data Z.",
                "The relationship among these matrices can be depicted as Figure 3.",
                "A Y C U Z V Figure 3: Relationship among the matrices.",
                "Node Y is the target of classification.",
                "Eq. (4) can be solved using gradient methods, such as the conjugate gradient method and quasi-Newton methods.",
                "Then main computation of gradient methods is evaluating the object function J and its gradients against variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV .",
                "Because of the sparsity of A, the computational complexity of multiplication of A and Z is O(µAl), where µA is the number of nonzero entries in A.",
                "Similarly, the computational complexity of C Z and CV is O(µC l), where µC is the number of nonzero entries in C. The computational complexity of the rest multiplications in the gradient computation is O(nl2 ).",
                "Therefore, the total computational complexity in one iteration is O(µAl + µC l + nl2 ).",
                "The number of links and the number of words in a web page are relatively small comparing to the number of web pages, and are almost constant as the number of web pages/documents increases, i.e. µA = O(n) and µC = O(n).",
                "Therefore, theoretically the computation time is almost linear to the number of web pages/documents, n. 4.",
                "SUPERVISED MATRIX FACTORIZATION Consider a web page classification problem.",
                "We can solve Eq. (4) to obtain Z as Section 3, then use a traditional classifier to perform classification.",
                "However, this approach does not take data labels into account in the first step.",
                "Believing that using data labels improves the accuracy by obtaining a better Z for the classification, we consider to use the data labels to guide the matrix factorization, called supervised matrix factorization [22].",
                "Because some data used in the matrix factorization have no label information, the supervised matrix factorization falls into the category of semi-supervised learning.",
                "Let C be the set of classes.",
                "For simplicity, we first consider binary class problem, i.e.",
                "C = {−1, 1}.",
                "Assume we know the labels {yi} for vertices in T ⊂ V. We want to find a hypothesis h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 otherwise.",
                "We assume a transform from the latent space to R is linear, i.e. h(vi) = w φ(vi) + b = w zi + b, (5) School course dept. faculty other project staff student total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Table 1: Dataset of WebKB where w and b are parameters to estimate.",
                "Here, w is the norm of the decision boundary.",
                "Similar to Support Vector Machines (SVMs) [7], we can use the hinge loss to measure the loss, X i:vi∈T [1 − yih(vi)]+ , where [x]+ is x if x ≥ 0, 0 if x < 0.",
                "However, the hinge loss is not smooth at the hinge point, which makes it difficult to apply gradient methods on the problem.",
                "To overcome the difficulty, we use a smoothed version of hinge loss for each data point, g(yih(vi)), (6) where g(x) = 8 >< >: 0 when x ≥ 2, 1 − x when x ≤ 0, 1 4 (x − 2)2 when 0 < x < 2.",
                "We reduce a multiclass problem into multiple binary ones.",
                "One simple scheme of reduction is the one-against-rest coding scheme.",
                "In the one-against-rest scheme, we assign a label vector for each class label.",
                "The element of a label vector is 1 if the data point belongs the corresponding class, −1, if the data point does not belong the corresponding class, 0, if the data point is not labeled.",
                "Let Y be the label matrix, each column of which is a label vector.",
                "Therefore, Y is a matrix of n × c, where c is the number of classes, |C|.",
                "Then the values of Eq. (5) form a matrix H = ZW + 1b , (7) where 1 is a vector of size n, whose elements are all one, W is a c × l parameter matrix, and b is a parameter vector of size c. The total loss is proportional to the sum of Eq. (6) over all labeled data points and the classes, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), where λ is the parameter to scale the term.",
                "To derive a robust solution, we also use Tikhonov regularization for W, ΩW (W) = ν 2 W 2 F , where ν is the parameter to scale the term.",
                "Then the supervised matrix factorization problem becomes min U,V,Z,W,b Js(U, V, Z, W, b) (8) where Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).",
                "We can also use gradient methods to solve the problem of Eq. (8).",
                "The gradients are ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, where G is an n×c matrix, whose ik-th element is Yikg (YikHik), and g (x) = 8 >< >: 0 when x ≥ 2, −1 when x ≤ 0, 1 2 (x − 2) when 0 < x < 2.",
                "Once we obtain w, b, and Z, we can apply h on the vertices with unknown class labels, or apply traditional classification algorithms on Z to get the classification results. 5.",
                "EXPERIMENTS 5.1 Data Description In this section, we perform classification on two datasets, to demonstrate the our approach.",
                "The two datasets are the WebKB data set[1] and the Cora data set [15].",
                "The WebKB data set consists of about 6000 web pages from computer science departments of four schools (Cornell, Texas, Washington, and Wisconsin).",
                "The web pages are classified into seven categories.",
                "The numbers of pages in each category are shown in Table 1.",
                "The Cora data set consists of the abstracts and references of about 34,000 computer science research papers.",
                "We use part of them to categorize into one of subfields of data structure (DS), hardware and architecture (HA), machine learning (ML), and programing language (PL).",
                "We remove those articles without reference to other articles in the set.",
                "The number of papers and the number of subfields in each area are shown in Table 2. area # of papers # of subfields Data structure (DS) 751 9 Hardware and architecture (HA) 400 7 Machine learning (ML) 1617 7 Programing language (PL) 1575 9 Table 2: Dataset of Cora 5.2 Methods The task of the experiments is to classify the data based on their content information and/or link structure.",
                "We use the following methods: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-content sup.",
                "MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Table 3: Classification accuracy (mean ± std-err %) on WebKB data set • SVM on content We apply support vector machines (SVM) on the content of documents.",
                "The features are the bag-ofwords and all word are stemmed.",
                "This method ignores link structure in the data.",
                "Linear SVM is used.",
                "The regularization parameter of SVM is selected using the cross-validation method.",
                "The implementation of SVM used in the experiments is libSVM[4]. • SVM on links We treat links as the features of each document, i.e. the i-th feature is link-to-pagei.",
                "We apply SVM on link features.",
                "This method uses link information, but not the link structure. • SVM on link-content We combine the features of the above two methods.",
                "We use different weights for these two set of features.",
                "The weights are also selected using crossvalidation. • Directed graph regularization This method is described in [25] and [24].",
                "This method is solely based on link structure. • PLSI+PHITS This method is described in [6].",
                "This method combines text content information and link structure for analysis.",
                "The PHITS algorithm is in spirit similar to Eq.1, with an additional nonnegative constraint.",
                "It models the outgoing and in-coming structures separately. • Link-content MF This is our approach of matrix factorization described in Section 3.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM using Z as the feature vectors, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output. • Link-content sup.",
                "MF This method is our approach of the supervised matrix factorization in Section 4.",
                "We use 50 latent factors for Z.",
                "After we compute Z, we train a linear SVM on the training portion of Z, then apply SVM on testing portion of Z to obtain the final result, because of the multiclass output.",
                "We randomly split data into five folds and repeat the experiment for five times, for each time we use one fold for test, four other folds for training.",
                "During the training process, we use the crossvalidation to select all model parameters.",
                "We measure the results by the classification accuracy, i.e., the percentage of the number of correct classified documents in the entire data set.",
                "The results are shown as the average classification accuracies and it standard deviation over the five repeats. 5.3 Results The average classification accuracies for the WebKB data set are shown in Table 3.",
                "For this task, the accuracies of SVM on links are worse than that of SVM on content.",
                "But the directed graph regularization, which is also based on link alone, achieves a much higher accuracy.",
                "This implies that the link structure plays an important role in the classification of this dataset, but individual links in a web page give little information.",
                "The combination of link and content using SVM achieves similar accuracy as that of SVM on content alone, which confirms individual links in a web page give little information.",
                "Since our approach consider the link structure as well as the content information, our two methods give results a highest accuracies among these approaches.",
                "The difference between the results of our two methods is not significant.",
                "However in the experiments below, we show the difference between them.",
                "The classification accuracies for the Cora data set are shown in Table 4.",
                "In this experiment, the accuracies of SVM on the combination of links and content are higher than either SVM on content or SVM on links.",
                "This indicates both content and links are infor45 50 55 60 65 70 75 80 PLMLHADS accuracy(%) dataset SVM on content SVM on link SVM on link-content Directed graph reg.",
                "PLSI+PHITS link-content MF link-content sup.",
                "MF method DS HA ML PL SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 link-content sup.",
                "MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Table 4: Classification accuracy (mean ± std-err %) on Cora data set mative for classifying the articles into subfields.",
                "The method of directed graph regularization does not perform as good as SVM on link-content, which confirms the importance of the article content in this task.",
                "Though our method of link-content matrix factorization perform slightly better than other methods, our method of linkcontent supervised matrix factorization outperform significantly. 5.4 The Number of Factors As we discussed in Section 3, the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.",
                "We perform experiments to study how the number of factors affects the accuracy of predication.",
                "We use different numbers of factors for the Cornell data of WebKB data set and the machine learning (ML) data of Cora data set.",
                "The result shown in Figure 4(a) and 4(b).",
                "The figures show that the accuracy 88 89 90 91 92 93 94 95 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (a) Cornell data 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 accuracy(%) number of factors link-content sup.",
                "MF link-content MF (b) ML data Figure 4: Accuracy vs number of factors increases as the number of factors increases.",
                "It is a different concept from choosing the optimal number of clusters in clustering application.",
                "It is how much information to represent in the latent variables.",
                "We have considered the regularization over the factors, which avoids the overfit problem for a large number of factors.",
                "To choose of the number of factors, we need to consider the trade-off between the accuracy and the computation time, which is quadratic to the number of factors.",
                "The difference between the method of matrix factorization and that of supervised one decreases as the number of factors increases.",
                "This indicates that the usefulness of supervised matrix factorization at lower number of factors. 6.",
                "DISCUSSIONS The loss functions LA in Eq. (2) and LC in Eq. (3) use squared loss due to computationally convenience.",
                "Actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative values, in our case, zero or one only, and the components of content matrix C can only take nonnegative integers.",
                "Therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.",
                "LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P i,j [1 − AijBij]+ .",
                "In our paper, we mainly discuss the application of classification.",
                "A entry of matrix Z means the relationship of a web page and a factor.",
                "The values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent topics.",
                "Therefore, we allow the components take any possible real values.",
                "When we come to the clustering application, we can use this model to find Z, then apply K-means to partition the web pages into clusters.",
                "Actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.",
                "As the example with nonnegative constraints shown in Section 3, we represent each cluster by a latent topic, i.e. the dimensionality of the latent space is set to the number of clusters we want.",
                "Then the problem of Eq. (4) becomes min U,V,Z J (U, V, Z), s.t.Z ≥ 0. (9) Solving Eq. (9), we can obtain more interpretable results, which could be used for clustering. 7.",
                "CONCLUSIONS In this paper, we study the problem of how to combine the information of content and links for web page analysis, mainly on classification application.",
                "We propose a simple approach using factors to model the text content and link structure of web pages/documents.",
                "The directed links are generated from the linear combination of linkage of between source and destination factors.",
                "By sharing factors between text content and link structure, it is easy to combine both the content information and link structure.",
                "Our experiments show our approach is effective for classification.",
                "We also discuss an extension for clustering application.",
                "Acknowledgment We would like to thank Dr. Dengyong Zhou for sharing his code of his algorithm.",
                "Also, thanks to the reviewers for constructive comments. 8.",
                "REFERENCES [1] CMU world wide knowledge base (WebKB) project.",
                "Available at http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry.",
                "Web search via hub synthesis.",
                "In IEEE Symposium on Foundations of Computer Science, pages 500-509, 2001. [3] S. Chakrabarti, B. E. Dom, and P. Indyk.",
                "Enhanced hypertext categorization using hyperlinks.",
                "In L. M. Haas and A. Tiwary, editors, Proceedings of SIGMOD-98, ACM International Conference on Management of Data, pages 307-318, Seattle, US, 1998.",
                "ACM Press, New York, US. [4] C.-C. Chang and C.-J.",
                "Lin.",
                "LIBSVM: a library for support vector machines, 2001.",
                "Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn and H. Chang.",
                "Learning to probabilistically identify authoritative documents.",
                "Proc.",
                "ICML 2000. pp.167-174., 2000. [6] D. Cohn and T. Hofmann.",
                "The missing link - a probabilistic model of document content and hypertext connectivity.",
                "In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 430-436.",
                "MIT Press, 2001. [7] C. Cortes and V. Vapnik.",
                "Support-vector networks.",
                "Machine Learning, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.",
                "Indexing by latent semantic analysis.",
                "Journal of the American Society of Information Science, 41(6):391-407, 1990. [9] X.",
                "He, H. Zha, C. Ding, and H. Simon.",
                "Web document clustering using hyperlink structures.",
                "Computational Statistics and Data Analysis, 41(1):19-45, 2002. [10] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor.",
                "Composite kernels for hypertext categorisation.",
                "In C. Brodley and A. Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 250-257, Williams College, US, 2001.",
                "Morgan Kaufmann Publishers, San Francisco, US. [12] J. M. Kleinberg.",
                "Authoritative sources in a hyperlinked environment.",
                "J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, and A. Joshi.",
                "SVMs for the Blogosphere: Blog Identification and Splog Detection.",
                "In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, March 2006. [14] O. Kurland and L. Lee.",
                "Pagerank without hyperlinks: structural re-ranking using links induced by language models.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 306-313, New York, NY, USA, 2005.",
                "ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.",
                "Automating the contruction of internet portals with machine learning.",
                "Information Retrieval Journal, 3(127-163), 2000. [16] H.-J.",
                "Oh, S. H. Myaeng, and M.-H. Lee.",
                "A practical hypertext catergorization method using links and incrementally available class information.",
                "In SIGIR 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 264-271, New York, NY, USA, 2000.",
                "ACM Press. [17] L. Page, S. Brin, R. Motowani, and T. Winograd.",
                "PageRank citation ranking: bring order to the web.",
                "Stanford Digital Library working paper 1997-0072, 1997. [18] C. Spearman.",
                "General Intelligence, objectively determined and measured.",
                "The American Journal of Psychology, 15(2):201-292, Apr 1904. [19] B. Taskar, P. Abbeel, and D. Koller.",
                "Discriminative probabilistic models for relational data.",
                "In Proceedings of 18th International UAI Conference, 2002. [20] W. Xu, X. Liu, and Y. Gong.",
                "Document clustering based on non-negative matrix factorization.",
                "In SIGIR 03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267-273.",
                "ACM Press, 2003. [21] Y. Yang, S. Slattery, and R. Ghani.",
                "A study of approaches to hypertext categorization.",
                "Journal of Intelligent Information Systems, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu, and V. Tresp.",
                "Multi-label informed latent semantic indexing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 258-265, New York, NY, USA, 2005.",
                "ACM Press. [23] T. Zhang, A. Popescul, and B. Dom.",
                "Linear prediction models with graph regularization for web-page categorization.",
                "In KDD 06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 821-826, New York, NY, USA, 2006.",
                "ACM Press. [24] D. Zhou, J. Huang, and B. Sch¨olkopf.",
                "Learning from labeled and unlabeled data on a directed graph.",
                "In Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany, 2005. [25] D. Zhou, B. Sch¨olkopf, and T. Hofmann.",
                "Semi-supervised learning on directed graphs.",
                "Proc.",
                "Neural Info.",
                "Processing Systems, 2004."
            ],
            "original_annotated_samples": [
                "But using cocitation similarity, the similarity between v4 and v5 is zero without considering other information. v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using <br>factor analysis</br>[18].",
                "Traditional <br>factor analysis</br> models the variables associated with entities through the factors.",
                "This is a key difference between traditional <br>factor analysis</br> and our model.",
                "In the formulation, we perform <br>factor analysis</br> based on matrix factorization: solution to the first component is based on factorizing the term-document matrix derived from content features; solution to the second component is based on factorizing the adjacency matrix derived from links."
            ],
            "translated_annotated_samples": [
                "Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando <br>análisis factorial</br>[18].",
                "El <br>análisis factorial</br> tradicional modela las variables asociadas con entidades a través de los factores.",
                "Esta es una diferencia clave entre el <br>análisis factorial</br> tradicional y nuestro modelo.",
                "En la formulación, realizamos un <br>análisis de factores</br> basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces."
            ],
            "translated_text": "Combinando contenido y enlaces para clasificación utilizando factorización de matrices Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {zsh,kyu,ychi,ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA 95014, EE. UU. RESUMEN La World Wide Web contiene contenidos textuales ricos que están interconectados a través de enlaces complejos. Esta enorme base de datos viola la suposición mantenida por la mayoría de los métodos estadísticos convencionales de que cada página web se considera como una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, la clasificación de páginas web, aprovechando tanto el contenido como la estructura de enlaces. La investigación en esta dirección ha recibido recientemente considerable atención pero aún se encuentra en una etapa temprana. Aunque algunos métodos explotan tanto la estructura de enlaces como la información de contenido, algunos de ellos combinan únicamente la información de autoridad con la información de contenido, y otros primero descomponen la estructura de enlaces en características de hub y autoridad, para luego aplicarlas como características adicionales del documento. Siendo prácticamente atractivo por su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que aproveche tanto la información de contenido como la de enlace, realizando una factorización conjunta en la matriz de adyacencia de enlaces y la matriz de documentos-términos, y deriva una nueva representación para las páginas web en un espacio de factores de baja dimensión, sin separar explícitamente factores de contenido, hub o autoridad. Se puede realizar un análisis adicional basado en la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los benchmarks WebKB y Cora. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información Términos Generales: Algoritmos, Experimentación 1. INTRODUCCIÓN Con el avance de la World Wide Web, cada vez hay más documentos de hipertexto disponibles en la red. Algunos ejemplos de estos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia WebKB, que contiene páginas web universitarias), artículos de investigación (por ejemplo, datos en CiteSeer), artículos de noticias en línea y medios generados por los clientes (por ejemplo, blogs). En comparación con los datos en la gestión tradicional de la información, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos desde la página de inicio de un estudiante que apuntan a la página de inicio de su asesor, citas de artículos, fuentes de un artículo de noticias, comentarios de un bloguero en las publicaciones de otro bloguero, y así sucesivamente. Realizar tareas de gestión de información en datos estructurados plantea muchos nuevos desafíos de investigación. En la siguiente discusión, utilizamos la tarea de clasificación de páginas web como ejemplo ilustrativo, mientras que las técnicas que desarrollamos en las secciones posteriores son igualmente aplicables a muchas otras tareas en recuperación de información y minería de datos. Para el problema de clasificación de páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación listas para usar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa únicamente en el contenido de las páginas web y no tiene en cuenta la estructura de los enlaces entre ellas. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos de WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre los documentos (por ejemplo, los enlaces suelen apuntar de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, la suposición usual de i.i.d. (independiente e idénticamente distribuido) de los documentos ya no se cumple. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura de enlaces pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], se basan únicamente en estructuras de enlaces. Sin embargo, es un caso muy raro que la información de contenido pueda ser ignorada. Por ejemplo, en el conjunto de datos de Cora, el contenido del resumen de un artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de páginas web, por lo tanto, tanto la estructura de enlaces como la información de contenido deben ser tomadas en consideración. Para lograr este objetivo, un enfoque simple es convertir un tipo de información en el otro. Por ejemplo, en la clasificación de blogs de spam, Kolari et al. [13] concatenan las características de enlaces salientes con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, el enlace y la información del contenido tienen propiedades diferentes. Por ejemplo, un enlace es una pieza de evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos detalladamente en el trabajo relacionado, que consideran la información de enlaces y la información de contenido por separado para luego combinarlas. Sostenemos que este enfoque ignora la coherencia inherente entre el enlace y la información del contenido y, por lo tanto, no logra combinar los dos de manera fluida. Algunos trabajos, como [3], incorporan información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura global de enlaces. En la Figura 1, por ejemplo, las páginas web v6 y v7 citan conjuntamente la página web v8, lo que implica que v6 y v7 son similares entre sí. A su vez, v4 y v5 deberían ser similares entre sí, ya que v4 y v5 citan páginas web similares v6 y v7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre v4 y v5 es cero sin considerar otra información. v1 v2 v3 v4 v5 v6 v7 v8 Figura 1: Un ejemplo de estructura de enlaces En este artículo, proponemos una técnica simple para analizar documentos interconectados, como páginas web, utilizando <br>análisis factorial</br>[18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlace se combinan de manera fluida a través de un único conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información del contenido. Este componente tiene una forma similar a la de los temas latentes en el Indexado Semántico Latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, los cuales a su vez se representan como vectores de términos. El segundo componente captura la información contenida en la estructura de enlaces subyacente, como los enlaces de las páginas de inicio de los estudiantes a las de los miembros del profesorado. Un factor puede considerarse vagamente como un tipo de documento (por ejemplo, aquellas páginas web pertenecientes a estudiantes). Vale la pena señalar que no definimos explícitamente la semántica de un factor a priori. En cambio, al igual que en LSI, los factores se aprenden a partir de los datos. El <br>análisis factorial</br> tradicional modela las variables asociadas con entidades a través de los factores. Sin embargo, en el análisis de estructuras de enlaces, necesitamos modelar la relación de los dos extremos de los enlaces, es decir, los bordes entre pares de vértices. Por lo tanto, el modelo debería involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el <br>análisis factorial</br> tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenidos) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que explique de la mejor manera posible tanto la estructura de contenido como la de enlaces de forma simultánea y coherente. En la formulación, realizamos un <br>análisis de factores</br> basado en la factorización de matrices: la solución al primer componente se basa en la factorización de la matriz término-documento derivada de las características del contenido; la solución al segundo componente se basa en la factorización de la matriz de adyacencia derivada de los enlaces. Debido a que las dos factorizaciones comparten una base común, las bases descubiertas (factores latentes) explican tanto la información de contenido como las estructuras de enlace, y luego se utilizan en tareas adicionales de gestión de información como la clasificación. Este documento está organizado de la siguiente manera. La sección 2 revisa el trabajo relacionado. La sección 3 presenta el enfoque propuesto para analizar la página web basado en la información combinada de enlaces y contenido. La sección 4 amplía el marco básico y algunas variantes para ajustes finos. La sección 5 muestra los resultados del experimento. La sección 6 discute los detalles de este enfoque y la sección 7 concluye. TRABAJO RELACIONADO En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la Indexación Semántica Latente (LSI) [8]. LSI mapea documentos en un espacio latente de dimensiones inferiores. El espacio latente captura implícitamente una gran cantidad de información de los documentos, por lo tanto se le llama espacio semántico latente. La similitud entre documentos podría definirse por los productos punto de los vectores correspondientes de los documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición de valores singulares (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también utiliza un espacio latente para representar páginas web (documentos), consideramos tanto la estructura de enlaces como el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de hubs y autoridades (HITS) [12] clasifica las páginas web en dos categorías, hubs y autoridades. Usando la noción recursiva, un centro es una página web con muchos enlaces salientes a autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de centros. En lugar de usar dos categorías, PageRank [17] utiliza una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de autoridades. Él et al. [9] proponen un algoritmo de agrupamiento para la agrupación de documentos web. El algoritmo incorpora la estructura de enlaces y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como aristas no dirigidas del grafo de enlaces. La información de contenido se utiliza únicamente para ponderar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al. [23] utiliza el marco de regularización de grafo no dirigido para la clasificación de documentos. Achlioptas et al[2] descomponen la web en atributos de centro y autoridad y luego los combinan con contenido. Zhou et al. [25] y [24] proponen un marco de regularización de grafo dirigido para el aprendizaje semi-supervisado. El marco combina la información de hub y autoridad de las páginas web. Pero es difícil combinar la información del contenido en ese marco. Nuestro enfoque considera el contenido y la vinculación dirigida entre los temas de las páginas web de origen y destino en un solo paso, lo cual implica que el tema combina la información de la página web como autoridades y como centros en un único conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlaces, utilizando un análisis de contenido basado en LSI probabilístico (PLSI) [10] y un análisis de enlaces basado en PHITS [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHITS) y nuestro enfoque está en la parte del análisis de enlaces. En PLSI+PHITS, el enlace se construye con la vinculación desde el tema de la página web de origen hasta la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen efecto en la página web de origen. En otras palabras, la estructura general de enlaces no se utiliza en PHITS. En nuestro enfoque, el enlace se construye con la conexión entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino en sí misma. El factor de la página web de destino contiene información sobre sus enlaces salientes. A su vez, esta información se transmite al factor de la página web de origen. Como resultado de la factorización de matrices, el factor forma un grafo de factores, una versión en miniatura del grafo original, preservando la estructura principal del grafo original. Taskar et al. [19] proponen redes de Markov relacionales (RMNs) para la clasificación de entidades, describiendo una distribución condicional de clases de entidades dadas las atributos y relaciones de las entidades. El modelo fue aplicado a la clasificación de páginas web, donde las páginas web son entidades y los hipervínculos son tratados como relaciones. Las RMNs aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en cliques de variables aleatorias, donde la estructura de enlace proporciona pistas para formar los cliques. Sin embargo, el modelo no proporciona una solución lista para usar, ya que el éxito depende en gran medida de las habilidades para diseñar las funciones potenciales. Por otro lado, la inferencia para las RMNs es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos sobre la combinación de documentos y enlaces, pero los métodos están poco relacionados con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al. [3] utilizan información de co-citación en su modelo de clasificación. Joachims et al. [11] combinan núcleos de texto y núcleos de co-citación para clasificación. Oh et al [16] utilizan el marco del Bayes ingenuo para combinar la información de enlaces con el contenido. 3. NUESTRO ENFOQUE En esta sección primero presentaremos un novedoso método de factorización de matrices, que es más adecuado que los métodos convencionales de factorización de matrices para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz documento-término y la matriz de enlaces, y obtiene factores compactos y altamente indicativos para representar documentos o páginas web. 3.1 Factorización de la Matriz de Enlaces Supongamos que tenemos un grafo dirigido G = (V, E), donde el conjunto de vértices V = {vi}n i=1 representa las páginas web y el conjunto de aristas E representa los hipervínculos entre las páginas web. Sea A = {asd} denota la matriz de adyacencia n×n de G, la cual también es llamada matriz de enlaces en este artículo. Para un par de vértices, vs y vd, dejemos que asd = 1 cuando haya una arista de vs a vd, y asd = 0, en caso contrario. Ten en cuenta que A es una matriz asimétrica, ya que los hipervínculos son dirigidos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de instancias en forma de vector de características. Para la clasificación de páginas web, sin embargo, el grafo de enlaces no proporciona fácilmente una representación vectorial para las páginas web. Si se utiliza directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual al número de páginas web. Por otro lado, producirá una baja precisión de clasificación (ver nuestros experimentos en la Sección 5), ya que A es extremadamente disperso. La idea de la factorización de matrices de enlaces es derivar una representación de características de alta calidad Z de las páginas web basada en el análisis de la matriz de enlaces A, donde Z es una matriz n × l, con cada fila siendo el vector de características de l dimensiones de una página web. La nueva representación de las páginas web captura los factores principales de la estructura de enlaces y hace que el procesamiento posterior sea más eficiente. Se puede utilizar un método similar a LSI para aplicar el conocido análisis de componentes principales (PCA) para derivar Z a partir de A. El problema de optimización correspondiente 2 es min Z,U A − ZU 2 F + γ U 2 F (1) donde γ es un número positivo pequeño, U es una matriz l ×n, y · F es la norma de Frobenius. La optimización tiene como objetivo aproximar A por ZU, un producto de dos matrices de rango bajo, con una regularización en U. Al final, el vector de fila i-ésimo de Z puede ser considerado como el vector de características del centro del vértice vi, y el vector de fila de U puede ser considerado como las características de autoridad. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque de PCA. Dado que A es una matriz no negativa aquí, también se puede considerar imponer restricciones no negativas en U y Z, lo que produce un algoritmo similar a PLSA [10] y NMF [20]. Debido a la dispersión de A, los enlaces de dos páginas similares pueden no compartir ninguna página de destino común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos. Otra forma equivalente es minZ,U A − ZU 2 F , sujeto a U U = I. La solución Z es idéntica sujeta a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matrices, el PCA (o otros métodos similares como PLSA) es restrictivo para la factorización de matrices de enlace. El problema principal es que, el PCA ignora el hecho de que las filas y columnas de A están indexadas por exactamente el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜A = ZU no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver la desventaja, consideremos una situación de transitividad de enlaces vi → vs → vj, donde la página i está enlazada a la página s, la cual a su vez está enlazada a la página j. Dado que ˜A = ZU trata a A como enlaces desde páginas web {vi} hacia un conjunto diferente de objetos, sea denotado por {oi}, ˜A = ZU realmente divide un objeto enlazado os desde vs y descompone la ruta de enlace en dos partes vi → os y vs → oj. Esto es claramente una mala interpretación del enlace original. Para superar el problema de PCA, en este artículo sugerimos utilizar una factorización diferente: min Z,U A − ZUZ 2 F + γ U 2 F (2) donde U es una matriz completa de tamaño l × l. Ten en cuenta que U no es simétrica, por lo tanto ZUZ produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector fila de Z corresponde a un vector de características de una página web. La nueva forma aproximada ˜A = ZUZ establece claramente que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factores en realidad mapea cada vértice, vi, en un vector zi = {zi,k; 1 ≤ k ≤ l} en el espacio Rl. Llamamos al espacio Rl el espacio factor. Entonces, {zi} codifica la información de la conectividad entrante y saliente de los vértices {vi}. Las cargas factoriales, U, explican cómo se produjeron estas conexiones observadas basadas en {zi}. Una vez que tengamos el vector zi, podemos utilizar muchos métodos de clasificación tradicionales (como las SVM) o herramientas de agrupamiento (como K-Means) para realizar el análisis. Ilustración basada en un problema sintético Para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta en la ecuación (2), consideremos el gráfico en la Figura 1. Dadas las observaciones v1 v2 v3 v4 v5 v6 v7 v8 de la Figura 2: Podemos resumir el gráfico agrupándolas como un grafo de factores representado en la Figura 2. En el siguiente paso realizamos los dos métodos de factorización Ecuación (2) y Ecuación (1) en esta matriz de enlaces. Una buena representación de bajo rango debería revelar la estructura del grafo de factores. Primero intentamos una descomposición similar a PCA, resolviendo la Ecuación (1) y obteniendo Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 −0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores fila de v6 y v7 son iguales en Z, lo que indica que v6 y v7 tienen los mismos atributos de centro. Los vectores fila de v2 y v3 son iguales en U, lo que indica que v2 y v3 tienen los mismos atributos de autoridad. No es claro ver la similitud entre v4 y v5, porque sus enlaces entrantes (y salientes) son diferentes. Entonces, factorizamos A por ZUZ resolviendo la Ec. (2), y obtenemos los resultados Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4 .6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5 .0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 El Z resultante es muy consistente con la estructura de agrupamiento de los vértices: los vectores de fila de v2 y v3 son iguales, los de v4 y v5 son iguales, los de v6 y v7 son iguales. Incluso interesantemente, si agregamos restricciones para asegurar que Z y U sean no negativos, tenemos Z = U = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 1. 0 0 0 0 0 3 7 7 7 7 7 7 7 7 5 lo cual claramente indica la asignación de vértices a clústeres desde Z y los enlaces del grafo de factores desde U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, clasificación, encontramos que se logran mejores precisión sin las restricciones no negativas. Dado nuestro análisis anterior, es claro que la factorización ZUZ es más expresiva que ZU en la representación de la matriz de enlaces A. 3.2 Factorización de la Matriz de Contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información de los enlaces y la información del contenido, queremos utilizar el mismo espacio latente para aproximar el contenido como el espacio latente para los enlaces. Utilizando el enfoque de bolsa de palabras, denotamos el contenido de las páginas web mediante una matriz C de n×m, donde cada una de sus filas representa un documento y cada columna representa una palabra clave, donde m es el número de palabras clave. Al igual que el indexado semántico latente (LSI) [8], el espacio latente l-dimensional para las palabras se denota por una matriz V de m × l. Por lo tanto, utilizamos ZV para aproximar la matriz C, min V,Z C − ZV 2 F + β V 2 F, (3) donde β es un número positivo pequeño, β V 2 F sirve como término de regularización para mejorar la robustez. 3.3 Factorización Conjunta de Matrices de Enlaces y Contenido Existen muchas formas de emplear tanto la información de contenido como de enlaces para la clasificación de páginas web. Nuestra idea en este artículo no es simplemente combinarlos, sino más bien fusionarlos en una representación de características única, consistente y compacta. Para lograr este objetivo, resolvemos el siguiente problema, min U,V,Z n J (U, V, Z) def = A − ZUZ 2 F + α C − ZV 2 F + γ U 2 F + β V 2 F o . (4) La ecuación (4) es la factorización de matriz conjunta de A y C con regularización. La nueva representación Z se asegura de capturar tanto las estructuras de la matriz de enlaces A como la matriz de contenido C. Una vez que encontramos el Z óptimo, podemos aplicar los métodos tradicionales de clasificación o agrupamiento en los datos vectoriales Z. La relación entre estas matrices puede ser representada como se muestra en la Figura 3. Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. La ecuación (4) se puede resolver utilizando métodos de gradiente, como el método del gradiente conjugado y los métodos quasi-Newton. La principal computación de los métodos de gradiente es evaluar la función objetivo J y sus gradientes con respecto a las variables, ∂J ∂U = Z ZUZ Z − Z AZ + γU, ∂J ∂V =α V Z Z − C Z + βV, ∂J ∂Z = ZU Z ZU + ZUZ ZU − A ZU − AZU + α ZV V − CV. Debido a la dispersión de A, la complejidad computacional de la multiplicación de A y Z es O(µAl), donde µA es el número de entradas no nulas en A. De manera similar, la complejidad computacional de C Z y CV es O(µC l), donde µC es el número de entradas no nulas en C. La complejidad computacional de las demás multiplicaciones en el cálculo del gradiente es O(nl2). Por lo tanto, la complejidad computacional total en una iteración es O(µAl + µC l + nl2). El número de enlaces y el número de palabras en una página web son relativamente pequeños en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas web/documentos, es decir, µA = O(n) y µC = O(n). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal al número de páginas web/documentos, n. 4. CONSIDERACIÓN DE FACTORIZACIÓN DE MATRIZ SUPERVISADA Considere un problema de clasificación de páginas web. Podemos resolver la Ec. (4) para obtener Z como sección 3, luego usar un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creemos que el uso de etiquetas de datos mejora la precisión al obtener un mejor Z para la clasificación, por lo que consideramos utilizar las etiquetas de datos para guiar la factorización de matrices, llamada factorización de matrices supervisada [22]. Debido a que algunos datos utilizados en la factorización de matrices no tienen información de etiqueta, la factorización de matrices supervisada se clasifica en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos un problema de clase binaria, es decir, C = {−1, 1}. \n\nC = {−1, 1}. Supongamos que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis h : V → R, de modo que asignemos vi a 1 cuando h(vi) ≥ 0, -1 en caso contrario. Suponemos que una transformación del espacio latente a R es lineal, es decir, h(vi) = w φ(vi) + b = w zi + b, donde w y b son parámetros a estimar. Aquí, w es la norma del límite de decisión. Similar a las Máquinas de Vectores de Soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, X i:vi∈T [1 − yih(vi)]+ , donde [x]+ es x si x ≥ 0, 0 si x < 0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta aplicar métodos de gradiente en el problema. Para superar la dificultad, utilizamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g(yih(vi)), donde g(x) = 8 >< >: 0 cuando x ≥ 2, 1 − x cuando x ≤ 0, 1 4 (x − 2)2 cuando 0 < x < 2. Reducimos un problema de múltiples clases en varios problemas binarios. Un esquema simple de reducción es el esquema de codificación uno contra el resto. En el esquema uno contra el resto, asignamos un vector de etiquetas para cada etiqueta de clase. El elemento de un vector de etiquetas es 1 si el punto de datos pertenece a la clase correspondiente, −1 si el punto de datos no pertenece a la clase correspondiente, 0 si el punto de datos no está etiquetado. Sea Y la matriz de etiquetas, donde cada columna es un vector de etiquetas. Por lo tanto, Y es una matriz de n × c, donde c es el número de clases, |C|. Entonces, los valores de la ecuación (5) forman una matriz H = ZW + 1b, (7) donde 1 es un vector de tamaño n, cuyos elementos son todos uno, W es una matriz de parámetros c × l, y b es un vector de parámetro de tamaño c. La pérdida total es proporcional a la suma de la ecuación (6) sobre todos los puntos de datos etiquetados y las clases, LY (W, b, Z) = λ X i:vi∈T ,j∈C g(YijHij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también utilizamos la regularización de Tikhonov para W, ΩW (W) = ν 2 W 2 F, donde ν es el parámetro para escalar el término. Entonces, el problema de factorización de matrices supervisado se convierte en min U,V,Z,W,b Js(U, V, Z, W, b) (8) donde Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W). También podemos utilizar métodos de gradiente para resolver el problema de la Ecuación (8). Los gradientes son ∂Js ∂U = ∂J ∂U , ∂Js ∂V = ∂J ∂V , ∂Js ∂Z = ∂J ∂Z + λGW, ∂Js ∂W =λG Z + νW, ∂Js ∂b =λG 1, donde G es una matriz n×c, cuyo elemento ik-ésimo es Yikg (YikHik), y g (x) = 8 >< >: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x − 2) cuando 0 < x < 2. Una vez que obtengamos w, b y Z, podemos aplicar h a los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en Z para obtener los resultados de clasificación. 5. EXPERIMENTOS 5.1 Descripción de los datos En esta sección, realizamos clasificación en dos conjuntos de datos, para demostrar nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB[1] y el conjunto de datos Cora [15]. El conjunto de datos de WebKB consiste en aproximadamente 6000 páginas web de los departamentos de informática de cuatro universidades (Cornell, Texas, Washington y Wisconsin). Las páginas web están clasificadas en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos Cora consiste en los resúmenes y referencias de alrededor de 34,000 artículos de investigación en informática. Utilizamos parte de ellos para categorizar en uno de los subcampos de estructuras de datos (DS), hardware y arquitectura (HA), aprendizaje automático (ML) y lenguaje de programación (PL). Eliminamos aquellos artículos que no hacen referencia a otros artículos en el conjunto. El número de artículos y el número de subcampos en cada área se muestran en la Tabla 2. área # de artículos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: Conjunto de datos de Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos basándose en su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 WisconsinWashingtonTexasCornell precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF Cornell Texas Washington Wisconsin SVM en contenido 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM en enlaces 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM en enlace-contenido 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90 Regularización de grafo dirigido 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 MF en enlace-contenido 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 sup. Aplicamos máquinas de vectores de soporte (SVM) en el contenido de los documentos. Las características son el modelo de bolsa de palabras y todas las palabras están truncadas. Este método ignora la estructura de enlaces en los datos. Se utiliza SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libSVM[4]. • SVM en enlaces Tratamos los enlaces como las características de cada documento, es decir, la i-ésima característica es enlace-a-páginai. Aplicamos SVM en las características de enlaces. Este método utiliza información de enlaces, pero no la estructura de enlaces. • SVM en contenido de enlaces. Combinamos las características de los dos métodos anteriores. Utilizamos diferentes pesos para estos dos conjuntos de características. Los pesos también son seleccionados utilizando validación cruzada. • Regularización de grafo dirigido. Este método está descrito en [25] y [24]. Este método se basa únicamente en la estructura de enlaces. • PLSI+PHITS Este método se describe en [6]. Este método combina información del contenido de texto y estructura de enlaces para su análisis. El algoritmo PHITS es en espíritu similar a la Ec.1, con una restricción adicional no negativa. Modela las estructuras salientes y entrantes por separado. • Contenido de enlace MF Este es nuestro enfoque de factorización de matrices descrito en la Sección 3. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal utilizando Z como vectores de características, luego aplicamos SVM en la porción de prueba de Z para obtener el resultado final, debido a la salida multiclase. • Contenido del enlace sup. Este método es nuestro enfoque de la factorización de matrices supervisada en la Sección 4. Utilizamos 50 factores latentes para Z. Después de calcular Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Dividimos los datos aleatoriamente en cinco pliegues y repetimos el experimento cinco veces, en cada ocasión utilizamos un pliegue para pruebas y los otros cuatro pliegues para entrenamiento. Durante el proceso de entrenamiento, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados mediante la precisión de la clasificación, es decir, el porcentaje del número de documentos clasificados correctamente en todo el conjunto de datos. Los resultados se muestran como las precisión de clasificación promedio y su desviación estándar en las cinco repeticiones. Los promedios de precisión de clasificación para el conjunto de datos de WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en enlaces son peores que las de SVM en contenido. Pero la regularización del grafo dirigido, que también se basa únicamente en los enlaces, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web proporcionan poca información. La combinación de enlaces y contenido utilizando SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web proporcionan poca información. Dado que nuestro enfoque considera la estructura de enlaces, así como la información de contenido, nuestros dos métodos ofrecen resultados con las mayores precisiones entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos Cora se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son mayores que las de SVM solo en contenido o solo en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS precisión(%) conjunto de datos SVM en contenido SVM en enlace SVM en enlace-contenido Grafo dirigido reg. PLSI+PHITS enlace-contenido MF enlace-contenido sup. Método MF DS HA ML PL SVM en contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en enlace-contenido 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00 Regularización de grafo dirigido 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 MF en enlace-contenido 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80 sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Exactitud de clasificación (media ± error estándar %) en el conjunto de datos de Cora informativo para clasificar los artículos en subcampos. El método de regularización de grafo dirigido no funciona tan bien como SVM en enlaces-contenido, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matrices de enlaces y contenido funciona ligeramente mejor que otros métodos, nuestro método de factorización de matrices supervisadas de enlaces y contenido supera significativamente. 5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrática respecto al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicción. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos Cora. El resultado mostrado en la Figura 4(a) y 4(b). Las cifras muestran que la precisión es del 88 al 95%, con un incremento de 0 a 50 en el número de factores de enlace de contenido compatible. Enlace de MF contenido de MF (a) Datos de Cornell 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 precisión(%) número de factores enlace de contenido sup. La precisión aumenta a medida que aumenta el número de factores. Es un concepto diferente al de elegir el número óptimo de grupos en una aplicación de agrupamiento. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo cual evita el problema de sobreajuste para un gran número de factores. Para elegir el número de factores, necesitamos considerar el equilibrio entre la precisión y el tiempo de cálculo, que es cuadrático respecto al número de factores. La diferencia entre el método de factorización de matrices y el supervisado disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de matrices supervisada es mayor con un menor número de factores. 6. Las funciones de pérdida LA en la ecuación (2) y LC en la ecuación (3) utilizan la pérdida al cuadrado debido a la conveniencia computacional. De hecho, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, ya que los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, solo cero o uno, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como la pérdida de bisagra o la pérdida de bisagra suavizada, por ejemplo. LA(U, Z) = µh(A, ZUZ ), donde h(A, B) =P i,j [1 − AijBij]+. En nuestro artículo, principalmente discutimos la aplicación de la clasificación. Una entrada de la matriz Z significa la relación entre una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de que las páginas web pertenezcan a temas latentes. Por lo tanto, permitimos que los componentes tomen cualquier valor real posible. Cuando llegamos a la aplicación de agrupamiento, podemos usar este modelo para encontrar Z, luego aplicar K-means para dividir las páginas web en clusters. De hecho, podemos utilizar la idea de factorización de matrices no negativas para el agrupamiento [20] para clusterizar directamente páginas web. Como se muestra en el ejemplo con restricciones no negativas en la Sección 3, representamos cada grupo con un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que deseamos. Entonces, el problema de la Ec. (4) se convierte en min U,V,Z J (U, V, Z), sujeto a Z ≥ 0. (9) Al resolver la Ec. (9), podemos obtener resultados más interpretables, que podrían ser utilizados para el agrupamiento. 7. CONCLUSIONES En este artículo, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de páginas web, principalmente en aplicaciones de clasificación. Proponemos un enfoque sencillo que utiliza factores para modelar el contenido de texto y la estructura de enlaces de páginas web/documentos. Los enlaces dirigidos se generan a partir de la combinación lineal de la vinculación entre los factores de origen y destino. Al compartir factores entre el contenido del texto y la estructura de enlaces, es fácil combinar tanto la información del contenido como la estructura de enlaces. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupamiento. Agradecimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir el código de su algoritmo. También, gracias a los revisores por los comentarios constructivos. 8. REFERENCIAS [1] Proyecto de base de conocimiento mundial de CMU (WebKB). Disponible en http://www.cs.cmu.edu/∼WebKB/. [2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de concentradores. En el Simposio de la IEEE sobre Fundamentos de la Ciencia de la Computación, páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización de hipertexto mejorada utilizando hiperenlaces. En L. M. Haas y A. Tiwary, editores, Actas de SIGMOD-98, Conferencia Internacional de ACM sobre Gestión de Datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU. [4] C.-C. Chang y C.-J. I'm sorry, but \"Lin\" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? LIBSVM: una biblioteca para máquinas de vectores de soporte, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm. [5] D. Cohn y H. Chang. Aprendiendo a identificar de manera probabilística documentos autoritativos. Procesado. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El eslabón perdido: un modelo probabilístico del contenido de documentos y la conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de vectores de soporte. Aprendizaje automático, 20:273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación mediante análisis semántico latente. Revista de la Sociedad Americana de Ciencia de la Información, 41(6):391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadística Computacional y Análisis de Datos, 41(1):19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vigésimo Segunda Conferencia Internacional Anual de SIGIR, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Núcleos compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional sobre Aprendizaje Automático, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU. [12] J. M. Kleinberg. Fuentes autorizadas en un entorno hiperenlazado. J. ACM, 48:604-632, 1999. [13] P. Kolari, T. Finin, y A. Joshi. SVM para la blogosfera: Identificación de blogs y detección de splogs. En el Simposio de Primavera de la AAAI sobre Enfoques Computacionales para Analizar Weblogs, marzo de 2006. [14] O. Kurland y L. Lee. Pagerank sin hiperenlaces: reordenamiento estructural utilizando enlaces inducidos por modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press. [15] A. McCallum, K. Nigam, J. Rennie, y K. Seymore. Automatizando la construcción de portales de internet con aprendizaje automático. Revista de Recuperación de Información, 3(127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H. Lee. Un método práctico de categorización de hipertexto utilizando enlaces e información de clase disponible de forma incremental. En SIGIR 00: Actas de la 23ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press. [17] L. Page, S. Brin, R. Motowani y T. Winograd. Clasificación de citas PageRank: orden en la web. Stanford Digital Library documento de trabajo 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida de manera objetiva. La Revista Americana de Psicología, 15(2):201-292, Abr 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu y Y. Gong. Agrupación de documentos basada en la factorización de matrices no negativas. En SIGIR 03: Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Revista de Sistemas de Información Inteligente, 18(2-3):219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada de múltiples etiquetas. En SIGIR 05: Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press. [23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de páginas web. En KDD 06: Actas de la 12ª conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press. [24] D. Zhou, J. Huang, y B. Sch¨olkopf. Aprendizaje a partir de datos etiquetados y no etiquetados en un grafo dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Automático, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semisupervisado en grafos dirigidos. Procesado. Información neural. Sistemas de Procesamiento, 2004. ",
            "candidates": [],
            "error": [
                [
                    "análisis factorial",
                    "análisis factorial",
                    "análisis factorial",
                    "análisis de factores"
                ]
            ]
        }
    }
}